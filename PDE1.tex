%% LyX 2.0.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,ngerman,english]{scrreprt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2.6cm,bmargin=3.5cm,lmargin=2.6cm,rmargin=2.6cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{color}
\usepackage{babel}
\usepackage{float}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,
 breaklinks=true,pdfborder={0 0 0},backref=page,colorlinks=false]
 {hyperref}
\hypersetup{pdftitle={Partial Differential Equations 1},
 pdfauthor={Andreas Völklein},
 pdfkeywords={Partial Differential Equations, Mathematics}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
\newcommand{\noun}[1]{\textsc{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{enumitem}		% customizable list environments
\newlength{\lyxlabelwidth}      % auxiliary length 

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tikz,pgfplots}
%\usepackage{tikz-3dplot,cancel,polynom}
\usetikzlibrary{matrix,arrows,calc,decorations,decorations.markings,intersections,shapes}
\usetikzlibrary{external}
\tikzexternalize
\usepackage{latexsym,stmaryrd,stackrel,braket,bbm,subfig,framed,esvect,scrhack,calc,upgreek}
\usepackage [OMLmathrm,OMLmathbf,sfdefault=fav]{isomath}
\usepackage[explicit]{titlesec}
\usepackage[activate]{pdfcprot}

\pgfkeys{/pgf/number format/dec sep={\text{,}}}
\pgfplotsset{compat=newest}

% Inhaltsverzeichnis
\usepackage[subfigure]{tocloft}

\tocloftpagestyle{fancy}

\renewcommand{\cftchapindent}{1 em}
\renewcommand{\cftchapnumwidth}{1.5 em}

\renewcommand{\cftsecindent}{2.7 em}
\renewcommand{\cftsecnumwidth}{2.5em}

\renewcommand{\cftsubsecindent}{5.2 em}
\renewcommand{\cftsubsecnumwidth}{3.8 em}

\renewcommand{\cftsubsubsecindent}{9 em}
\renewcommand{\cftsubsubsecnumwidth}{4.5 em}

% Mathe-Operatoren
\DeclareMathOperator*{\exsop}{\exists}
\DeclareMathOperator*{\exsgop}{\exists!}
\DeclareMathOperator*{\fallop}{\forall}
\DeclareMathOperator*{\bcupdop}{\dot{\bigcup}}
\DeclareMathOperator*{\bcapdop}{\dot{\bigcap}}

%Operatornorm
\newcommand{\opnor}[1]{\abs{\hspace*{-1.1pt}\norm{#1}\hspace*{-1.1pt}}}

% nicht-totales Differential
\newcommand{\dBar}{\mathchar'26\mkern-12mu \textnormal{d}}

% Angström
\newcommand{\ang}{\textup{\AA}}

% schöne Vektorpfeile
\renewcommand{\vec}[1]{\vv{#1}}

% Rotieren
\newcommand{\Rotate}[1]{
\tikzset{external/export next=false}
\begin{tikzpicture}
\node[rotate=90] {\ensuremath{#1}};
\end{tikzpicture}
}

%QED-Zeichen (Box)
\newcommand{\qed}{\ensuremath{\Box}}
\newcommand{\qqed}[1][\arabic{chapter}.\arabic{section}\ifnum\arabic{subsection}>0{.\arabic{subsection}}\fi]{\hspace*{1mm}\hfill\qed\ensuremath{_{\text{#1}}}}

% Mengen Modulo
\newcommand{\moduloT}[2]{
\mbox{\raisebox{0.6ex}{\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\Large /}
\raisebox{-0.6ex}{\hspace*{-1.5mm}\ensuremath{\displaystyle #2}}
}}

% Links Modulo
\newcommand{\lmoduloT}[2]{
\mbox{\raisebox{-0.6ex}{\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\Large \ensuremath{\backslash}}
\raisebox{0.6ex}{\hspace*{-1.5mm}\ensuremath{\displaystyle #2}}
}}

% Für Z/2Z, um nicht soviel schreiben zu müssen
\newcommand{\modloT}[2]{\moduloT{ \mathbb{#1}}{#2\mathbb{#1}}}

%Laplace-Beltrami-Operator
\newcommand{\LBO}{
\begin{minipage}{6mm}
 \tikzset{external/export next=false}
 \begin{tikzpicture}
   \node at (0,0){$\upDelta$};
   \draw[line width=0.75] (0.25,-0.13) -- (0.1,0.15);
 \end{tikzpicture}
\end{minipage}
}

%Die Modulo-Kommandos in klein, für die Darstellungen unter Quantoren.
\newcommand{\moduloScriptT}[2]{
\mbox{\raisebox{0.4ex}{\scriptsize\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\footnotesize /}
\raisebox{-0.4ex}{\hspace*{-1.5mm}\scriptsize\ensuremath{\displaystyle #2}}
}}

\newcommand{\lmoduloScriptT}[2]{
\mbox{\raisebox{-0.4ex}{\scriptsize\ensuremath{\displaystyle #1}}
{\hspace*{-1.5mm}\footnotesize \ensuremath{\backslash}}
\raisebox{0.4ex}{\hspace*{-1.5mm}\scriptsize\ensuremath{\displaystyle #2}}
}}

\newcommand{\modloScriptT}[2]{\moduloScriptT{ \mathbb{#1}}{#2\mathbb{#1}}}

% stehendes Winkelzeichen
\newcommand{\winkel}{
\tikzset{external/export next=false}
\begin{tikzpicture}[scale=0.25]
\draw ({-2+3^(1/2)},0) -- (0,1) -- ({2-3^(1/2)},0);
\draw ($(0,1) + ({cos(235)*0.7},{sin(315)*0.7})$) arc (235:315:0.7);
\end{tikzpicture}}

% Wurzel mit Häkchen
\newcommand{\hsqrt}[2][{}]{\setbox0=\hbox{$\sqrt[#1]{\phantom{|}\!\! #2\hspace*{1pt}}$}\dimen0=\ht0
  \advance\dimen0-0.2\ht0
  \setbox2=\hbox{\vrule height\ht0 depth -\dimen0}
  {\box0\lower0.4pt\box2}}

% Damit nicht immer "Kapitel 1" etc. über der Kapitelüberschrift steht
\titleformat{\chapter}
  {\huge\bfseries}
  {\textrm{\thechapter} }{0pt}
  {\textrm{#1} \thispagestyle{fancy}
  }

% Neudefinition der Abschnittsmarker für die Kopfzeile
\renewcommand\partmark[1]{\markboth{#1}{}}
\renewcommand\chaptermark[1]{\markright{\arabic{chapter} #1}}
\renewcommand\sectionmark[1]{}
\renewcommand\subsectionmark[1]{}

% Schriften auf Serif umstellen
\addtokomafont{descriptionlabel}{\rmfamily}
\addtokomafont{disposition}{\rmfamily}

% Zeilenumbrüche in Gleichungen
 \allowdisplaybreaks

% Kopf- und Fußzeile
% Höhe der Kopfzeile
\setlength{\headheight}{14pt}
% obere Trennlinie
%\renewcommand{\headrulewidth}{0.4pt}
\fancyhf{} %alle Kopf- und Fußzeilenfelder bereinigen
\fancyhead[L]{\textbf{Partial Differential Equations I}} %Kopfzeile links
%\fancyhead[C]{\leftmark} %zentrierte Kopfzeile
\fancyhead[R]{\rightmark} %Kopfzeile rechts
\fancyfoot[C]{\thepage\quad\!\!\!\slash\quad\!\!\!\pageref{END-front}} %Seitenzahl der Front-Matter

\AtBeginDocument{
  \def\labelitemi{\normalfont\bfseries{--}}
  \def\labelitemii{\(\circ\)}
  \def\labelitemiii{\(\triangleright\)}
}

\makeatother

\begin{document}




\selectlanguage{ngerman}%


\selectlanguage{english}%
\global\long\def\norm#1{\left\lVert #1\right\rVert }


\global\long\def\abs#1{\left\lvert #1\right\rvert }


\global\long\def\opnorm#1{\opnor{#1}}


\global\long\def\BRA#1{\Bra{#1}}


\global\long\def\KET#1{\Ket{#1}}


\global\long\def\BraKet#1{\Braket{#1}}


\global\long\def\mins{\textnormal{-}}


\global\long\def\LB{\LBO}


\global\long\def\exs{\exsop}


\global\long\def\exsg{\exsgop}


\global\long\def\fall{\fallop}


\global\long\def\bcupd{\bcupdop}


\global\long\def\bcapd{\bcapdop}


\global\long\def\sr#1#2#3{\underset{#3}{\overset{#2}{#1}}}


\global\long\def\dd{\textnormal{d}}


\global\long\def\DD{\textnormal{D}}


\global\long\def\dbar{\dBar}


\global\long\def\TT{\textnormal{T}}


\global\long\def\ii{\textbf{i}}


\global\long\def\modulo#1#2{\moduloT{#1}{#2}}


\global\long\def\lmodulo#1#2{\lmoduloT{#1}{#2}}


\global\long\def\modlo#1#2{\modloT{#1}{#2}}


\global\long\def\moduloScript#1#2{\moduloScriptT{#1}{#2}}


\global\long\def\lmoduloScript#1#2{\lmoduloScriptT{#1}{#2}}


\global\long\def\modloScript#1#2{\modloScriptT{#1}{#2}}


\global\long\def\vek#1{\vectorsym{#1}}


\global\long\def\mat#1{\matrixsym{#1}}


\global\long\def\ten#1{\tensorsym{#1}}


\global\long\def\msd#1{\mathstrut_{#1}}


\global\long\def\msu#1{\mathstrut^{#1}}


\pagenumbering{roman}


\title{\hspace*{1mm}\vspace*{-15mm}\\
{\Huge{Partial Differential Equations I}}}


\author{\vspace*{-5mm}\\
\textit{\small{lecture by}}\\
\textit{\noun{\small{Prof. Dr. Felix Finster}}}\\
\textit{\small{during the summer semester 2013}}\\
\textit{\small{revision and layout in \LyX{} by}}\\
\textit{\noun{\small{Andreas Völklein}}}\\
\vspace*{5mm}\\
\includegraphics[clip,width=15cm]{unir}\\
\vspace*{3mm}\\
{\normalsize{Last changed: \today}}\\
\vspace*{-30mm}}

\maketitle
\fancyhead[R]{License}


\subsubsection*{ATTENTION}

This script does \emph{not} replace the lecture.

Therefore it is recommended \emph{strongly} to attend the lecture.

\vfill{}



\subsubsection*{Copyright Notice}

Copyright © 2013 \noun{Andreas Völklein}

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;

with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
Texts.

A copy of the license is included in the document entitled “GFDL”.


\subsubsection*{Disclaimer of Warranty}

\noun{Unless otherwise mutually agreed to by the parties in writing
and to the extent not prohibited by applicable law, }\textbf{\noun{the
Copyright Holders and any other party, who may distribute the Document
as permitted above,   provide the Document “as is}}\textbf{”,}\textbf{\noun{
without warranty of any kind}}\noun{, expressed, implied, statutory
or otherwise, including, but not limited to, the implied warranties
of merchantability, fitness for a particular purpose, non-infringement,
the absence of latent or other defects, accuracy, or the absence of
errors, whether or not discoverable.}


\subsubsection*{Limitation of Liability}

\textbf{\noun{In no event}}\noun{ unless required by applicable law
or agreed to in writing }\textbf{\noun{will the Copyright Holders,
or any other party, who may distribute the Document as permitted above,
be liable to you for any damages}}\noun{, including, but not limited
to, any general, special, incidental, consequential, punitive or exemplary
damages, however caused, regardless of the theory of liability, arising
out of or related to this license or any use of or inability to use
the Document, even if they have been advised of the possibility of
such damages.}

\textbf{\noun{In no event will the Copyright Holders'/Distributor's
liability to you}}\noun{, whether in contract, tort (including negligence),
or otherwise, }\textbf{\noun{exceed the amount you paid the Copyright
Holders/Distributor}}\noun{ for the document under this agreement.}


\subsubsection*{Links}

The text of the “GNU Free Documentation License” can also be read
on the following site:
\begin{quote}
\url{https://www.gnu.org/licenses/fdl-1.3.en.html}
\end{quote}
A transparent copy of the recent version of this document can be downloaded
from:
\begin{quote}
\url{https://github.com/andiv/PDE1}
\end{quote}
\newpage{}

\fancyhead[R]{Literature}


\subsection*{Literature}

Elliptic and parabolic partial differential equations:
\begin{itemize}
\item \noun{Jürgen Jost}: \emph{Partial Differential Equations}; Springer,
2007;\\
ISBN: 978-0-387-49318-3; \href{http://dx.doi.org/10.1007/978-0-387-49319-0}{doi: 10.1007/978-0-387-49319-0}\\
(good book, but not all details, small errors)
\item \noun{Lawrence C. Evans}: \emph{Partial Differential Equations}; American
Mathematical Society, 2010; ISBN: 978-0-8218-4974-3\\
(part of the lecture follows this book, lots of details)
\item \noun{David Gilbarg, Neil S. Trudinger}: \emph{Elliptic Partial Differential
Equations of second order}; Springer, 2001; ISBN: 3-540-41160-7\\
(classic textbook, complete treatment)
\item \noun{Robert A. Adams, John J. F. Fournier}: \emph{Sobolev Spaces};
Acadademic Press, 2009;\\
ISBN: 978-0-12-044143-3
\item \noun{Richard Courant, David Hilbert}: \emph{Methods of Mathematical
Physics: Partial Differential Equations, Volume II}; Wiley, 2008;\\
ISBN: 978-0471504399; \href{http://dx.doi.org/10.1002/9783527617234}{doi: 10.1002/9783527617234}\\
(Lebesgue spine)
\item \noun{Walter Rudin}: \emph{Real and Complex Analysis}; McGraw-Hill,
2009;\\
ISBN: 978-0-07-054234-1\\
(Urysohn's Lemma)
\end{itemize}
Hyperbolic partial differential equations (for the lecture ``Partial
Differential Equations II''):
\begin{itemize}
\item \noun{Fritz John}: \emph{Partial Differential Equations}; Springer,
1999\\
ISBN: 0-387-90609-6
\item \noun{Michael E. Taylor}: \emph{Partial Differential Equations I -
III}; Springer, 1997\\
ISBN: 0-387-94653-5, 0-387-94651-9, 0-387-94652-7\\
(nice detailed text books)
\item \noun{Joel Smoller}: \emph{Shock waves and reaction-diffusion equations};
Springer, 1994\\
ISBN: 3-540-94259-9\\
(nicely presented, good motivations, covers most of the material)
\item \noun{Friedrich Sauvigny}: Partial Differential Equations I-II; Springer,
2012\\
ISBN: 978-1-4471-2981-3, 978-1-4471-2984-4;\\
\href{http://dx.doi.org/10.1007/978-1-4471-2981-3}{doi: 10.1007/978-1-4471-2981-3},
\href{http://dx.doi.org/10.1007/3-540-27540-1}{10.1007/3-540-27540-1}
\item and many more $\ldots$
\end{itemize}
{\small{\newpage{}}}\fancyhead[R]{Table of Contents}
\fancyhead[C]{}

\tableofcontents{}\label{END-front}\newpage{}\pagenumbering{arabic}
\fancyfoot[C]{\thepage\quad\!\!\!\slash\quad\!\!\!\pageref{END}} % Seitenzahl des Hauptteils
\fancyhead[R]{\rightmark}
%\fancyhead[C]{\leftmark}%DATE: Mi 17.04.2013


\chapter{A Brief Introduction}

An ordinary differential equation (ODE) can be written as:
\begin{align*}
\frac{\dd}{\dd t}u\left(t\right)=\dot{u}\left(t\right) & =v\left(t,u\right)\\
u:I\subseteq\mathbb{R} & \to\mathbb{R}^{N}
\end{align*}
This equation involves only derivatives with respect to \emph{one}
variable $t$.
\begin{align*}
\frac{\partial}{\partial x}f\left(x,y\right)+\frac{\partial}{\partial y}f\left(x,y\right) & =0
\end{align*}
This is an example for a partial differential equation.


\section{Definition \textmd{(Partial Differential Equation)}}

A \emph{partial differential equation} (PDE) is a (scalar) equation,
which involves partial derivatives of an unknown function $u:\Omega\subseteq\mathbb{R}^{n}\to\mathbb{R}$.
We always assume that $\Omega\subseteq\mathbb{R}^{n}$ is open.\\
More generally, a \emph{system of partial differential equations}
is a system of equations involving partial derivatives of a function
$u:\Omega\subseteq\mathbb{R}^{n}\to\mathbb{R}^{N}$.\\
Similarly one can define partial differential equations on manifolds.

For ordinary differential equations we considered the initial-value
problem:
\begin{align*}
\dot{u}\left(t\right) & =v\left(t,u\right) & u\left(t_{0}\right) & =u_{0}
\end{align*}
For partial differential equations one considers
\begin{itemize}
\item the initial-value problem and
\item the boundary-value problem.
\end{itemize}

\section{Examples}
\begin{enumerate}
\item Cauchy-Riemann equations: Let
\begin{align*}
f:\Omega\stackrel{\text{open}}{\subseteq}\mathbb{C} & \to\mathbb{C}
\end{align*}
be holomorphic.
\begin{align*}
f & =a+\ii b & a & :=\text{Re}\left(f\right) & b & :=\text{Im}\left(f\right)
\end{align*}
\begin{align*}
\frac{\partial a}{\partial x} & =\frac{\partial b}{\partial y} & \frac{\partial b}{\partial x} & =-\frac{\partial a}{\partial y}
\end{align*}
This is a system of two partial differential equations.
\begin{align*}
u & :=\left(\begin{array}{c}
a\\
b
\end{array}\right) & u:\Omega\subseteq\mathbb{C}=\mathbb{R}^{2} & \to\mathbb{R}^{2}
\end{align*}
\begin{align*}
\frac{\partial^{2}a}{\partial x^{2}}+\frac{\partial^{2}a}{\partial y^{2}} & =\frac{\partial b}{\partial x\partial y}-\frac{\partial b}{\partial y\partial x}=0
\end{align*}
\begin{align*}
\Rightarrow\qquad\underbrace{\left(\frac{\partial^{2}}{\partial x^{2}}+\frac{\partial^{2}}{\partial y^{2}}\right)}_{=:\upDelta}a & =0 & \upDelta b & =0
\end{align*}
This is the \emph{Laplace equation} with the \emph{Laplace operator
}(or\emph{ Laplacian}) $\upDelta$. Solutions of the Laplace equation
are called \emph{harmonic functions}.
\item Let $\left(M,g\right)$ be a Riemannian manifold. Here exists the
Laplace-Beltrami operator $\LBO$.

\begin{itemize}
\item In the special case $M=\mathbb{R}^{n}$ we have:
\begin{align*}
\upDelta & =\frac{\partial^{2}}{\partial x_{1}^{2}}+\ldots+\frac{\partial^{2}}{\partial x_{n}^{2}}\\
\upDelta\varphi & =0
\end{align*}

\item With the Riemannian metric $g_{ij}$ we can define:
\begin{align*}
\LBO\varphi & =g^{ij}\nabla_{i}\nabla_{j}\varphi=\text{div}\left(\text{grad}\left(\varphi\right)\right)=\frac{1}{\sqrt{\det\left(g\right)}}\partial_{j}\left(\sqrt{\det\left(g\right)}g^{jk}\partial_{k}\varphi\right)
\end{align*}

\end{itemize}

This gives an elliptic equation.

\item Newton's gravitational law: Let $\varrho\left(x\right)$ be the mass
density and $\varphi\left(x\right)$ the Newtonian potential.
\begin{align*}
\upDelta\varphi & =\underbrace{-4\pi\varrho}_{\text{inhomogeneity}}
\end{align*}
Such an inhomogeneous Laplace equation is usually referred to as Poisson
equation and it is elliptic.
\item Heat flow equation (\foreignlanguage{ngerman}{Wärmeleitungsgleichung}):
Let $\varphi\left(t,x\right)$ be the temperature at time $t\in\mathbb{R}$
and position $x\in\mathbb{R}^{n}$.
\begin{align*}
\partial_{t}\varphi\left(t,x\right) & =\upDelta\varphi\left(t,x\right)
\end{align*}
This is a parabolic equation.
\item The Schrödinger equation is a parabolic equation:
\begin{align*}
\ii\hbar\partial_{t}\psi\left(t,x\right) & =\left(-\frac{\hbar^{2}}{2m}\upDelta+V\right)\psi\left(t,x\right)
\end{align*}
Additionally to the the heat flow equation there is the potential
$V$, but more important there is a factor of $\ii$ in front of the
partial derivative. The time-independent Schrödinger equation is:
\begin{align*}
E\psi\left(x\right) & =\left(-\frac{\hbar^{2}}{2m}\upDelta+V\right)\psi\left(x\right)
\end{align*}
This is similar to the Poisson equation and also elliptic.
\item The wave equation
\begin{align*}
\left(\partial_{t}^{2}-\upDelta_{x}\right)\psi\left(t,x\right) & =0
\end{align*}
is hyperbolic. We will consider it in the lecture ``Partial Differential
Equations II''.
\item Maxwell's equations: $E\left(t,x\right)$ is the electric field and
$B\left(t,x\right)$ the magnetic field.
\begin{align*}
\text{div}\left(E\right) & =4\pi\varrho &  & \text{Gauss law}\\
\text{rot}\left(E\right) & =-\partial_{t}B &  & \text{Maxwell}\\
\text{div}\left(B\right) & =0\\
\text{rot}\left(B\right) & =4\pi j-\partial_{t}E &  & \text{Faraday}
\end{align*}
This is a system of 8 partial differential equation.
\item Einstein's field equation:
\begin{align*}
R_{ij}-\frac{1}{2}Rg_{ij} & =4\pi\kappa T_{ij}
\end{align*}
This is a geometric partial differential equation. $R_{ij}$ is the
Ricci curvature, $R$ the scalar curvature and $T_{ij}$ the energy-momentum
tensor. It is a system of 10 partial differential equations.
\item Equations of relativistic quantum mechanics:
\begin{align*}
\left(-\partial_{t}^{2}+\upDelta\right)\psi & =m^{2}\psi
\end{align*}
This is the Klein-Gordon equation with the mass $m$.
\begin{align*}
\ii\gamma^{j}\partial_{j}\psi & =m\psi
\end{align*}
This is the Dirac equation, a system of 4 complex-valued or 8 real-valued
partial differential equations for a particle with spin $\frac{1}{2}$.
\item Water waves can be described by the Korteweg-de Vries equation:
\begin{align*}
\partial_{t}u+u\partial_{x}u+\partial_{x}^{3}u & =0
\end{align*}
\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \draw (-4,1) -- (4,1) (-4,-1) -- (4,-1);
  \draw[dashed] (-1,0.7) -- (-1,-0.7) (2,0.7) -- (2,-0.7);
  \draw[thick] plot[smooth,tension=.7] coordinates{(-2,-0.4) (-1.6,-0.25) (-1,0.4) (-0.4,-0.25) (0,-0.4)};
  \draw[thick,xshift=3cm] plot[smooth,tension=.7] coordinates{(-2,-0.4) (-1.6,-0.25) (-1,0.4) (-0.4,-0.25) (0,-0.4)};
  \draw[thick,decoration={markings,mark=at position 1 with {\arrow[scale=1.3]{>}};},postaction={decorate}] (0,0.1) -- (1,0.1);
  \draw plot[smooth cycle,tension=.3] coordinates{(-3.5,-0.3) (-2.75,-0.27) (-2.5,-0.2) (-2.25,0) (-2.5,0.2) (-2.75,0.27) (-3.5,0.3)};
\end{tikzpicture}
\par\end{centering}

\caption{Solitons (discovered by John Russel in 1834): When the ship suddenly
stops, the water flows on along the channel. This wave moves with
a constant speed and its shape stays the same.}


\end{figure}

\item Shock waves: Burger's equation
\begin{align*}
\partial_{t}u+u\partial_{x}u & =0
\end{align*}
is hyperbolic.
\item Turbulence can be described by the incompressible Navier-Stokes equations
for the velocity $v:\Omega\subseteq\mathbb{R}^{3}\to\mathbb{R}^{3}$.
\begin{align*}
\text{div}\left(v\right) & =0\\
\rho\partial_{t}v^{j}+\rho v^{i}\partial_{i}v^{j}-\eta\upDelta v^{j} & =-\partial_{j}P
\end{align*}
Here $\varrho$ is the gas density, $P$ the pressure and $\eta$
the viscosity.
\end{enumerate}

\section{Classification}
\begin{enumerate}[label=\Roman*)]
\item The \emph{order} of a partial differential equation is the highest
order of the derivatives in it.
\begin{align*}
\upDelta u & =f &  & \text{second order}\\
\partial_{t}\varphi & =\upDelta\varphi &  & \text{second order}\\
\partial_{t}u+u\partial_{x}u+\partial_{x}^{3}u & =0 &  & \text{third order}
\end{align*}

\item Algebraic classification:

\begin{enumerate}[label=\alph*)]
\item \emph{Linear} equations: The unknown function $u$ and its derivatives
appear only linearly.
\begin{align*}
\partial_{t}u & =u &  & \text{linear}\\
\partial_{t}u+u\partial_{x}u & =0 &  & \text{non-linear}
\end{align*}

\item Linear \emph{homogeneous} equations: If $u$ is a solution, then $\lambda u$
for $\lambda\in\mathbb{R}$ is also a solution.
\begin{align*}
\upDelta u & =0 &  & \text{linear homogeneous}\\
\upDelta u & =\varrho &  & \text{linear inhomogeneous}\\
\LBO u & =0 &  & \text{linear homogeneous}
\end{align*}

\item Linear with \emph{constant coefficients}:
\begin{align*}
\upDelta u & =\rho &  & \text{linear with constant coefficients}\\
\LBO u & =\varrho &  & \text{in general non-constant coefficients}
\end{align*}

\end{enumerate}
\item Classification by type: elliptic, parabolic, hyperbolic\\
Here we only consider scalar second order equations with $x\in\Omega\subseteq\mathbb{R}^{n}$.
\begin{align*}
F\left(x,u,\DD u,\DD^{2}u\right) & =0\\
F:\Omega\times\mathbb{R}\times\mathbb{R}^{n}\times\mathbb{R}^{n\times n} & \to\mathbb{R}
\end{align*}
\begin{align*}
A_{ij} & :=\frac{\partial F\left(x,u,p_{i},p_{ij}\right)}{\partial p_{ij}}
\end{align*}
is a symmetric $n\times n$ matrix.

\begin{itemize}
\item If $A$ is positive definite, the equation is called \emph{elliptic}.
\item If $A$ has $n-1$ positive and one negative eigenvalue, the equation
is called \emph{hyperbolic}.
\item If $A$ has $n-1$ positive eigenvalues and a non-trivial kernel,
the equation is called \emph{parabolic}.
\item If all eigenvalues are negative or $n-1$ are negative, then we replace
$F$ by $-F$.\\
All other case of \emph{mixed type} are difficult and we do not consider
them in this lecture.
\end{itemize}
\end{enumerate}

\section{Examples}

Consider the Poisson equation:
\begin{align*}
\upDelta u & =\varrho\\
F\left(x,u,\DD u,\DD^{2}u\right) & =-\varrho\left(x\right)+\delta^{ij}\partial_{ij}u\\
F\left(x,u,p_{i},p_{ij}\right) & =-\varrho\left(x\right)+\delta^{ij}p_{ij}\\
A_{ij}=\frac{\partial F\left(x,u,p_{i},p_{ij}\right)}{\partial p_{ij}} & =\delta_{ij}
\end{align*}
So we have $A=\mathbbm{1}$ and thus the equation is elliptic.\\
Now consider the inhomogeneous wave equation:
\begin{align*}
\left(\partial_{t}^{2}-\upDelta\right)\phi\left(t,x_{1},x_{2},x_{3}\right) & =\varrho
\end{align*}
\begin{align*}
F\left(x,u,p_{i},p_{ij}\right) & =\varrho\left(x\right)+\eta^{ij}p_{ij} & \eta & =\text{diag}\left(-1,1,1,1\right)
\end{align*}
So $A=\eta$ has one negative and three positive eigenvalues which
means that the equation is hyperbolic.
\begin{align*}
\partial_{t}\phi & =\upDelta\phi
\end{align*}
\begin{align*}
F\left(x,u,\DD\phi,\DD^{2}\phi\right) & =-\partial_{0}\phi+\sum_{i,j=1}^{3}\delta^{ij}\partial_{ij}\phi\\
A_{ij} & =\left(\begin{array}{cccc}
0 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}\right)
\end{align*}
Therefore the equation is parabolic.


\chapter{Distributions and Fourier Transform}


\subsubsection*{Motivation}

We want to solve partial differential equations with constant coefficients
in $\Omega=\mathbb{R}^{n}$, for example:
\begin{align*}
\left(-\partial_{t}^{2}+\upDelta\right)\phi & =0
\end{align*}
Now we make a ``plane wave ansatz'' with $t,\omega\in\mathbb{R}$
and $k,x\in\mathbb{R}^{n-1}$:
\begin{align*}
\phi\left(t,x\right) & =e^{-\ii\omega t+\ii\left\langle k,x\right\rangle }\\
\partial_{t}\phi\left(t,x\right) & =-\ii\omega\phi\left(t,x\right)\\
\partial_{j}\phi\left(t,x\right) & =\ii k_{j}\phi\left(t,x\right)
\end{align*}
This gives an algebraic equation:
\begin{align*}
\left(-\left(-\ii\omega\right)^{2}+\left(\ii k\right)^{2}\right)\phi & =0\\
\Leftrightarrow\qquad\omega^{2} & =k^{2}
\end{align*}
%DATE: Fr 19.4.13\\
We also want to differentiate non-smooth functions, e.g.:
\begin{align*}
\upDelta_{\mathbb{R}^{3}}\frac{1}{\abs x} & =-4\pi\delta\left(x\right)
\end{align*}
$\delta\left(x\right)$ is called Dirac $\delta$-distribution.


\section{The Schwartz Space and Distributions}

Laurent Schwartz was the first to investigate distributions systematically.
He was awarded the fields medal for his research.


\subsection{Definition \textmd{(Multi-Index)}}

For $\mathbb{R}^{n}$ we denote indices by $i,j,k\in\left\{ 1,\ldots,n\right\} $.
We call $\alpha=\left(i_{1},\ldots,i_{k}\right)$ with $i_{l}\in\left\{ 1,\ldots,n\right\} $
a \emph{multi-index}. $\abs{\alpha}:=k$ is called the \emph{order}
or \emph{absolute value} of the multi-index.

With this we can write differentials of order $k$ as
\begin{align}
\DD^{\alpha} & :=\frac{\partial}{\partial x^{i_{1}}}\cdots\frac{\partial}{\partial x^{i_{k}}}
\end{align}
and homogeneous polynomials of degree $k$ in the components of a
vector $x=\left(x^{1},\ldots,x^{n}\right)$ as:
\begin{align}
x^{\alpha} & :=x^{i_{1}}\cdots x^{i_{k}}
\end{align}
For $f\in C^{\infty}\left(\mathbb{R}^{n}\right)$ and $r,s\in\mathbb{N}$
we define the \emph{Schwartz norm}:
\begin{align}
\norm f_{r,s} & :=\sum_{\alpha,\abs{\alpha}\le r}\sum_{\beta,\abs{\beta}\le s}\sup_{x\in\mathbb{R}^{n}}\abs{x^{\alpha}\DD^{\beta}f\left(x\right)}
\end{align}
For example for $r=0=s$ we have:
\begin{align*}
\norm f_{0,0} & =\sup_{x\in\mathbb{R}^{n}}\abs{f\left(x\right)}=\norm f_{C^{0}}
\end{align*}



\subsection{Definition \textmd{(Schwartz Space)}}

The \emph{Schwartz space} $\mathcal{S}\left(\mathbb{R}^{n}\right)$
is the vector space of all $f\in C^{\infty}\left(\mathbb{R}^{n}\right)$
for which all Schwartz norms are finite, i.e. for all $r,s\in\mathbb{N}$
holds:
\begin{align*}
\norm f_{r,s} & <\infty
\end{align*}
This space is an infinite-dimensional vector space.\\
On a normed space $\left(E,\norm .\right)$, the topology is given
by the open sets.

$\Omega\subseteq E$ is defined as \emph{open} if holds:
\begin{align*}
\fall_{x\in\Omega}\ \exs_{\varepsilon>0}\ :\ B_{\varepsilon}\left(x\right) & \subseteq\Omega
\end{align*}
A subset $\Omega\subseteq\mathcal{S}\left(\mathbb{R}^{n}\right)$
is called \emph{open} if for every $f\in\Omega$ there is a $\varepsilon>0$
and $r,s\in\mathbb{N}$ such that holds:
\begin{align}
\left\{ g\in\mathcal{S}\Big|\norm{g-f}_{r,s}<\varepsilon\right\}  & \subseteq\Omega
\end{align}

\begin{description}
\item [{Note:}] This topology is fine, because it involves many open sets,
since the condition for open only involves the statement ``there
are $r,s\in\mathbb{N}$''.\\
Convergence $f_{n}\to f$ in $\mathcal{S}$ means that \emph{every}
open neighborhood $U$ of $f$ contains almost all $f_{n}$. For a
finer topology, the condition for a sequence to converge is stronger.
\end{description}

\subsection{Theorem \textmd{(Criterion for Convergence)}}

Convergence $f_{n}\to f$ in $\mathcal{S}$ is equivalent to the convergence
$\norm{f_{n}-f}_{r,s}\to0$ for all $r,s\in\mathbb{N}$.


\subsubsection*{Proof}

``$\Rightarrow$'': Suppose that $f_{n}\to f$ converges. By definition
of the convergence, every open neighborhood of $f$ contains almost
all $f_{n}$. For all $r,s\in\mathbb{N}$ the sets $U_{\varepsilon}^{r,s}:=\big\{ g\big|\norm{g-f}_{r,s}<\varepsilon\big\}$
are open by definition. So the inequality
\begin{align*}
\norm{f_{n}-f}_{r,s} & <\varepsilon
\end{align*}
holds for almost all $f_{n}$ and thus converges $\norm{f_{n}-f}_{r,s}\to0$.

``$\Leftarrow$'': Assume that $\norm{f_{n}-f}_{r,s}\to0$ converges
for all $r,s\in\mathbb{N}$. Let $A$ be an open neighborhood of $f$.
This means by definition that there exist $r,s\in\mathbb{N}$ and
$\varepsilon\in\mathbb{R}_{>0}$ with $U_{\varepsilon}^{r,s}\subseteq A$.
For this $\left(r,s\right)$ we know that $\norm{f_{n}-f}_{r,s}\to0$
converges. Hence there exists a $N\in\mathbb{N}$ such that $\norm{f_{n}-f}_{r,s}<\varepsilon$
holds for all $n\in\mathbb{N}_{>N}$, in other words $f_{n}\in U_{\varepsilon}^{r,s}\subseteq A$.
So $f_{n}\to f$ converges in $\mathcal{S}$.\qqed

A vector space with a topology generated by a family of norms or semi-norms
is a \emph{uniform space} and is called \emph{topological vector space}.


\subsection{Definition \textmd{(Tempered Distribution)}}

Let $\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ be the dual space
of $\mathcal{S}\left(\mathbb{R}^{n}\right)$. It is called the space
of \emph{tempered distributions} (\foreignlanguage{ngerman}{temperierte
Distributionen}).

In linear algebra for a finite-dimensional vector space $V$, the
dual space $V^{*}=L\left(V,\mathbb{R}\right)$ is the space of linear
functionals. $V^{*}$ is again a vector space with $\dim\left(V^{*}\right)=\dim\left(V\right)$.\\
Here $\mathcal{S}\left(\mathbb{R}^{n}\right)$ is an infinite-dimensional
vector space with a topology. $\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$
is the space of all \emph{continuous} linear functionals.

In a Banach space $\left(E,\norm .\right)$ holds: A linear functional
$A:E\to\mathbb{R}$ is continuous if and only if $A$ is bounded,
i.e. $\abs{Au}\le c\norm U$ for all $u\in E$.


\subsection{Lemma \textmd{(Criterion for Continuity)}\label{sub:Lem-continuous_functional}}

A linear functional $T:\mathcal{S}\left(\mathbb{R}^{n}\right)\to\mathbb{R}$
is continuous if and only if there are $r,s\in\mathbb{N}$ and a $c\in\mathbb{R}_{>0}$
such that holds:
\begin{align}
\abs{Tf} & \le c\norm f_{r,s}\label{eq:T_bounded}
\end{align}



\subsubsection*{Proof}

``$\Leftarrow$'': Assume that (\ref{eq:T_bounded}) holds for some
$r,s\in\mathbb{N}$. We want to show that $T$ is continuous. To this
end, let $f_{n}\to f$ be a convergent series in $\mathcal{S}$. Our
task is to show that $Tf_{n}\to Tf$ converges.\\
The convergence $f_{n}\to f$ implies $\norm{f_{n}-f}_{r',s'}\to0$
for all $r',s'\in\mathbb{N}$ and thus in particular for $r,s$ satisfying
the inequality (\ref{eq:T_bounded}). By linearity follows:
\begin{align*}
\abs{Tf_{n}-Tf} & =\abs{T\left(f_{n}-f\right)}\le c\norm{f_{n}-f}_{r,s}\xrightarrow{n\to\infty}0
\end{align*}
So $T$ maps convergent sequences to convergent sequences and is thus
continuous.

``$\Rightarrow$'': Assume that $T$ is continuous. Then the preimage
of open sets is open, in particular $T^{-1}\left(B_{1}\left(0\right)\right)\subseteq\mathcal{S}\left(\mathbb{R}^{n}\right)$
is open. So there exist $r,s\in\mathbb{N}$ and a $\varepsilon\in\mathbb{R}_{>0}$
such that holds:
\begin{align*}
T^{-1}\left(B_{1}\left(0\right)\right) & \supseteq U_{\varepsilon}^{r,s}:=\big\{ g\big|\norm g_{r,s}<\varepsilon\big\}
\end{align*}
This implies:
\begin{align*}
\norm g_{r,s}<\varepsilon & \quad\Rightarrow\quad g\in T^{-1}\left(B_{1}\left(0\right)\right)
\end{align*}
Now $g\in T^{-1}\left(B_{1}\left(0\right)\right)$ means $\abs{Tg}<1$.
For any $f\in\mathcal{S}\left(\mathbb{R}^{n}\right)$ apply this to
$g=\frac{f}{\lambda}$ with $\lambda\in\mathbb{R}_{>0}$.
\begin{align*}
\frac{1}{\lambda}\norm f_{r,s}<\varepsilon & \quad\Rightarrow\quad\frac{1}{\lambda}\abs{Tf}<1\qquad/\cdot\lambda\\
\norm f_{r,s}<\lambda\varepsilon & \quad\Rightarrow\quad\abs{Tf}<\lambda
\end{align*}
Now choose $\lambda=\frac{2}{\varepsilon}\norm f_{r,s}$, so the left
side holds, which implies:
\begin{align*}
\abs{Tf} & <\frac{2}{\varepsilon}\norm f_{r,s}\qquad\fall_{f\in\mathcal{S}\left(\mathbb{R}^{n}\right)}
\end{align*}
\qqed


\subsection{Example \textmd{(\texorpdfstring{$\delta$}{Delta}-Distribution)}}
\begin{enumerate}[label=\alph*)]
\item Consider the following functional:
\begin{align}
\delta:\mathcal{S}\left(\mathbb{R}\right) & \to\mathbb{R}\\
f & \mapsto f\left(0\right)\nonumber 
\end{align}
This is obviously linear.
\begin{align*}
\abs{\delta\left(f\right)} & =\abs{f\left(0\right)}\le\sup_{\mathbb{R}}\abs f=\norm f_{0,0}
\end{align*}
Hence $\delta$ is continuous, which means that $\delta\in\mathcal{S}^{*}\left(\mathbb{R}\right)$
is a tempered distribution.\\
A convenient \emph{notation} with $f\in\mathcal{S}\left(\mathbb{R}\right)$
is:
\begin{align*}
\delta\left(f\right) & =\int_{\mathbb{R}}f\left(x\right)\delta\left(x\right)\dd x
\end{align*}

\item In higher dimension $n\in\mathbb{N}$ we define:
\begin{align*}
\delta:\mathcal{S}\left(\mathbb{R}^{n}\right) & \to\mathbb{R}\\
f & \mapsto f\left(0\right)
\end{align*}
Again holds $\abs{\delta\left(f\right)}\le\norm f_{0,0}$. The physicists'
notation for this is:
\begin{align*}
\delta\left(f\right) & =\int_{\mathbb{R}^{n}}f\left(x\right)\delta^{\left(n\right)}\left(x\right)\dd^{n}x\\
\delta^{\left(n\right)}\left(x\right) & =\delta\left(x^{1}\right)\cdots\delta\left(x^{n}\right)
\end{align*}

\end{enumerate}

\subsubsection*{Remark}

$\delta$ can also be introduced as a \emph{measure} on $\mathbb{R}^{n}$,
the \emph{Dirac measure}. For $A\subseteq\mathbb{R}^{n}$ define:
\begin{align}
\delta\left(x\right) & =\begin{cases}
1 & \text{if }0\in A\\
0 & \text{otherwise}
\end{cases}
\end{align}
Then for $f\in C^{0}\left(\mathbb{R}^{n}\right)$ the expression 
\begin{align*}
\int_{\mathbb{R}^{n}}f\left(x\right)\dd\delta\left(x\right) & =f\left(0\right)
\end{align*}
makes mathematical sense as an integral.

This is useful because convergence theorems and so on from measure
theory are available. The problem is, that this does not work for
every distribution and thus is not general enough for most purposes,
e.g. the derivative $\delta'\left(x\right)$ is a distribution, but
cannot be written as a measure.


\subsection{Example \textmd{(Integral Operator)}}
\begin{enumerate}[label=\alph*)]
\item Consider $g\in C^{\infty}\left(\mathbb{R}^{n}\right)$ with at most
polynomial growth, i.e. there are $c\in\mathbb{R}_{>0}$ and $r\in\mathbb{N}$
such that holds:
\begin{align*}
\abs{g\left(x\right)} & \le c\left(1+\abs x^{r}\right)
\end{align*}
Now define:
\begin{align*}
T_{g}:\mathcal{S}\left(\mathbb{R}^{n}\right) & \to\mathbb{R}\\
f & \mapsto\int_{\mathbb{R}^{n}}g\left(x\right)f\left(x\right)\dd^{n}x
\end{align*}
The integral here is just the Lebesgue integral and it exists:\\
For $f\in\mathcal{S}\left(\mathbb{R}^{n}\right)$ holds $\norm f_{r,s}<\infty$
for all $r,s\in\mathbb{N}$.
\begin{align*}
\sup_{\mathbb{R}}\abs f+\sup_{x\in\mathbb{R}}\left(\abs x^{\tilde{r}}\cdot\abs{f\left(x\right)}\right) & \le\norm f_{\tilde{r},0}<\infty\\
\Rightarrow\qquad\sup_{x\in\mathbb{R}}\left(\left(1+\abs x^{\tilde{r}}\right)\abs{f\left(x\right)}\right) & \le\norm f_{\tilde{r},0}\\
\Rightarrow\qquad\abs{f\left(x\right)} & \le\frac{\norm f_{\tilde{r},0}}{1+\abs x^{\tilde{r}}}\qquad\fall_{\tilde{r}\in\mathbb{N}}
\end{align*}
So we get:
\begin{align*}
T_{g}f & =\int g\left(x\right)f\left(x\right)\dd^{n}x\le\int c\left(1+\abs x^{r}\right)\frac{\norm f_{\tilde{r},0}}{\left(1+\abs x^{\tilde{r}}\right)}\dd^{n}x=\\
 & \sr ={\text{polar coordinates}}{\rho:=\abs x}c\norm f_{\tilde{r},0}\underbrace{\mu\left(S^{n-1}\right)}_{\text{volume of unit sphere}}\int_{0}^{\infty}\rho^{n-1}\frac{1+\rho^{r}}{1+\rho^{\tilde{r}}}\dd\rho\stackrel{\tilde{r}>r+n}{<}\infty
\end{align*}
This is finite if and only if the integrand decays faster than $\rho^{-1}$,
i.e. $n-1+r-\tilde{r}<-1$ and thus $\tilde{r}>n+r$. Since $\tilde{r}\in\mathbb{N}$
is arbitrary, the integral exists.\\
Continuity: The previous estimate implies with $\tilde{r}=n+r+1$:
\begin{align*}
\abs{T_{g}f} & \le C\left(g,n\right)\norm f_{\tilde{r},0}
\end{align*}
Thus $T_{g}\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ is a tempered
distribution.
\item Chose $g\left(x\right)=e^{x}$ and define:
\begin{align*}
T_{g}f & :=\int_{-\infty}^{\infty}f\left(x\right)e^{x}\dd x
\end{align*}
This is \emph{not} a well-defined tempered distribution. Namely, choose:
\begin{align*}
f\left(x\right) & =\frac{1}{\cosh\left(\frac{x}{2}\right)}
\end{align*}
\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
 \begin{axis}[width=16cm, axis equal image, axis x line=middle, axis y line=middle, xlabel=$x$, domain=-4:4, ymin=0, ymax=1.2, samples=300]
  \addplot[mark=none] {1/cosh(x/2)};
  \addlegendentry{$f(x)$}
 \end{axis}
\end{tikzpicture} 
\par\end{centering}

\caption{$f\left(x\right)$ decays rapidly.}
\end{figure}
$f\left(x\right)$ and all its derivatives decay rapidly (exponentially
fast $\sim e^{-\frac{x}{2}}$) at $\pm\infty$, so $f\in\mathcal{S}$
is a Schwartz function. But $T_{g}f$ diverges:
\begin{align*}
T_{g}f & =\int_{-\infty}^{\infty}\frac{e^{x}}{\cosh\left(\frac{x}{2}\right)}\dd x=+\infty
\end{align*}

\end{enumerate}
%DATE: Mi 24.4.13


\subsection{Remark \textmd{(Schwartz Functions as Distributions)}}

The mapping
\begin{align*}
T:\mathcal{S}\left(\mathbb{R}^{n}\right) & \to\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)\\
f & \mapsto T_{f}
\end{align*}
is injective.


\subsubsection*{Proof}

If $T$ was injective, there were $f_{1},f_{2}\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
with $T_{f_{1}}=T_{f_{2}}$ and $f_{1}\not=f_{2}$. By linearity this
would imply $T_{g}=0$ with $g=f_{1}-f_{2}\not=0$ and we could choose
a $y\in\mathbb{R}^{n}$ with $g\left(y\right)\not=0$. By continuity
follows $g>0$ or $g<0$ in a neighborhood $U$ of $y$. Now choose
a test function $h$ with $\text{supp}\left(h\right)\subseteq U$
and $h\ge0$. Then follows the contradiction:
\begin{align*}
0=T_{g}\left(h\right) & =\int_{\mathbb{R}^{n}}g\left(x\right)h\left(x\right)\dd^{n}x\not=0
\end{align*}
\qqed

Thus we can regard distributions as ``generalized functions''. Namely
we identify a function $g$ with $T_{g}$. (Later on many people often
do not distinguish between $g$ and $T_{g}$.)


\subsubsection*{Operations on Schwartz Functions and Distributions}
\begin{itemize}
\item $\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ is a vector space with
addition $T+S$ and scalar multiplication $\alpha\cdot f$ for distributions
$T,S\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ and $\alpha\in\mathbb{R}$.
\item Multiplication of a distribution by a Schwartz function is defined
for $T\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ and $f,g\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
as:
\begin{align}
\left(fT\right)\left(g\right) & :=T\left(f\cdot g\right)
\end{align}
This is well defined, because $f\cdot g$ is again a Schwartz function
and for $h\in\mathcal{S}\left(\mathbb{R}^{n}\right)$ holds:
\begin{align*}
\left(fT_{h}\right)\left(g\right) & \stackrel{\text{definition of }fT_{h}}{=}T_{h}\left(f\cdot g\right)\stackrel{\text{definition of }T_{h}}{=}\int_{\mathbb{R}^{n}}h\left(x\right)\left(f\cdot g\right)\left(x\right)\dd^{n}x=\\
 & =\int_{\mathbb{R}^{n}}\left(f\cdot h\right)\left(x\right)g\left(x\right)\dd^{n}x=T_{fh}\left(g\right)
\end{align*}
So this definition is extends the multiplication of Schwartz functions
to distributions. But we still have to show, that this operation gives
a continuous functional.
\end{itemize}

\subsection{Definition \textmd{(regular/singular distribution)}}

A tempered distribution $T$ is called \emph{regular} distribution
if there is a $g\in L_{\text{loc}}^{1}\left(\mathbb{R}^{n}\right)$
(locally integrable, i.e. integrable on every compact interval) with
$T=T_{g}$. Otherwise, $T$ is called \emph{singular}.

For example $\delta\left(x\right)$ is singular.


\subsection{Lemma \textmd{(Multiplication of a Distribution by a Schwartz Function)}}

Let $f\in\mathcal{S}\left(\mathbb{R}^{n}\right)$ be a Schwartz function
and $T\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ a distribution.
Then $fT$ is a \emph{continuous} linear functional on $\mathcal{S}\left(\mathbb{R}^{n}\right)$,
in other words $fT\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$
is also a distribution.


\subsubsection*{Proof}

According to Lemma \ref{sub:Lem-continuous_functional}, our task
is to show that there are $r,s\in\mathbb{N}$ and a $C\in\mathbb{R}_{>0}$
with:
\begin{align*}
\abs{\left(fT\right)\left(g\right)} & \le C\norm g_{r,s}
\end{align*}
Since $T$ is continuous, there exist $r,s\in\mathbb{N}$ and a $\tilde{C}\in\mathbb{R}_{>0}$
with:
\begin{align*}
\abs{T\left(fg\right)} & \le\tilde{C}\norm{fg}_{r,s}
\end{align*}
Thus it remains to show that there is a $c\left(r,s\right)\in\mathbb{R}_{>0}$
such that for all $f,g\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
holds:
\begin{align*}
\norm{fg}_{r,s} & \le c\left(r,s\right)\norm f_{r,s}\cdot\norm g_{r,s}
\end{align*}
This inequality can be proven by induction in $s$.

Induction basis $s=0$:
\begin{align*}
\norm{fg}_{r,0} & =\sum_{\abs{\alpha}\le r}\sup_{x\in\mathbb{R}^{n}}\left(\abs{x^{\alpha}f\left(x\right)g\left(x\right)}\right)\le\\
 & \le\sup_{y\in\mathbb{R}^{n}}\abs{g\left(y\right)}\sum_{\abs{\alpha}\le r}\sup_{x\in\mathbb{R}^{n}}\left(\abs{x^{\alpha}f\left(x\right)}\right)=\\
 & =\norm g_{0,0}\cdot\norm f_{r,0}\le\norm g_{r,0}\cdot\norm f_{r,0}
\end{align*}
Induction step $s\leadsto s+1$: Assume that the statement holds for
a $s\in\mathbb{N}$ for all $r\in\mathbb{N}$. Let $\beta$ be a multi-index
with $\abs{\beta}=s+1$, i.e. $\beta=\left(i_{1},\ldots,i_{s+1}\right)$.
Now set:
\begin{align*}
\hat{\beta} & :=\left(i_{1},\ldots,i_{s}\right) & j & :=i_{s+1} & \DD^{\beta} & =\DD^{\hat{\beta}}\frac{\partial}{\partial x^{j}}
\end{align*}
It holds:
\begin{align*}
\DD^{\beta}\left(fg\right) & =\DD^{\hat{\beta}}\frac{\partial}{\partial x^{j}}\left(fg\right)=\DD^{\hat{\beta}}\left(\left(\frac{\partial}{\partial x^{j}}f\right)g+f\left(\frac{\partial}{\partial x^{j}}g\right)\right)
\end{align*}
\begin{align*}
\norm{fg}_{r,s+1} & =\norm{f\cdot g}_{r,s}+\sum_{\sr{}{\abs{\alpha}\le r}{\abs{\beta}=s+1}}\sup_{x\in\mathbb{R}}\abs{x^{\alpha}\DD^{\beta}\left(f\cdot g\right)\left(x\right)}=\\
 & =\norm{fg}_{r,s}+\sum_{\sr{}{\abs{\alpha}\le r}{\abs{\hat{\beta}}=s}}\sum_{j=1}^{n}\sup_{x\in\mathbb{R}}\abs{x^{\alpha}\DD^{\hat{\beta}}\left(\left(\frac{\partial}{\partial x^{j}}f\left(x\right)\right)g\left(x\right)+f\left(x\right)\left(\frac{\partial}{\partial x^{j}}g\left(x\right)\right)\right)}\le\\
 & \sr{\le}{\text{induction}}{\text{hypothesis}}c\left(r,s\right)\norm f_{r,s}\norm g_{r,s}+\sum_{j=1}^{n}c\left(r,s\right)\left(\norm{\frac{\partial}{\partial x^{j}}f}_{r,s}\norm g_{r,s}+\norm f_{r,s}\norm{\frac{\partial}{\partial x^{j}}g}_{r,s}\right)\le\\
 & \le c\left(r,s\right)\norm f_{r,s}\norm g_{r,s}+n\cdot c\left(r,s\right)\left(\norm f_{r,s+1}\norm g_{r,s}+\norm f_{r,s}\norm g_{r,s+1}\right)\le\\
 & \le\underbrace{\left(2n+1\right)c\left(r,s\right)}_{=c\left(r,s+1\right)}\norm f_{r,s+1}\norm g_{r,s+1}
\end{align*}


\qqed


\subsection{Example \textmd{(Derivative of the \texorpdfstring{$\delta$}{Delta}-Distribution)}}

We make a formal computation:
\begin{align*}
\int_{\mathbb{R}}\delta'\left(x\right)f\left(x\right)\dd x & =\int_{\mathbb{R}}\left(\frac{\dd}{\dd x}\delta\left(x\right)\right)f\left(x\right)\dd x\sr ={\text{integration}}{\text{by parts}}-\int_{\mathbb{R}}\delta\left(x\right)f'\left(x\right)\dd x=-f'\left(0\right)
\end{align*}
This motivates us to \emph{define}:
\begin{align}
\delta':\mathcal{S}\left(\mathbb{R}\right) & \to\mathbb{R}\\
f & \mapsto-f'\left(0\right)\nonumber 
\end{align}
This is obviously linear and it is continuous, because for all $f\in\mathcal{S}\left(\mathbb{R}\right)$
holds:
\begin{align*}
\abs{\delta'\left(f\right)} & =\abs{f'\left(0\right)}\le\norm f_{0,1}
\end{align*}
Hence we have $\delta'\in\mathcal{S}^{*}\left(\mathbb{R}\right)$.


\subsubsection*{Remark}

In contrast to $\delta$, the distribution $\delta'$ cannot be introduced
as a measure:
\begin{align*}
\delta\left(\Omega\right) & =\begin{cases}
1 & \text{if }0\in\Omega\\
0 & \text{otherwise}
\end{cases}\\
\delta'\left(\Omega\right) & =?
\end{align*}



\subsection{Definition \textmd{(Distributional Derivative, Convolution)}}
\begin{itemize}
\item For a tempered distribution $T\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$
we define the \emph{distributional derivative} $\DD^{\alpha}T$ by:
\begin{align}
\left(\DD^{\alpha}T\right)\left(f\right) & :=\left(-1\right)^{\abs{\alpha}}T\left(\DD^{\alpha}f\right)
\end{align}
$\DD^{\alpha}T$ is a distribution, since it is a continuous functional:
\begin{align*}
\abs{\left(\DD^{\alpha}T\right)\left(f\right)} & =\abs{T\left(\DD^{\alpha}f\right)}\stackrel{T\text{ continuous}}{\le}C\norm{\DD^{\alpha}f}_{r,s}\le C\norm f_{r,s+\abs{\alpha}}
\end{align*}
So we have a mapping $\DD^{\alpha}:\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)\to\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$.
\item The convolution (\foreignlanguage{ngerman}{Faltung}) for $f,g\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
is defined as:
\begin{align}
\left(f*g\right)\left(x\right) & :=\int_{\mathbb{R}^{n}}f\left(x-y\right)g\left(y\right)\dd^{n}y
\end{align}

\end{itemize}

\subsection{Lemma \textmd{(Commutativity and Associativity of the Convolution)}}

The convolution is commutative and associative, i.e. for $f,g,h\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
holds:
\begin{align}
f*g & =g*f & f*\left(g*h\right) & =\left(f*g\right)*h
\end{align}



\subsubsection*{Proof}

\begin{align*}
\left(f*g\right)\left(x\right) & =\int_{\mathbb{R}^{n}}f\left(x-y\right)g\left(y\right)\dd^{n}y\sr ={z:=x-y}{\dd^{n}z=\dd^{n}y}\int_{\mathbb{R}^{n}}f\left(z\right)g\left(x-z\right)\dd^{n}z=\left(g*f\right)\left(x\right)
\end{align*}
Associativity follows analogously using Fubini's theorem, which can
be applied, since the function $\left(y,z\right)\mapsto f\left(x-y\right)g\left(y-z\right)h\left(z\right)$
is an element of $\mathcal{S}\left(\mathbb{R}^{2n}\right)\subseteq L^{1}\left(\mathbb{R}^{2n}\right)$:
\begin{align*}
f*\left(g*h\right)\left(x\right) & =\int_{\mathbb{R}^{n}}f\left(x-y\right)\left(g*h\right)\left(y\right)\dd^{n}y=\\
 & =\int_{\mathbb{R}^{n}}f\left(x-y\right)\left(\int_{\mathbb{R}^{n}}g\left(y-z\right)h\left(z\right)\dd^{n}z\right)\dd^{n}y=\\
 & \sr ={\tilde{y}:=y-z}{\dd^{n}\tilde{y}=\dd y}\int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}f\left(x-\tilde{y}-z\right)g\left(\tilde{y}\right)h\left(z\right)\dd^{n}\tilde{y}\dd^{n}z=\\
 & =\int_{\mathbb{R}^{n}}\left(\int_{\mathbb{R}^{n}}f\left(x-z-\tilde{y}\right)g\left(\tilde{y}\right)\dd^{n}\tilde{y}\right)h\left(z\right)\dd^{n}z=\\
 & =\int_{\mathbb{R}^{n}}\left(f*g\right)\left(x-z\right)h\left(z\right)\dd^{n}z=\\
 & =\left(f*g\right)*h\left(x\right)
\end{align*}
\qqed


\subsection{Proposition \textmd{(Convolution is Continuous)}}

The convolution $*:\mathcal{S}\times\mathcal{S}\to\mathcal{S}$ is
continuous.


\subsubsection*{Proof}

\begin{align*}
\norm{f*g}_{r,s} & =\sum_{\sr{}{\abs{\alpha}\le r}{\abs{\beta}\le s}}\sup_{x\in\mathbb{R}^{n}}\abs{x^{\alpha}\DD_{x}^{\beta}\int_{\mathbb{R}^{n}}f\left(x-y\right)g\left(y\right)\dd^{n}y}
\end{align*}
The derivative may be commuted with the integral:
\begin{align*}
\frac{\partial}{\partial x_{j}}\left(f*g\right)\left(x\right) & =\lim_{\varepsilon\searrow0}\frac{1}{\varepsilon}\left(\left(f*g\right)\left(x+\varepsilon e_{j}\right)-\left(f*g\right)\left(x\right)\right)=\\
 & =\lim_{\varepsilon\searrow0}\int_{\mathbb{R}^{n}}\frac{f\left(x+\varepsilon e_{j}-y\right)-f\left(x-y\right)}{\varepsilon}g\left(y\right)\dd^{n}y
\end{align*}
Using the estimate
\begin{align*}
\frac{f\left(x+\varepsilon e_{j}-y\right)-f\left(x-y\right)}{\varepsilon} & =\frac{1}{\varepsilon}\int_{0}^{1}\frac{\dd}{\dd\tau}\left(f\left(x+\varepsilon\tau e_{j}-y\right)\right)\dd\tau=\\
 & =\frac{1}{\varepsilon}\int_{0}^{1}\left(\partial_{j}f\right)\left(x+\varepsilon\tau e_{j}-y\right)\varepsilon\dd\tau\\
\Rightarrow\qquad\abs{\frac{f\left(x+\varepsilon e_{j}-y\right)-f\left(x-y\right)}{\varepsilon}} & \le\sup_{\mathbb{R}^{n}}\abs{\partial_{j}f}\le\norm f_{0,1}
\end{align*}
we get:
\begin{align*}
\abs{\frac{f\left(x+\varepsilon e_{j}-y\right)-f\left(x-y\right)}{\varepsilon}g\left(y\right)} & \le\norm f_{0,1}\cdot\abs{g\left(y\right)}\in L^{1}\left(\mathbb{R}^{n}\right)
\end{align*}
Thus the dominated convergence theorem implies:
\begin{align*}
\frac{\partial}{\partial x_{j}}\left(f*g\right)\left(x\right) & =\int_{\mathbb{R}^{n}}\frac{\partial}{\partial x^{j}}f\left(x-y\right)g\left(y\right)\dd^{n}y
\end{align*}
By induction follows:
\begin{align*}
x^{\alpha}\DD^{\beta}\int_{\mathbb{R}}f\left(x-y\right)g\left(y\right)\dd^{n}y & =x^{\alpha}\int_{\mathbb{R}^{n}}\left(\DD^{\beta}f\right)\left(x-y\right)g\left(y\right)\dd^{n}y
\end{align*}
Now we treat the $x^{\alpha}$:
\begin{align*}
x^{\alpha} & =\left(\left(x-y\right)+y\right)^{\alpha}=\sum_{\gamma,\delta\text{ with }\alpha=\gamma+\delta}c_{\gamma\delta}\left(x-y\right)^{\gamma}\cdot y^{\delta}
\end{align*}
Here holds $\abs{\gamma},\abs{\delta}\le\abs{\alpha}$. Now we can
estimate:
\begin{align*}
\abs{\left(x-y\right)^{\gamma}\DD^{\beta}f\left(x\right)} & \le\norm f_{r,s}
\end{align*}
Hence we get:
\begin{align*}
\norm{f*g}_{r,s} & \le c\left(r,s\right)\norm f_{r,s}\sum_{\delta}\int_{\mathbb{R}^{n}}\abs{y^{\delta}g\left(y\right)}\cdot\frac{\left(1+\abs y\right)^{n+1}}{\left(1+\abs y\right)^{n+1}}\dd^{n}y\le\\
 & \le c\left(r,s\right)\norm f_{r,s}\norm g_{n+1+r,0}\underbrace{\int_{\mathbb{R}^{n}}\frac{\dd^{n}y}{\left(1+\abs y\right)^{n+1}}}_{<\infty}
\end{align*}
\qqed


\subsection{Definition \textmd{(Convolution with Distribution)}}

How can we define the convolution of $T\in S^{*}\left(\mathbb{R}^{n}\right)$
with $f\in S\left(\mathbb{R}^{n}\right)$? $f*T_{g}$ should be equal
to $T_{f*g}$. For $h\in\mathcal{S}\left(\mathbb{R}^{n}\right)$ holds:
\begin{align*}
T_{f*g}\left(h\right) & =\int_{\mathbb{R}^{n}}\left(f*g\right)\left(x\right)\cdot h\left(x\right)\dd^{n}x=\\
 & =\int_{\mathbb{R}^{n}}\left(\int_{\mathbb{R}^{n}}f\left(x-y\right)g\left(y\right)\dd^{n}y\right)\cdot h\left(x\right)\dd^{n}x
\end{align*}
By Fubini's theorem we may interchange the order of integration to
get:
\begin{align*}
T_{f*g}\left(h\right) & =\int_{\mathbb{R}^{n}}g\left(y\right)\left(\int_{\mathbb{R}^{n}}f\left(x-y\right)\cdot h\left(x\right)\dd^{n}x\right)\dd^{n}y=\\
 & \sr ={\tilde{f}\left(z\right):=f\left(-z\right)}{}\int_{\mathbb{R}^{n}}g\left(y\right)\left(\int_{\mathbb{R}^{n}}\tilde{f}\left(y-x\right)\cdot h\left(x\right)\dd^{n}x\right)\dd^{n}y=\\
 & =\int_{\mathbb{R}^{n}}g\left(y\right)\left(\tilde{f}*h\right)\left(y\right)\dd^{n}y=T_{g}\left(\tilde{f}*h\right)
\end{align*}
So for a distribution $T\in S^{*}\left(\mathbb{R}^{n}\right)$ we
define the \emph{convolution} as:
\begin{align}
*:\mathcal{S}\left(\mathbb{R}^{n}\right)\times\mathcal{S}^{*}\left(\mathbb{R}^{n}\right) & \to\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)\\
\left(f*T\right)\left(h\right) & :=T\left(\tilde{f}*h\right)\nonumber 
\end{align}


For $S,T\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$, $S*T$ and
$S\cdot T$ are ill-defined in general. For example $\delta\left(x\right)\cdot\delta\left(x\right)$
makes no sense, as well as $T_{f}*T_{f}$ for $f=1$.


\section{The Fourier Transform}

First consider the Fourier transform on $\mathcal{S}\left(\mathbb{R}^{n}\right)$
and later on $\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$.


\subsection{Definition \textmd{(Fourier Transform)}}

Define linear functionals $\mathcal{F}$ and $\overline{\mathcal{F}}$
on $\mathcal{S}$:
\begin{align}
\left(\mathcal{F}f\right)\left(p\right) & :=\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}e^{-\ii px}f\left(x\right)\dd^{n}x\\
\left(\overline{\mathcal{F}}f\right)\left(x\right) & :=\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}e^{\ii px}f\left(p\right)\dd^{n}p
\end{align}
The integrals are well-defined and finite, because $f$ has suitable
decay properties at infinity.

An alternative convention, which is not convenient here, because it
has less symmetry, is:
\begin{align*}
\left(\mathcal{F}f\right)\left(p\right) & :=\int_{\mathbb{R}^{n}}e^{-\ii px}f\left(x\right)\dd^{n}x\\
\left(\overline{\mathcal{F}}f\right)\left(x\right) & :=\frac{1}{\left(2\pi\right)^{n}}\int_{\mathbb{R}^{n}}e^{\ii px}f\left(p\right)\dd^{n}p
\end{align*}


%DATE: Fr 26.4.13


\subsection{Proposition \textmd{(Fourier Transform)}}

$\mathcal{F}$ and $\overline{\mathcal{F}}$ are well-defined linear
operators from $\mathcal{S}\left(\mathbb{R}^{n}\right)$ to $\mathcal{S}\left(\mathbb{R}^{n}\right)$.


\subsubsection*{Proof}

The linearity is clear. We still have to show, that all norms $\norm{\mathcal{F}f}_{r,s}$
are finite. First consider the norm $\norm ._{0,0}$:
\begin{align*}
\abs{\left(\mathcal{F}f\right)\left(p\right)} & \le\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\abs{f\left(x\right)}\cdot\frac{\left(1+\abs x\right)^{n+1}}{\left(1+\abs x\right)^{n+1}}\dd^{n}x\le\\
 & \le\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\norm f_{n+1,0}\underbrace{\int_{\mathbb{R}^{n}}\frac{\dd^{n}x}{\left(1+\abs x\right)^{n+1}}}_{<\infty}\le c\norm f_{n+1,0}
\end{align*}
Now we consider $\abs{p^{\alpha}\DD^{\beta}\left(\mathcal{F}f\right)\left(p\right)}$.
\begin{align*}
\frac{\partial}{\partial p^{j}}\left(\mathcal{F}f\right)\left(p\right) & =\frac{\partial}{\partial p^{j}}\int e^{-\ii px}f\left(x\right)\dd^{n}x=\ldots=\int\left(\frac{\partial}{\partial p^{j}}e^{-\ii px}\right)f\left(x\right)\dd^{n}x=\\
 & =\int\left(-\ii x^{j}\right)e^{-\ii px}f\left(x\right)\dd^{n}x
\end{align*}
That the derivative and the integral can be interchanged is shown
as follows:
\begin{align*}
\frac{\partial}{\partial p^{j}}\int e^{-\ii px}f\left(x\right)\dd^{n}x & =\lim_{\varepsilon\searrow0}\int\frac{e^{-\ii\left(p+\varepsilon e_{j}\right)x}-e^{-\ii px}}{\varepsilon}f\left(x\right)\dd^{n}x\\
\frac{e^{-\ii\left(p+\varepsilon e_{j}\right)x}-e^{-\ii px}}{\varepsilon} & =\frac{1}{\varepsilon}\int_{0}^{1}\frac{\dd}{\dd\tau}e^{-\ii\left(p+\varepsilon\tau e_{j}\right)x}\dd\tau=-\ii e_{j}x\int_{0}^{1}e^{-\ii\left(p+\varepsilon\tau e_{j}\right)x}\dd\tau\\
\Rightarrow\qquad\abs{\frac{e^{-\ii\left(p+\varepsilon e_{j}\right)x}-e^{-\ii px}}{\varepsilon}} & \le\norm x\cdot\int_{0}^{1}\underbrace{\abs{e^{-\ii\left(p+\varepsilon\tau e_{j}\right)x}}}_{=1}\dd\tau=\norm x\\
\abs{\frac{e^{-\ii\left(p+\varepsilon e_{j}\right)x}-e^{-\ii px}}{\varepsilon}f\left(x\right)} & \le\norm x\cdot\abs{f\left(x\right)}\le\frac{\norm x}{1+\norm x^{n+2}}\norm f_{n+2,0}=:h\left(x\right)\in L^{1}\left(\mathbb{R}^{n}\right)
\end{align*}
This allows us to apply the dominated convergence theorem to take
the limit $\varepsilon\to0$ inside the integral. Iteration of this
process gives:
\begin{align*}
\DD^{\beta}\left(\mathcal{F}f\right)\left(p\right) & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}}\left(-\ii\right)^{\abs{\beta}}x^{\beta}f\left(x\right)e^{-\ii px}\dd^{n}x\\
p^{\alpha}\DD^{\beta}\left(\mathcal{F}f\right)\left(p\right) & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}}\left(-\ii\right)^{\abs{\beta}}x^{\beta}f\left(x\right)p^{\alpha}e^{-\ii px}\dd^{n}x\\
p^{j}e^{-\ii px} & =\ii\frac{\partial}{\partial x^{j}}e^{-\ii px}\\
\Rightarrow\qquad p^{\alpha}e^{-\ii px} & =\ii^{\abs{\alpha}}\DD_{x}^{\alpha}e^{-\ii px}\\
p^{\alpha}\DD^{\beta}\left(\mathcal{F}f\right)\left(p\right) & =\frac{\left(-\ii\right)^{\abs{\beta}}\ii^{\abs{\alpha}}}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\underbrace{\left(x^{\beta}f\left(x\right)\right)}_{\text{rapid decay}}\left(\DD_{x}^{\alpha}e^{-\ii px}\right)\dd^{n}x=\\
 & \sr ={\text{integration}}{\text{by parts}}\frac{\left(-\ii\right)^{\abs{\beta}}\ii^{\abs{\alpha}}}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\left(-1\right)^{\abs{\alpha}}\DD_{x}^{\alpha}\left(x^{\beta}f\left(x\right)\right)e^{-\ii px}\dd^{n}x=\\
 & =\frac{\left(-\ii\right)^{\abs{\alpha}+\abs{\beta}}}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\DD_{x}^{\alpha}\left(x^{\beta}f\left(x\right)\right)e^{-\ii px}\dd^{n}x
\end{align*}
From the computation we did earlier in the proof we know:
\begin{align*}
\abs{\DD_{x}^{\alpha}\left(\mathcal{F}f\right)\left(p\right)} & \le C\norm{\DD_{x}^{\alpha}\left(x^{\beta}f\right)}_{n+1,0}\le\tilde{C}\left(\alpha,\beta\right)\norm f_{\abs{\beta}+n+1,\abs{\alpha}}
\end{align*}
\begin{align*}
\norm{\mathcal{F}f}_{r,s} & \le\tilde{c}\left(s,r,n\right)\norm f_{s+n+1,r}
\end{align*}
Therefore $\mathcal{F}:\mathcal{S}\left(\mathbb{R}^{n}\right)\to\mathcal{S}\left(\mathbb{R}^{n}\right)$
is well-defined. The same follows analogously for $\overline{\mathcal{F}}$.\qqed

So we have the following correspondence:
\begin{align}
-\ii x^{j} & \leftrightarrow\frac{\partial}{\partial p^{j}}\\
-\ii\frac{\partial}{\partial x^{j}} & \leftrightarrow p^{j}
\end{align}
Here the derivatives always act on $f$ or $\mathcal{F}f$ and not
on $e^{-\ii px}$.
\begin{align*}
x^{\alpha}\DD^{\beta}f & \leftrightarrow\ii^{\abs{\alpha}+\abs{\beta}}\DD_{p}^{\alpha}p^{\beta}\left(\mathcal{F}f\right)\left(p\right)=\ii^{\abs{\alpha}+\abs{\beta}}p^{\beta}\DD^{\alpha}\left(\mathcal{F}f\right)\left(p\right)+\text{lower order terms}
\end{align*}
Suppose we had worked with $\norm f_{0,k}=\abs f_{C^{k}}$ as family
of norms. Then the norms of the Fourier transform of a function with
finite norms would not necessarily be finite.


\subsection{Theorem \textmd{(Plancherel, Convergence Generating Factor)\label{sub:Thm-Plancherel}}}

$\mathcal{F}$ and $\overline{\mathcal{F}}$ are inverse to each other:
\begin{align}
\overline{\mathcal{F}}\mathcal{F} & =\mathcal{F}\overline{\mathcal{F}}=\mathbbm{1}:\mathcal{S}\left(\mathbb{R}^{n}\right)\to\mathcal{S}\left(\mathbb{R}^{n}\right)
\end{align}



\subsubsection*{Proof}

\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}f\right)\left(p\right) & =\frac{1}{\left(2\pi\right)^{n}}\int_{\mathbb{R}^{n}}e^{-\ii px}\underbrace{\left(\int_{\mathbb{R}^{n}}e^{\ii qx}f\left(q\right)\dd^{n}q\right)}_{\left(\overline{\mathcal{F}}f\right)\left(x\right)}\dd^{n}x\stackrel{?}{=}f\left(p\right)\\
 & \stackrel{?}{=}\frac{1}{\left(2\pi\right)^{n}}\int_{\mathbb{R}^{n}}f\left(q\right)\left(\int_{\mathbb{R}^{n}}e^{-\ii\left(p-q\right)x}\dd^{n}x\right)\dd^{n}q
\end{align*}
The problem here is, that $e^{-\ii\left(p-q\right)x}$ does not decay
at infinity, so the integral is not well-defined. Instead we have
to introduce a \emph{convergence generating factor} $e^{-\varepsilon x^{2}}$
and, after integrating, calculate the limes $\varepsilon\to0$.
\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}f\right)\left(p\right) & =\frac{1}{\left(2\pi\right)^{n}}\lim_{\varepsilon\searrow0}\int_{\mathbb{R}^{n}}e^{-\ii px}e^{-\varepsilon x^{2}}\left(\int_{\mathbb{R}^{n}}e^{\ii qx}f\left(q\right)\dd^{n}q\right)\dd^{n}x=\\
 & \stackrel{\text{Fubini}}{=}\frac{1}{\left(2\pi\right)^{n}}\lim_{\varepsilon\searrow0}\int_{\mathbb{R}^{n}}f\left(q\right)\left(\int_{\mathbb{R}^{n}}e^{-\ii\left(p-q\right)x}e^{-\varepsilon x^{2}}\dd^{n}x\right)\dd^{n}q
\end{align*}
The resulting Gaussian integral can be computed in closed form. In
one dimension it is:
\begin{align*}
\int_{\mathbb{R}}e^{-\ii\lambda x}e^{-\varepsilon x^{2}}\dd x & =\int_{\mathbb{R}}e^{-\varepsilon\left(x+\frac{\ii\lambda}{2\varepsilon}\right)^{2}-\frac{\lambda^{2}}{4\varepsilon}}\dd x=e^{-\frac{\lambda^{2}}{4\varepsilon}}\int_{\mathbb{R}}e^{-\varepsilon\left(x+\frac{\ii\lambda}{2\varepsilon}\right)^{2}}\dd x=\\
 & \sr ={z=\sqrt{\varepsilon}\left(x+\frac{\ii\lambda}{2\varepsilon}\right)}{\dd z=\sqrt{\varepsilon}\dd x}e^{-\frac{\lambda^{2}}{4\varepsilon}}\int_{\mathbb{R}+\frac{\ii\lambda}{2\sqrt{\varepsilon}}}e^{-z^{2}}\frac{\dd z}{\sqrt{\varepsilon}}=\\
 & \sr ={\text{contour}}{\text{deformation}}\frac{e^{-\frac{\lambda^{2}}{4\varepsilon}}}{\sqrt{\varepsilon}}\underbrace{\int_{\mathbb{R}}e^{-z^{2}}\dd z}_{=\sqrt{\pi}}=\sqrt{\frac{\pi}{\varepsilon}}e^{-\frac{\lambda^{2}}{4\varepsilon}}
\end{align*}
So we get:
\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}f\right)\left(p\right) & =\frac{1}{\left(2\pi\right)^{n}}\lim_{\varepsilon\searrow0}\int_{\mathbb{R}^{n}}f\left(q\right)\left(\frac{\pi}{\varepsilon}\right)^{\frac{n}{2}}e^{-\frac{\left(p-q\right)^{2}}{4\varepsilon}}\dd^{n}q
\end{align*}


\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
 \begin{axis}[width=9cm, axis x line=middle, axis y line=middle, xtick={0}, ytick={0}, xlabel=$q$,domain=-6:6,samples=300, ymin=-0.5]
  \addplot[mark=none] {exp(-(1-x)^2/4)};
  \addlegendentry{$\varepsilon = 1$}
  \addplot[blue,dashed,mark=none] {exp(-(1-x)^2/0.4)/sqrt(0.5)};
  \addlegendentry{$\varepsilon = 0.5$}
  \addplot[red!50!black,dotted,mark=none] {exp(-(1-x)^2/0.03)/sqrt(0.1)};
  \addlegendentry{$\varepsilon = 0.1$}
  \draw (axis cs: 1,-0.05) -- (axis cs: 1,0.05) node[above]{$p$};
 \end{axis}
\end{tikzpicture} \hfill{}\begin{tikzpicture}
 \begin{axis}[width=9cm, axis equal image, axis x line=middle, axis y line=middle, xtick={0}, ytick={0}, xlabel=$q^i$, ylabel=$q^j$, xmin=-1, xmax=3, ymin=-0.45, ymax=3]
  \draw (axis cs: 2.05,0.95) -- (axis cs: 1.95,1.05) (axis cs: 2.05,1.05) -- (axis cs: 1.95,0.95) (axis cs:2,1) node[below]{$p$};
  \draw (axis cs: 2,1) circle (10.8mm);
  \draw (axis cs: 2,1) -- node[above]{$\varepsilon$} (axis cs: 1.4,1);
 \end{axis}
\end{tikzpicture} \caption{The Gaussian gets very narrow and very high as $\varepsilon$ decreases.}

\par\end{centering}

\end{figure}


Estimate the integral as follows:
\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}f\right)\left(p\right) & =\lim_{\varepsilon\searrow0}\frac{1}{\left(4\pi\varepsilon\right)^{\frac{n}{2}}}\left(\int_{\mathbb{R}^{n}}f\left(p\right)e^{-\frac{\left(p-q\right)^{2}}{4\varepsilon}}\dd^{n}q+\int_{\mathbb{R}^{n}}\left(f\left(q\right)-f\left(p\right)\right)e^{-\frac{\left(p-q\right)^{2}}{4\varepsilon}}\dd^{n}q\right)
\end{align*}
The first integral gives:
\begin{align*}
\int_{\mathbb{R}^{n}}e^{-\frac{\left(p-q\right)^{2}}{4\varepsilon}}\dd^{n}q & \sr ={z=\frac{p-q}{2\sqrt{\varepsilon}}}{\dd^{n}z=\frac{\dd^{n}q}{\left(4\varepsilon\right)^{\frac{n}{2}}}}\left(4\varepsilon\right)^{\frac{n}{2}}\underbrace{\int_{\mathbb{R}^{n}}e^{-z^{2}}\dd^{n}q}_{=\pi^{\frac{n}{2}}}=\left(4\pi\varepsilon\right)^{\frac{n}{2}}
\end{align*}
So we get:
\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}f\right)\left(p\right) & =f\left(p\right)+\lim_{\varepsilon\searrow0}\frac{1}{\left(4\pi\varepsilon\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\left(f\left(q\right)-f\left(p\right)\right)e^{-\frac{\left(p-q\right)^{2}}{4\varepsilon}}\dd^{n}q
\end{align*}
It remains to show that the second summand goes to zero for $\varepsilon\to0$.
We use the following scaling argument:
\begin{align*}
\frac{1}{\varepsilon^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\left(f\left(q\right)-f\left(p\right)\right)e^{-\frac{\left(p-q\right)^{2}}{4\varepsilon}}\dd^{4}q & \sr ={u=\frac{p-q}{\sqrt{\varepsilon}}}{\dd^{n}q=\varepsilon^{\frac{n}{2}}\dd^{n}u}\frac{1}{\varepsilon^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\left(f\left(p-\sqrt{\varepsilon}u\right)-f\left(p\right)\right)e^{-\frac{u^{2}}{4}}\varepsilon^{\frac{n}{2}}\dd^{n}u=\\
 & =\int_{\mathbb{R}^{n}}\underbrace{\left(f\left(p-\sqrt{\varepsilon}u\right)-f\left(p\right)\right)}_{\xrightarrow{\varepsilon\searrow0}0\ \text{pointwise}}e^{-\frac{u^{2}}{4}}\dd^{n}u
\end{align*}
For the integrand holds:
\begin{align*}
\underbrace{\left(f\left(p-\sqrt{\varepsilon}u\right)-f\left(p\right)\right)}_{\xrightarrow{\varepsilon\searrow0}0\ \text{pointwise}}e^{-\frac{u^{2}}{4}} & \le\norm f_{0,0}e^{-\frac{u^{2}}{4}}\in L^{1}\left(\mathbb{R}^{n}\right)
\end{align*}
So the dominated convergence theorem can be applied to get:
\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}f\right)\left(p\right) & =f\left(p\right)+\lim_{\varepsilon\searrow0}\frac{1}{\left(4\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\underbrace{\lim_{\varepsilon\searrow0}\left(f\left(p-\sqrt{\varepsilon}u\right)-f\left(p\right)\right)}_{=0}e^{-\frac{u^{2}}{4}}\dd^{n}u=f\left(p\right)
\end{align*}


$\overline{\mathcal{F}}\mathcal{F}=\mathbbm{1}$ follows analogously.\qqed

We want to generalize the Fourier transform to $\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$.
We begin with the case $T_{g}$ with $g\in\mathcal{S}\left(\mathbb{R}^{n}\right)$.
We want:
\begin{align*}
\mathcal{F}\left(T_{g}\right) & =T_{\mathcal{F}g}
\end{align*}
\begin{align*}
T_{\mathcal{F}g}\left(f\right) & =\int_{\mathbb{R}^{n}}\left(\mathcal{F}g\right)\left(p\right)f\left(p\right)\dd^{n}p=\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}\left(\int_{\mathbb{R}^{n}}e^{-\ii px}g\left(x\right)\dd^{n}x\right)f\left(p\right)\dd^{n}p
\end{align*}
Fubini's theorem allows us to interchange the order of integration:
\begin{align*}
T_{\mathcal{F}g}\left(f\right) & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}g\left(x\right)\underbrace{\left(\int_{\mathbb{R}^{n}}e^{-\ii px}f\left(p\right)\dd^{n}p\right)}_{=\mathcal{F}f\left(x\right)}\dd^{n}x=\int_{\mathbb{R}^{n}}g\left(x\right)\left(\mathcal{F}f\right)\left(x\right)\dd^{n}x=\TT_{g}\left(\mathcal{F}f\right)
\end{align*}
Since we want $\left(\mathcal{F}T_{g}\right)\left(f\right)=T_{\mathcal{F}g}=T_{g}\left(\mathcal{F}f\right)$,
this motivates the following general definition:


\subsection{Definition \textmd{(Fourier Transform of Distributions)\label{sub:Def-Fourier_distribution}}}

$\mathcal{F},\overline{\mathcal{F}}:\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)\to\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$
are defined by their action on a test function $f\in\mathcal{S}\left(\mathbb{R}^{n}\right)$:
\begin{align}
\left(\mathcal{F}T\right)\left(f\right) & :=T\left(\mathcal{F}f\right)\\
\left(\overline{\mathcal{F}}T\right)\left(f\right) & :=T\left(\overline{\mathcal{F}}f\right)
\end{align}
It holds:
\begin{align*}
\abs{\left(\mathcal{F}T\right)\left(f\right)} & =\abs{T\left(\mathcal{F}f\right)}\sr{\le}{T\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)}{\Rightarrow\exs r,s\in\mathbb{N},c\in\mathbb{R}_{>0}}c\norm{\mathcal{F}f}_{r,s}\le\tilde{c}\norm f_{s+n+1,r}
\end{align*}
Thus $\mathcal{F}T$ is indeed a tempered distribution.


\subsection{Theorem \textmd{(Plancherel for Distributions)}}

Plancherel's theorem holds on $\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$
as well:
\begin{align}
\mathcal{F}\overline{\mathcal{F}} & =\overline{\mathcal{F}}\mathcal{F}=\mathbbm{1}_{\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)}
\end{align}



\subsubsection*{Proof}

\begin{align*}
\left(\mathcal{F}\overline{\mathcal{F}}T\right)\left(f\right) & \stackrel{\text{Definition }\ref{sub:Def-Fourier_distribution}}{=}\left(\overline{\mathcal{F}}T\right)\left(\mathcal{F}f\right)=T\left(\overline{\mathcal{F}}\mathcal{F}f\right)\stackrel{\text{Plancherel }\ref{sub:Thm-Plancherel}}{=}T\left(f\right)
\end{align*}
Since this holds for all $f\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
and all $T\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$ it follows:
\begin{align*}
\mathcal{F}\overline{\mathcal{F}} & =\mathbbm{1}_{\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)}
\end{align*}
The same follows for $\overline{\mathcal{F}}\mathcal{F}$.\qqed


\subsection{Examples}
\begin{enumerate}
\item The Fourier transform of the $\delta$-Distribution can be calculated
as follows:
\begin{align*}
\left(\mathcal{F}\delta\right)\left(f\right) & =\delta\left(\mathcal{F}f\right)=\left(\mathcal{F}f\right)\left(0\right)=\\
 & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}e^{-\ii px}f\left(x\right)\dd^{n}x\bigg|_{p=0}=\int_{\mathbb{R}^{n}}\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}f\left(x\right)\dd^{n}x=T_{\left(2\pi\right)^{-\frac{n}{2}}}\left(f\right)
\end{align*}
This means:
\begin{align*}
\mathcal{F}\delta & =T_{\left(2\pi\right)^{-\frac{n}{2}}}
\end{align*}
Or, in a more computational manner, one can write this as:
\begin{align*}
\mathcal{F}\delta & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}e^{-\ii px}\delta\left(x\right)\dd^{n}x=\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}
\end{align*}
This is not satisfying from a mathematical point of view, because
one does not know, that the usual formula for the Fourier transform
also works for distributions.
\item Consider $T_{f}$ with $f\left(p\right)=e^{\ii py}$ for a given $y\in\mathbb{R}^{n}$.
\begin{align*}
\left(\mathcal{F}T_{f}\right)\left(h\right) & =T_{f}\left(\mathcal{F}h\right)=T_{f}\left(\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int e^{-\ii px}h\left(x\right)\dd^{n}x\right)=\\
 & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int e^{\ii py}\left(\int e^{-\ii px}h\left(x\right)\dd^{n}x\right)\dd^{n}p=\\
 & =\left(2\pi\right)^{\frac{n}{2}}\left(\overline{\mathcal{F}}\mathcal{F}\right)h\left(y\right)=\left(2\pi\right)^{\frac{n}{2}}h\left(y\right)
\end{align*}
So we get:
\begin{align*}
\mathcal{F}T_{f}h & =\left(2\pi\right)^{\frac{n}{2}}h\left(y\right)\\
\left(\mathcal{F}T_{f}\right)\left(x\right) & =\left(2\pi\right)^{\frac{n}{2}}\delta^{\left(n\right)}\left(x-y\right)
\end{align*}
Formally one can write:
\begin{align*}
\left(\mathcal{F}f\right)\left(x\right) & =\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}e^{-\ii px}e^{\ii py}\dd^{n}p=\frac{1}{\left(2\pi\right)^{\frac{n}{2}}}\int_{\mathbb{R}^{n}}e^{-\ii p\left(x-y\right)}\dd^{n}p
\end{align*}
This is ill-defined, but physicists use the formal relation:
\begin{align*}
\int_{\mathbb{R}^{n}}e^{-\ii p\left(x-y\right)}\dd^{n}p & =\left(2\pi\right)^{n}\delta^{\left(n\right)}\left(x-y\right)
\end{align*}
%DATE: Fr 3.5.13
\item Consider $T=T_{g}$ with $g\left(p\right)=p^{2}e^{\ii py}$ for a
given $y\in\mathbb{R}$. 
\begin{align*}
\left(\mathcal{F}T_{g}\right)\left(f\right) & =T_{g}\left(\mathcal{F}f\right)=\int_{-\infty}^{\infty}g\left(p\right)\left(\mathcal{F}f\right)\left(p\right)\dd p=\\
 & =\int g\left(p\right)\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f\left(x\right)e^{-\ii px}\dd x\right)\dd p
\end{align*}
One cannot interchange the integrals, because the integral
\begin{align*}
\int_{-\infty}^{\infty}\underbrace{g\left(p\right)e^{-\ii p\alpha}}_{\not\in L^{1}}\dd p
\end{align*}
does not exist. Therefore we work again with a convergence generating
factor, which we can due to the dominated convergence theorem:
\begin{align*}
\left(\mathcal{F}T_{g}\right)\left(f\right) & =\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}g\left(p\right)e^{-\varepsilon\abs p}\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}f\left(x\right)e^{-\ii px}\dd x\right)\dd p=\\
 & =\frac{1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\left(\int_{-\infty}^{\infty}g\left(p\right)e^{-\varepsilon\abs p}e^{-\ii px}\dd p\right)\dd x=\\
 & =\frac{1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\left(\int_{-\infty}^{\infty}p^{2}e^{\ii py}e^{-\varepsilon\abs p}e^{-\ii px}\dd p\right)\dd x=\\
 & =\frac{1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\left(\int_{-\infty}^{\infty}\left(\ii\partial_{x}\right)^{2}e^{-\ii p\left(x-y\right)-\varepsilon\abs p}\dd p\right)\dd x=\\
 & \stackrel{\text{Lebesgue theorem}}{=}\frac{1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\left(\ii\partial_{x}\right)^{2}\left(\int_{-\infty}^{\infty}e^{-\ii p\left(x-y\right)-\varepsilon\abs p}\dd p\right)\dd x
\end{align*}
Now one can decompose the integral into integrals from $-\infty$
to 0 and from $0$ to $\infty$ and calculate the result.
\begin{align*}
\left(\mathcal{F}T_{g}\right)\left(f\right) & =\frac{1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\left(\ii\partial_{x}\right)^{2}\left(\int_{-\infty}^{0}e^{-\ii p\left(x-y\right)+\varepsilon p}\dd p+\int_{0}^{\infty}e^{-\ii p\left(x-y\right)-\varepsilon p}\dd p\right)\dd x=\\
 & =\frac{-1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\partial_{x}^{2}\left(\frac{1}{-\ii\left(x-y\right)+\varepsilon}-\frac{1}{-\ii\left(x-y\right)-\varepsilon}\right)\dd x=\\
 & =\frac{-1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\partial_{x}^{2}\left(\frac{-\ii\left(x-y\right)-\varepsilon-\left(-\ii\left(x-y\right)+\varepsilon\right)}{-\left(x-y\right)^{2}-\varepsilon^{2}}\right)\dd x=\\
 & =\frac{1}{\sqrt{2\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f\left(x\right)\partial_{x}^{2}\left(\frac{2\varepsilon}{\left(x-y\right)^{2}+\varepsilon^{2}}\right)\dd x=\\
 & \sr ={\text{integration}}{\text{by parts}}\sqrt{\frac{2}{\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f''\left(x\right)\cdot\frac{\varepsilon}{\left(x-y\right)^{2}+\varepsilon^{2}}\dd x=\\
 & \sr ={z:=\frac{x-y}{\varepsilon}}{\dd z=\frac{\dd x}{\varepsilon}}\sqrt{\frac{2}{\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f''\left(\varepsilon z+y\right)\cdot\frac{\varepsilon}{\varepsilon^{2}z^{2}+\varepsilon^{2}}\varepsilon\dd z=\\
 & =\sqrt{\frac{2}{\pi}}\lim_{\varepsilon\searrow0}\int_{-\infty}^{\infty}f''\left(\varepsilon z+y\right)\cdot\frac{1}{z^{2}+1}\dd z=\\
 & =\sqrt{\frac{2}{\pi}}f''\left(y\right)\underbrace{\int_{-\infty}^{\infty}\frac{1}{z^{2}+1}\dd z}_{=\pi}=\sqrt{2\pi}f''\left(y\right)
\end{align*}
So we get:
\begin{align*}
\left(\mathcal{F}T_{g}\right)\left(x\right) & =\sqrt{2\pi}\delta''\left(x-y\right)
\end{align*}

\item Consider the following Hilbert space:
\begin{align*}
L^{2}\left(\mathbb{R}^{n}\right) & =\left\{ f:\mathbb{R}^{n}\to\mathbb{R}\bigg|f\text{ measurable with }\int\abs f^{2}\dd^{n}x<\infty\right\} 
\end{align*}
For $f\in L^{2}\left(\mathbb{R}^{n}\right)$ define:
\begin{align*}
T_{f}\left(g\right) & :=\int_{\mathbb{R}^{n}}f\left(x\right)g\left(x\right)\dd^{n}x\\
\abs{T_{f}\left(g\right)} & \stackrel{\text{Schwarz}}{\le}\norm f_{L^{2}\left(\mathbb{R}^{n}\right)}\norm g_{L^{2}\left(\mathbb{R}^{n}\right)}\le\norm f_{L^{2}\left(\mathbb{R}^{n}\right)}\cdot c\left(n\right)\norm g_{n+1,0}
\end{align*}
Here we used:
\begin{align*}
\abs{g\left(x\right)} & \le\tilde{c}\left(n\right)\frac{\norm g_{n+1,0}}{\left(1+\abs x\right)^{n+1}}
\end{align*}
So we have $T_{f}\in\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)$.
\begin{align*}
L^{2}\left(\mathbb{R}^{n}\right) & \hookrightarrow\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)\xrightarrow{\mathcal{F}}\mathcal{S}^{*}\left(\mathbb{R}^{n}\right)\\
f & \mapsto\ \ \ T_{f}\quad\mapsto\mathcal{F}T_{f}
\end{align*}

\end{enumerate}

\subsection{Theorem \textmd{(Fourier Transform is isometry)}}

The mappings $\mathcal{F},\overline{\mathcal{F}}:L^{2}\left(\mathbb{R}^{n}\right)\to L^{2}\left(\mathbb{R}^{n}\right)$
are isometries, i.e.:
\begin{align}
\norm f_{L^{2}\left(\mathbb{R}^{n}\right)} & =\norm{\mathcal{F}f}_{L^{2}\left(\mathbb{R}^{n}\right)}
\end{align}
Due to $\mathcal{F}\overline{\mathcal{F}}=1$ they are even unitary
transformations.


\subsubsection*{Proof}

Consider first $f,g\in\mathcal{S}\left(\mathbb{R}^{n}\right)\subseteq L^{2}\left(\mathbb{R}^{n}\right)$.
\begin{align*}
\left\langle \mathcal{F}f,\mathcal{F}g\right\rangle _{L^{2}\left(\mathbb{R}^{n}\right)} & =\int_{\mathbb{R}^{n}}\overline{\left(\mathcal{F}f\right)\left(x\right)}\left(\mathcal{F}g\right)\left(x\right)\dd^{n}x\stackrel{?}{=}\int\overline{f}\left(x\right)g\left(x\right)\dd^{n}x\\
 & =T_{\overline{\mathcal{F}}\,\overline{f}}\left(\mathcal{F}g\right)=\left(\overline{\mathcal{F}}T_{\overline{f}}\right)\left(\mathcal{F}g\right)=T_{\overline{f}}\left(\overline{\mathcal{F}}\mathcal{F}g\right)=\\
 & \stackrel{\overline{\mathcal{F}}\mathcal{F}=1}{=}T_{\overline{f}}\left(g\right)=\int\overline{f}\left(x\right)g\left(x\right)\dd^{n}x
\end{align*}
$\mathcal{S}\left(\mathbb{R}^{n}\right)$ is dense in $L^{2}\left(\mathbb{R}^{n}\right)$.
Because $\mathcal{F}$ is continuous, $\mathcal{F}$ is also isometric
on $L^{2}\left(\mathbb{R}^{n}\right)$.\\
For $f\in L^{2}\left(\mathbb{R}^{n}\right)$ choose $f_{n}\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
with $f_{n}\to f$ converging in $L^{2}\left(\mathbb{R}^{n}\right)$.
Then holds for all $n\in\mathbb{N}$:
\begin{align*}
\norm{f_{n}}_{L^{2}} & =\norm{\mathcal{F}f_{n}}
\end{align*}
In the limes $n\to\infty$ we get, since $\mathcal{F}$ is continuous:
\begin{align*}
\norm f & =\norm{\mathcal{F}f}
\end{align*}
\qqed


\section{Applications to Partial Differential Equations with Constant Coefficients}

Consider as example the Poisson equation
\begin{align*}
\upDelta u & =f
\end{align*}
 in $\mathbb{R}^{n}$ with a given $f$ and assume for simplicity
$f\in\mathcal{S}\left(\mathbb{R}^{n}\right)$. After a Fourier transform
and defining $\hat{u}:=\mathcal{F}u$ and $\hat{f}=\mathcal{F}f$
we get:
\begin{align*}
\left(-\norm p^{2}\right)\hat{u}\left(p\right) & =\hat{f}\left(p\right)\\
\Rightarrow\qquad\hat{u}\left(p\right) & =-\frac{\hat{f}\left(p\right)}{\norm p^{2}}
\end{align*}
Then $\overline{\mathcal{F}}\hat{u}$ is a solution of the Poisson
equation.
\begin{itemize}
\item For $\norm p^{-2}\hat{f}\left(p\right)\in\mathcal{S}\left(\mathbb{R}^{n}\right)$
the method works directly and one gets a $u\in\mathcal{S}\left(\mathbb{R}^{n}\right)$.
\item In the case of $n\ge3$, $\hat{u}=-\norm p^{-2}\hat{f}\left(p\right)$
is a regular distribution. (If $n<3$ the integral does not necessarily
converge.)
\begin{align*}
\left(T_{\hat{u}}\right)\left(g\right) & :=\int\left(-\frac{\hat{f}\left(p\right)}{\norm p^{2}}\right)g\left(p\right)\dd^{n}p
\end{align*}
Therefore $u:=\overline{\mathcal{F}}T_{\hat{u}}$ is a distributional
solution of the Poisson equation, so we get $\upDelta u=T_{f}$.

\begin{description}
\item [{Problem:}] The distributional solution is not unique, because e.g.
\begin{align*}
\hat{u}\left(p\right) & =-\frac{\hat{f}\left(p\right)}{\norm p^{2}}+c\delta\left(p\right)
\end{align*}
is also a solution. Therefore we have to specify the behavior of $u\left(x\right)$
in the limit $\norm x\to\infty$.
\end{description}
\end{itemize}

\chapter{The Laplace Equation\texorpdfstring{ in $\Omega\subseteq\mathbb{R}^{n}$}{}}

In this chapter we always consider an open subset $\Omega\subseteq\mathbb{R}^{n}$
with boundary $\partial\Omega$. With the Laplacian
\begin{align}
\upDelta & =\frac{\partial^{2}}{\partial x_{1}^{2}}+\ldots+\frac{\partial^{2}}{\partial x_{n}^{2}}
\end{align}
the Laplace equation can be written as:
\begin{align*}
\upDelta u & =0
\end{align*}
The inhomogeneous Laplace equation is called Poisson equation:
\begin{align*}
\upDelta u & =f
\end{align*}
For both one needs to specify boundary conditions on $\partial\Omega$
to get a unique solution.

\setcounter{section}{-1}


\section{Reminder}


\subsection{Theorem \textmd{(Gauss's Theorem)}}

If $Y$ is a smooth vector field on the closure $\overline{\Omega}$
and $\partial\Omega$ is smooth with outer normal $\nu$, then holds:
\begin{align}
\int_{\Omega}\text{div}\left(Y\right)\underbrace{\dd^{n}x}_{=\dd\mu} & =\int_{\partial\Omega}\left\langle Y,\nu\right\rangle \dd\mu_{\partial\Omega}
\end{align}



\subsubsection*{(Proof omitted)}


\subsection{Theorem \textmd{(Green's Identities)}}

For $u,w\in C^{\infty}\left(\overline{\Omega}\right)$ holds:
\begin{align}
\int_{\Omega}w\upDelta u\dd\mu_{\Omega} & =\int_{\partial\Omega}w\left(\nabla_{\nu}u\right)\dd\mu_{\partial\Omega}-\int_{\Omega}\left\langle \nabla w,\nabla u\right\rangle \dd\mu_{\Omega}\\
\int_{\Omega}\left(w\left(\upDelta u\right)-\left(\upDelta w\right)u\right)\dd\mu_{\Omega} & =\int_{\partial\Omega}\left(w\left(\nabla_{\nu}u\right)-\left(\nabla_{\nu}w\right)u\right)\dd\mu_{\partial\Omega}
\end{align}



\subsubsection*{Proof}

We use $\upDelta=\text{div}\,\text{grad}$ and integrate by parts:
\begin{align*}
\int_{\Omega}w\upDelta u\dd\mu_{\Omega} & =\int_{\Omega}w\cdot\text{div}\left(\nabla u\right)\dd\mu_{\Omega}=\int_{\Omega}\text{div}\left(w\nabla u\right)-\left\langle \nabla w,\nabla u\right\rangle \dd\mu_{\Omega}
\end{align*}
Then one can use Gauss's theorem to get the first identity. Now one
subtracts the identity with $w$ and $u$ commuted:
\begin{align*}
w\upDelta u-u\upDelta w & =\text{div}\left(w\nabla u\right)-\left\langle \nabla w,\nabla u\right\rangle -\left(\text{div}\left(u\nabla w\right)-\left\langle \nabla u,\nabla w\right\rangle \right)=\\
 & \stackrel{\left\langle .,.\right\rangle \text{ symmetric}}{=}\text{div}\left(w\nabla u\right)-\text{div}\left(u\nabla w\right)
\end{align*}
Using Gauss's theorem the second identity follows.\qqed


\section{Representation Formulas for Harmonic Functions}


\subsection{Definition \textmd{(Harmonic Functions)}}

A function $u\in C^{2}\left(\overline{\Omega}\right)$ is called \emph{harmonic},
if the Laplacian vanishes:
\begin{align*}
\upDelta u & =0
\end{align*}
The harmonic functions form a vector space.


\subsubsection*{Examples}
\begin{itemize}
\item Constant or linear functions
\item $u\left(x_{1},x_{2}\right)=x_{1}^{2}-x_{2}^{2}$ is harmonic on $\mathbb{R}^{2}$.
\item Holomorphic functions on $\Omega\subseteq\mathbb{C}\stackrel{\sim}{=}\mathbb{R}^{2}$
\end{itemize}
Consider now \emph{spherically symmetric harmonic functions}. For
this we choose polar coordinates $\left(r,\vartheta,\varphi\right)$
in $\mathbb{R}^{3}$:
\begin{align*}
x & =r\cos\left(\vartheta\right)\\
y & =r\sin\left(\vartheta\right)\cos\left(\varphi\right)\\
z & =r\sin\left(\vartheta\right)\sin\left(\varphi\right)
\end{align*}
More general in $\mathbb{R}^{n}$ with $n\in\mathbb{N}_{\ge2}$ we
choose $r=\norm x$ and $\omega\in S^{n-1}$. Regard $\mathbb{R}^{n}$
with the Euclidian metric as a Riemannian manifold $\left(M,g\right)$.
Polar coordinates give a special chart on $\Omega\subseteq M$ with
$0\not\in\Omega$. The we can calculate $\upDelta=\LBO$ as Laplace-Beltrami
operator in polar coordinates. The metric is:
\begin{align*}
g & =\left(\begin{array}{cc}
1 & 0\\
0 & r^{2}g_{S^{n-1}}
\end{array}\right) & g^{-1} & =\left(\begin{array}{cc}
1 & 0\\
0 & r^{-2}g_{S^{n-1}}^{-1}
\end{array}\right)
\end{align*}
\begin{align*}
\det\left(g\right) & =r^{2\left(n-1\right)}g_{S^{n-1}}\\
\sqrt{\det\left(g\right)} & =r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}
\end{align*}
Now the Laplace-Beltrami operator can be calculated with the \emph{Koszul
formula}:
\begin{align}
\LBO u & =\nabla_{j}\nabla^{j}u=\frac{1}{\sqrt{\det\left(g\right)}}\frac{\partial}{\partial x^{j}}\left(\sqrt{\det\left(g\right)}g^{jk}\frac{\partial}{\partial x^{k}}u\right)=\\
 & =\frac{1}{r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}}\frac{\partial}{\partial x^{j}}\left(r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}g^{jk}\frac{\partial}{\partial x^{k}}u\right)=\nonumber \\
 & =\frac{1}{r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}}\frac{\partial}{\partial r}\left(r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}\frac{\partial}{\partial r}u\right)+\nonumber \\
 & \qquad+\sum_{j,k=2}^{n}\frac{1}{r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}}\frac{\partial}{\partial x^{j}}\left(r^{n-1}\sqrt{\det\left(g_{S^{n-1}}\right)}\frac{1}{r^{2}}\left(g_{S^{n-1}}\right)^{jk}\frac{\partial}{\partial x^{k}}u\right)=\nonumber \\
 & =\frac{1}{r^{n-1}}\frac{\partial}{\partial r}\left(r^{n-1}\frac{\partial}{\partial r}u\right)+\frac{1}{r^{2}}\LBO_{S^{n-1}}u\nonumber 
\end{align}
The important formula is:
\begin{align}
\LBO & =\frac{1}{r^{n-1}}\frac{\partial}{\partial r}\left(r^{n-1}\frac{\partial}{\partial r}\,.\,\right)+\frac{1}{r^{2}}\LBO_{S^{n-1}}
\end{align}
For spherically symmetric solutions $\Gamma\in C^{\infty}\left(\mathbb{R}^{n}\setminus\left\{ 0\right\} \right)$,
the Laplace equation reads:
\begin{align*}
\LB\Gamma=\frac{1}{r^{n-1}}\frac{\partial}{\partial r}\left(r^{n-1}\frac{\partial}{\partial r}\Gamma\left(r\right)\right) & =0
\end{align*}


%DATE: Mi 8.5.13

This gives:
\begin{align*}
r^{n-1}\frac{\partial}{\partial r}\Gamma\left(r\right) & =c\\
\frac{\partial}{\partial r}\Gamma\left(r\right) & =cr^{1-n}
\end{align*}
\begin{align*}
\Gamma\left(r\right) & =a+C\int^{r}\tau^{1-n}\dd\tau=\begin{cases}
a+\tilde{c}\ln\left(r\right) & \text{if }n=2\\
a+\frac{c}{r^{n-2}} & \text{if }n>2
\end{cases}
\end{align*}
Now we choose specific values for $a$ and $c$ or $\tilde{c}$.


\subsection{Definition \textmd{(Fundamental Solution of Laplace Equation)}}

The \emph{fundamental solution} $\Gamma$ of the Laplace equation
in $\mathbb{R}^{n}$ is defined by:
\begin{align}
\Gamma\left(x,y\right) & =\Gamma\left(\norm{x-y}\right):=\begin{cases}
\frac{1}{2\pi}\ln\left(\norm{x-y}\right) & \text{if }n=2\\
\frac{1}{n\left(2-n\right)\omega_{n}}\norm{x-y}^{2-n} & \text{if }n>2
\end{cases}
\end{align}
Here $\omega_{n}:=\mu\left(B_{1}\left(0\right)\right)$ is the Lebesgue
measure of the unit ball.

For example for $n=3$ we have $\omega_{3}=\frac{4\pi}{3}$ and thus:
\begin{align}
\Gamma\left(x,y\right) & =\frac{1}{3\left(-1\right)\frac{4\pi}{3}}\cdot\frac{1}{\norm{x-y}}=-\frac{1}{4\pi}\frac{1}{\norm{x-y}}
\end{align}



\subsection{Theorem \textmd{(Green's representation)}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be open with smooth boundary
and $u\in C^{2}\left(\overline{\Omega}\right)$. Then for any $y\in\Omega$
holds:
\begin{align}
u\left(y\right) & =\int_{\partial\Omega}\left(u\left(x\right)\nabla_{\nu}\Gamma\left(x,y\right)-\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}\Gamma\left(x,y\right)\left(\upDelta u\right)\left(x\right)\dd\mu\left(x\right)
\end{align}
For harmonic functions with $\upDelta u=0$, this simplifies to:
\begin{align}
u\left(y\right) & =\int_{\partial\Omega}\left(u\left(x\right)\nabla_{\nu}\Gamma\left(x,y\right)-\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)
\end{align}
Thus $u$ has an explicit representation in terms of its boundary
values on $\partial\Omega$.


\subsubsection*{Proof}

Choose $\varepsilon\in\mathbb{R}_{>0}$ such that $B_{\varepsilon}\left(y\right)\subseteq\Omega$.
\begin{align*}
\int_{\Omega\setminus B_{\varepsilon}\left(y\right)}\underbrace{\Gamma\left(x,y\right)}_{\text{smooth}}\underbrace{\left(\upDelta u\right)\left(x\right)}_{\text{continuous}}\dd\mu\left(x\right)
\end{align*}


\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}[arr/.style={decoration={markings,mark=at position 1 with {\arrow[scale=2]{>}};},postaction={decorate}},scale=0.8]
  \draw (-0.1,-0.1) -- (0.1,0.1);
  \draw (-0.1,0.1) -- (0.1,-0.1);
  \node[right=0pt] at (0,0) {$y$};
  \draw (0,0) circle(1);
  \draw[dashed] (0,0) -- node[above left=-2pt]{$\varepsilon$} ({cos(100)},{sin(100)});
  \draw plot[smooth cycle,tension=.7,scale=2] coordinates{(0,-2) (1,-0.95) (2,0.2) (1.5,1.4) (0,2) (-2,-0.5)};
  \node at (0.5,4.5) {$\Omega$};
  \foreach \x in {20,80,...,360} {
    \draw[arr, orange] ({cos(\x)},{sin(\x)}) -- ({1.5*cos(\x)},{1.5*sin(\x)});
    \draw[arr] ({cos(\x)},{sin(\x)}) -- ({0.5*cos(\x)},{0.5*sin(\x)});
	}
  \node[orange] at (1.5,0.3) {$\nu$};
  \draw[arr] (0,4) -- (-0.3,4.45);
  \draw[arr] (4.05,1) -- (4.55,1);
  \draw[arr] (2,-1.9) -- (2.3,-2.2);
  \draw[arr] (-2,-3.27) -- node[below right=2pt]{$\nu$} (-2.25,-3.74);
  \draw[arr] (-3,1.2) -- (-3.4,1.5);
\end{tikzpicture}
\par\end{centering}

\caption{Outer normal $\nu$ of $\Omega\setminus B_{\varepsilon}\left(y\right)$
(black) and of $B_{\varepsilon}\left(y\right)$ (orange) }
\end{figure}


We apply the second Green's identity to obtain with $v\left(x\right):=\Gamma\left(x,y\right)$:
\begin{align*}
\int_{\Omega\setminus B_{\varepsilon}\left(y\right)}\big(v\cdot\upDelta u-\underbrace{\left(\upDelta v\right)}_{=0}u\big)\dd\mu & =\int_{\partial\left(\Omega\setminus B_{\varepsilon}\left(y\right)\right)}\left(v\left(\nabla_{\nu}u\right)-\left(\nabla_{\nu}v\right)u\right)\dd\mu_{\partial\Omega}=\\
 & =\int_{\partial\Omega}\left(\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)-\left(\nabla_{\nu}\Gamma\right)\left(x,y\right)u\left(x\right)\right)\dd\mu_{\partial\Omega}+\\
 & \qquad-\int_{\partial B_{\varepsilon}\left(y\right)}\left(\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)-\nabla_{\nu}\Gamma\left(x,y\right)u\left(x\right)\right)\dd\mu_{\partial B_{\varepsilon}\left(y\right)}
\end{align*}
The minus in front of $\int_{\partial B_{\varepsilon}\left(y\right)}$
comes from the fact, that the outer normal of $\partial\left(\Omega\setminus B_{\varepsilon}\left(y\right)\right)$
shows in the opposite direction of the outer normal of $\partial B_{\varepsilon}\left(y\right)$.
The left side of the integral gives in the limit $\varepsilon\searrow0$:
\begin{align*}
\lim_{\varepsilon\searrow0}\int_{\Omega\setminus B_{\varepsilon}\left(y\right)}v\cdot\upDelta u\dd\mu & =\int_{\Omega}v\cdot\upDelta u\dd\mu=\int_{\Omega}\Gamma\left(x,y\right)\cdot\left(\upDelta u\right)\left(x\right)\dd\mu\left(x\right)
\end{align*}
We estimate the integral over $\partial B_{\varepsilon}\left(y\right)$
in the limit $\varepsilon\searrow0$:
\begin{align*}
\abs{\int_{\partial B_{\varepsilon}\left(y\right)}\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)\dd\mu_{\partial B_{\varepsilon}\left(y\right)}} & \le\Gamma\left(\varepsilon\right)\sup_{B_{\varepsilon}\left(y\right)}\norm{\nabla u}\cdot\underbrace{n\omega_{n}}_{\sr{}{\text{surface area of}}{\text{the unit sphere}}}\varepsilon^{n-1}\sim\\
 & \sim\begin{cases}
\varepsilon & \text{if }n>2\\
\varepsilon\ln\left(\varepsilon\right) & \text{if }n=2
\end{cases}\xrightarrow{\varepsilon\searrow0}0
\end{align*}
Now we expand the second part around $\varepsilon=0$:
\begin{align*}
\int_{\partial B_{\varepsilon}\left(y\right)}\left(\nabla_{\nu}\Gamma\right)\left(x,y\right)u\left(x\right)\dd\mu_{\partial B_{\varepsilon}\left(y\right)} & =\underbrace{\frac{\partial}{\partial\varepsilon}\Gamma\left(\varepsilon\right)}_{=\frac{1}{n\omega_{n}}\varepsilon^{1-n}}\underbrace{\int_{\partial B_{\varepsilon}\left(y\right)}u\left(x\right)\dd\mu_{B_{\varepsilon}\left(y\right)}}_{=u\left(y\right)n\omega_{n}\varepsilon^{n-1}+o_{0}\left(\varepsilon^{n-1}\right)}=\\
 & =u\left(y\right)+o_{0}\left(\varepsilon^{0}\right)\xrightarrow{\varepsilon\searrow0}u\left(y\right)
\end{align*}
This gives:
\begin{align*}
\int_{\Omega}\Gamma\left(x,y\right)\left(\upDelta u\right)\left(x\right)\dd\mu\left(x\right) & =\int_{\partial\Omega}\left(\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)-\left(\nabla_{\nu}\Gamma\right)\left(x,y\right)u\left(x\right)\right)\dd\mu_{\partial\Omega}+u\left(y\right)
\end{align*}
\begin{align*}
\Rightarrow\qquad u\left(y\right) & =\int_{\partial\Omega}\left(u\left(x\right)\nabla_{\nu}\Gamma\left(x,y\right)-\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}\Gamma\left(x,y\right)\left(\upDelta u\right)\left(x\right)\dd\mu\left(x\right)
\end{align*}
\qqed


\subsection{Corollary \textmd{(Laplacian of Fundamental Solution is Delta Distribution)}}

For any $\varphi\in C_{0}^{\infty}\left(\Omega\right)$ holds:
\begin{align}
\varphi\left(y\right) & =\int_{\Omega}\Gamma\left(x,y\right)\left(\upDelta\varphi\right)\left(x\right)\dd\mu\left(x\right)\label{eq:phi-as-integral}
\end{align}
This can also be expressed in terms of distributions as:
\begin{align}
\upDelta_{x}\Gamma\left(x,y\right) & =\delta^{\left(n\right)}\left(x-y\right)
\end{align}
More correctly, for fixed $y\in\mathbb{R}^{n}$, $T\left(x\right):=T_{\Gamma\left(x,y\right)}$
defines a regular distribution. Equation (\ref{eq:phi-as-integral})
means, that for all $\varphi\in C_{0}^{\infty}\left(\Omega\right)\subseteq\mathcal{S}\left(\mathbb{R}^{n}\right)$:
\begin{align*}
\left(\upDelta T\right)\left(\varphi\right) & =\varphi\left(y\right)\\
\Leftrightarrow\qquad\upDelta T & =\delta_{y}
\end{align*}



\subsubsection*{Proof}

Since the support of $\varphi$ lies inside of $\Omega$, the first
term in Green's representation vanishes:
\begin{align*}
\varphi\left(y\right) & =\int_{\partial\Omega}\left(\varphi\left(x\right)\nabla_{\nu}\Gamma\left(x,y\right)-\Gamma\left(x,y\right)\nabla_{\nu}\varphi\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}\Gamma\left(x,y\right)\left(\upDelta\varphi\right)\left(x\right)\dd\mu\left(x\right)=\\
 & =\int_{\Omega}\Gamma\left(x,y\right)\left(\upDelta\varphi\right)\left(x\right)\dd\mu\left(x\right)
\end{align*}
\qqed

Next we investigate the existence of solutions.
\begin{align}
\upDelta u & =0 & u\big|_{\partial\Omega} & =u_{0} &  & \text{Dirichlet problem}\\
\upDelta u & =0 & \nabla_{\nu}u\big|_{\partial\Omega} & =u_{1} &  & \text{Neumann problem}
\end{align}
The problem is, that the representation
\begin{align*}
u\left(y\right) & =\int_{\partial\Omega}\left(u\left(x\right)\nabla_{\nu}\Gamma\left(x,y\right)-\Gamma\left(x,y\right)\nabla_{\nu}u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)
\end{align*}
needs booth $u$ and $\nabla_{\nu}u$ on the boundary. Suppose we
want to solve the Dirichlet problem. Then $u_{0}=u\big|_{\partial\Omega}$
is given, but $\nabla_{\nu}u\big|_{\partial\Omega}$ is unknown.


\section{The Green's Function}

Consider the Dirichlet problem for the Poisson equation:
\begin{align}
\upDelta u\left(x\right) & =f\left(x\right)\qquad\fall_{x\in\Omega} & u\big|_{\partial\Omega} & =\varphi
\end{align}
Assume $u\in C^{2}\left(\overline{\Omega}\right)$, $f\in C^{0}\left(\overline{\Omega}\right)$
and $\varphi\in C^{2}\left(\partial\Omega\right)$.


\subsection{Definition \textmd{(Green's function)}}

A function $G\left(x,y\right)$ defined for $x,y\in\overline{\Omega}$
with $x\not=y$ is called \emph{Green's function} for the domain $\Omega$
if the following conditions are satisfied:
\begin{enumerate}[label=\roman*)]
\item For all $x\in\partial\Omega$ and $y\not=x$ holds ${\displaystyle G\left(x,y\right)=0}$.
\item $h\left(x,y\right):=G\left(x,y\right)-\Gamma\left(x,y\right)$ is
in $C^{2}\left(\Omega\right)$, even for $x=y$, and harmonic in $x\in\Omega$.
\end{enumerate}

\subsection{Proposition \textmd{(Solution of Dirichlet Problem)}}

For a solution $u$ of the Dirichlet problem for the Poisson equation
holds:
\begin{align}
u\left(y\right) & =\int_{\partial\Omega}u\left(x\right)\nabla_{\nu}G\left(x,y\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}f\left(x\right)G\left(x,y\right)\dd\mu\left(x\right)
\end{align}



\subsubsection*{Proof}

\begin{align*}
\int_{\Omega}h\left(x,y\right)\left(\upDelta u\right)\left(x\right)\dd\mu\left(x\right) & =\int_{\Omega}h\left(x,y\right)f\left(x\right)\dd\mu\left(x\right)=\\
 & \sr ={\text{2. Green's}}{\text{identity}}\int_{\partial\Omega}\left(h\left(x,y\right)\left(\nabla_{\nu}u\right)\left(x\right)-\left(\nabla_{\nu}h\left(x,y\right)\right)\cdot u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)
\end{align*}
Now we add the Green's representation
\begin{align*}
\int_{\Omega}\Gamma\left(x,y\right)f\left(x\right)\dd\mu\left(x\right) & =\int_{\partial\Omega}\left(\Gamma\left(x,y\right)\left(\nabla_{\nu}u\right)\left(x\right)-\left(\nabla_{\nu}\Gamma\left(x,y\right)\right)u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)+u\left(y\right)
\end{align*}
to get:
\begin{align*}
\int_{\Omega}G\left(x,y\right)f\left(x\right)\dd\mu\left(x\right) & =\int_{\partial\Omega}\left(G\left(x,y\right)\left(\nabla_{\nu}u\right)\left(x\right)-\left(\nabla_{\nu}G\left(x,y\right)\right)u\left(x\right)\right)\dd\mu_{\partial\Omega}\left(x\right)+u\left(y\right)
\end{align*}
Since $G\left(x,y\right)=0$ on $\partial\Omega$, the proposition
follows.\qqed


\subsection{Theorem \textmd{(Symmetry of the Green's Function)}}

For all $x,y\in\Omega$ with $x\not=y$ holds:
\begin{align}
G\left(x,y\right) & =G\left(y,x\right)
\end{align}



\subsubsection*{Proof}

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}[scale=0.7]
  \node at (0.5,4.5) {$\Omega$};
  \draw[clip] plot[smooth cycle,tension=.7,scale=2] coordinates{(0,-2) (1,-0.95) (2,0.2) (1.5,1.4) (0,2) (-2,-0.5)};
  \foreach \x in {-4.5,-3,...,2}     \draw (\x,-5) -- ({\x + 3},5);
  \draw[fill=white] (1,0) circle(1);
  \draw (1,0) +(-0.1,-0.1) -- +(0.1,0.1) +(-0.1,0.1) -- +(0.1,-0.1);
  \node[right=0pt] at (1,0) {$y$};
  \draw[dashed] (1,0) -- node[above left=-2pt]{$\varepsilon$} +({cos(90)},{sin(90)});
  \draw[fill=white] (-2.5,-1) circle(1);
  \draw (-2.5,-1) +(-0.1,-0.1) -- +(0.1,0.1) +(-0.1,0.1) -- +(0.1,-0.1);
  \node[right=0pt] at (-2.5,-1) {$x$};
  \draw[dashed] (-2.5,-1) -- node[above left=-2pt]{$\varepsilon$} +({cos(90)},{sin(90)});
\end{tikzpicture}
\par\end{centering}

\caption{$B_{\varepsilon}\left(x\right)\subseteq\Omega$, $B_{\varepsilon}\left(y\right)\subseteq\Omega$,
$B_{\varepsilon}\left(x\right)\cap B_{\varepsilon}\left(y\right)=\emptyset$}
\end{figure}


Choose $\varepsilon\in\mathbb{R}_{>0}$ such that $B_{\varepsilon}\left(x\right)$
and $B_{\varepsilon}\left(y\right)$ are disjoint subsets of $\Omega$.
\begin{align*}
u\left(z\right) & :=G\left(z,x\right) & v\left(z\right) & :=G\left(z,y\right)
\end{align*}
It holds $u,v\in C^{2}\left(\Omega\setminus\left(B_{\varepsilon}\left(x\right)\cup B_{\varepsilon}\left(y\right)\right)\right)$
and $u,v$ are harmonic. Moreover for $z\in\partial\Omega$ holds:
\begin{align*}
u\left(z\right) & =G\left(z,x\right)=0 & v\left(z\right) & =G\left(z,y\right)=0
\end{align*}
Apply again the second Green's identity:
\begin{align*}
0 & =\int_{\Omega\setminus\left(B_{\varepsilon}\left(x\right)\cup B_{\varepsilon}\left(y\right)\right)}\big(v\underbrace{\left(\upDelta u\right)}_{=0}-\underbrace{\left(\upDelta v\right)}_{=0}u\big)\dd\mu=\\
 & =\int_{\partial\Omega\setminus\left(B_{\varepsilon}\left(x\right)\cup B_{\varepsilon}\left(y\right)\right)}\left(v\left(\nabla_{\nu}u\right)-\left(\nabla_{\nu}v\right)u\right)\dd\mu_{\partial\Omega\setminus\left(B_{\varepsilon}\left(x\right)\cup B_{\varepsilon}\left(y\right)\right)}
\end{align*}
Moreover, the boundary values on $\partial\Omega$ vanish. We conclude:
\begin{align*}
0 & =\int_{\partial B_{\varepsilon}\left(x\right)\cup\partial B_{\varepsilon}\left(y\right)}\left(v\left(\nabla_{\nu}u\right)-\left(\nabla_{\nu}v\right)u\right)\dd\mu_{\partial B_{\varepsilon}\left(x\right)\cup\partial B_{\varepsilon}\left(y\right)}
\end{align*}
Consider the integral over $\partial B_{\varepsilon}\left(x\right)$
first. Since $G=\Gamma+h$ and $h\in C^{2}$ is bounded, we know for
all $z\in\partial B_{\varepsilon}\left(x\right)$:
\begin{align*}
u\left(z\right) & =G\left(z,x\right)\sim\Gamma\left(z,x\right)\sim\begin{cases}
\ln\left(\varepsilon\right) & \text{if }n=2\\
\varepsilon^{2-n} & \text{if }n>2
\end{cases}
\end{align*}
Since $\nabla_{\nu}v$ is also bounded due to $v\in C^{2}\left(B_{\varepsilon}\left(x\right)\right)$,
we get:
\begin{align*}
\int_{\partial B_{\varepsilon}\left(x\right)}\left(\nabla_{\nu}v\right)u\dd_{\partial B_{\varepsilon}\left(x\right)} & \xrightarrow{\varepsilon\searrow0}0
\end{align*}
The other term gives:
\begin{align*}
\int_{\partial B_{\varepsilon}\left(x\right)}v\left(\nabla_{\nu}u\right)\dd_{\partial B_{\varepsilon}\left(x\right)} & \sr ={\nabla_{\nu}u=\frac{\partial}{\partial\varepsilon}\Gamma\left(\varepsilon\right)+o_{0}\left(\varepsilon^{0}\right)=}{=\tilde{c}\varepsilon^{1-n}+o_{0}\left(\varepsilon^{0}\right)}cv\left(x\right)\varepsilon^{1-n}\underbrace{\int_{\partial B_{\varepsilon}\left(x\right)}1\dd\mu_{\partial B_{\varepsilon}\left(x\right)}}_{=n\omega_{n}\varepsilon^{n-1}}+o_{0}\left(\varepsilon\right)\xrightarrow{\varepsilon\searrow0}cv\left(x\right)
\end{align*}
Here the constant $c\not=0$ does not vanish. Now follows:
\begin{align*}
\int_{\partial B_{\varepsilon}\left(x\right)}\left(\left(\nabla_{\nu}v\right)u-v\left(\nabla_{\nu}u\right)\right)\dd\mu_{\partial B_{\varepsilon}\left(x\right)} & \xrightarrow{\varepsilon\searrow0}cv\left(x\right)\\
\int_{\partial B_{\varepsilon}\left(y\right)}\left(\left(\nabla_{\nu}v\right)u-v\left(\nabla_{\nu}u\right)\right)\dd\mu_{\partial B_{\varepsilon}\left(x\right)} & \xrightarrow{\varepsilon\searrow0}cu\left(y\right)
\end{align*}
Adding these two integrals gives:
\begin{align*}
0 & =c\left(v\left(x\right)-u\left(y\right)\right)\\
\Rightarrow\qquad G\left(x,y\right) & =v\left(x\right)=u\left(y\right)=G\left(y,x\right)
\end{align*}
\qqed

Any solution $u$ of the Dirichlet problem
\begin{align*}
\upDelta u & =f & u\big|_{\partial\Omega} & =\varphi
\end{align*}
has the representation:
\begin{align*}
u\left(y\right) & =\int_{\partial\Omega}u\left(x\right)\nabla_{\nu}G\left(x,y\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}G\left(x,y\right)f\left(x\right)\dd\mu\left(x\right)
\end{align*}
We define:
\begin{align*}
u\left(y\right) & :=\int_{\partial\Omega}\varphi\left(x\right)\nabla_{\nu}G\left(x,y\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}G\left(x,y\right)f\left(x\right)\dd\mu\left(x\right)
\end{align*}
Since $G$ is symmetric, $G\left(x,y\right)=G\left(y,x\right)$, the
equation $\upDelta_{x}G\left(x,y\right)=\delta^{\left(n\right)}\left(x-y\right)$
implies $\upDelta_{y}G\left(x,y\right)=\delta^{\left(n\right)}\left(x-y\right)$
as well. As a consequence, a formal computation gives:
\begin{align*}
\upDelta_{y}u\left(y\right) & =\int_{\partial\Omega}\varphi\left(x\right)\nabla_{\nu}\delta^{\left(n\right)}\left(x-y\right)\dd\mu_{\partial\Omega}\left(x\right)+\int_{\Omega}\delta^{\left(n\right)}\left(x-y\right)f\left(x\right)\dd\mu\left(x\right)
\end{align*}
For $y\in\Omega$ the first term vanishes, so we get:
\begin{align*}
\upDelta_{y}u\left(y\right) & =f\left(y\right)
\end{align*}
This formal calculation can be made rigorous, once an explicit Green's
function is given. Then one can also check, whether the boundary conditions
are satisfied.

%DATE: Fr 10.5.13


\subsection{Example \textmd{(Green's function for \texorpdfstring{$B_{R}\left(0\right)$}{Br(0)})}}

Now we want to construction the Green's function for $\Omega:=B_{R}\left(0\right)\subseteq\mathbb{R}^{n}$.
Consider an electric charge inside an earthed, electrically conducting
sphere, that screens the electric field, so that it vanishes outside
the sphere. The electric field inside the sphere can be calculated
using the concept of a mirror charge, which ensures, that the electric
field is perpendicular to the sphere.

\begin{figure}[H]
\noindent \begin{centering}
\hfill{}\subfloat[charge in earthed sphere]{\begin{tikzpicture}[scale=1]
  \draw (0,0) circle (2cm);
  \draw (1,0) node[above]{$y$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (0,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (0,0) -- node[above]{$R$} (-2,0);
  \draw ({2cm*cos(-10)},{2cm*sin(-10)}) -- ++(0,-2) +(-0.7,0) -- +(0.7,0) ++(0,-0.2) +(-0.4,0) -- +(0.4,0) ++(0,-0.2) +(-0.2,0) -- +(0.2,0);
\end{tikzpicture}}\hfill{}\subfloat[equivalent mirror charge outside sphere]{\begin{tikzpicture}
  \draw[dashed] (0,0) circle (2cm);
  \draw (0,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (1,0) node[above]{$y$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (4,0) node[above]{$\tilde{y}$} node[below]{mirror charge} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw[dashed] ({2cm*cos(-10)},{2cm*sin(-10)}) -- ++(0,-2) +(-0.7,0) -- +(0.7,0) ++(0,-0.2) +(-0.4,0) -- +(0.4,0) ++(0,-0.2) +(-0.2,0) -- +(0.2,0);
\end{tikzpicture}}\hfill{}
\par\end{centering}

\caption{The mirror charge replicates the boundary conditions of the sphere.}
\end{figure}


The position of the mirror charge must be:
\begin{align*}
\tilde{y} & =\frac{R^{2}}{y^{2}}\cdot y\\
\Rightarrow\qquad\norm y\cdot\norm{\tilde{y}} & =R^{2}
\end{align*}
Compare this with the inversion with respect to the unit circle (\foreignlanguage{ngerman}{Spiegelung
am Einheitskreis}) in the complex plane:
\begin{align*}
z & \mapsto\frac{1}{\overline{z}}=\frac{z}{\abs z^{2}}
\end{align*}
This motivates the ansatz:
\begin{align*}
G\left(x,y\right) & =\begin{cases}
\Gamma\left(\norm{x-y}\right)-\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & \text{if }y\not=0\\
\Gamma\left(\norm x\right)-\Gamma\left(R\right) & \text{if }y=0
\end{cases}
\end{align*}
Let us verify that $G$ has all the required properties:
\begin{align*}
G\left(x,y\right)-\Gamma\left(x,y\right) & =\begin{cases}
-\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & \text{if }y\not=0\\
-\Gamma\left(R\right) & \text{if }y=0
\end{cases}
\end{align*}
For $x,y\in B_{R}\left(0\right)$ follows $\tilde{y}\not\in B_{R}\left(0\right)$
and thus $x\not=\tilde{y}$. So $\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)$
is smooth if $y\not=0$. To see smoothness in the case $y=0$, we
rewrite:
\begin{align}
\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & =\Gamma\left(\sqrt{\frac{y^{2}}{R^{2}}\left(x^{2}+\tilde{y}^{2}-2\left\langle x,\tilde{y}\right\rangle \right)}\,\right)=\nonumber \\
 & =\Gamma\left(\sqrt{\frac{y^{2}}{R^{2}}\left(x^{2}+\frac{R^{4}}{y^{2}}-2\frac{R^{2}}{y^{2}}\left\langle x,y\right\rangle \right)}\,\right)=\nonumber \\
 & =\Gamma\left(\sqrt{\frac{x^{2}y^{2}}{R^{2}}+R^{2}-2\left\langle x,y\right\rangle }\,\right)\label{eq:G-symmetric}
\end{align}
The argument is smooth at $y=0$ and it holds:
\begin{align*}
\lim_{y\to0}\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & =\Gamma\left(R\right)
\end{align*}
So $G\left(x,y\right)-\Gamma\left(x,y\right)$ is in $C^{2}$ for
$x,y\in\Omega$.

For $x\in\partial\Omega$, i.e. $\norm x=R$, holds:
\begin{align*}
G\left(x,y\right) & =\Gamma\left(\norm{x-y}\right)-\Gamma\left(\sqrt{\frac{\norm x^{2}\norm y^{2}}{R^{2}}+R^{2}-2\left\langle x,y\right\rangle }\,\right)=\\
 & \sr ={\norm x=R}{}\Gamma\left(\norm{x-y}\right)-\Gamma\left(\sqrt{\norm y^{2}+\norm x^{2}-2\left\langle x,y\right\rangle }\,\right)=\\
 & =\Gamma\left(\norm{x-y}\right)-\Gamma\left(\sqrt{\norm{x-y}^{2}}\:\right)=0
\end{align*}
Thus $G\left(x,y\right)$ satisfies the boundary condition $G\left(x,y\right)=0$
for $x\in\partial\Omega$.\\
Now we show that $G\left(x,y\right)-\Gamma\left(x,y\right)$ is harmonic:
\begin{align*}
G\left(x,y\right)-\Gamma\left(x,y\right) & =-\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\\
\nabla\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & =\Gamma'\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\frac{\norm y}{R}\nabla\norm{x-\tilde{y}}\\
\upDelta\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & =\Gamma''\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\left(\frac{\norm y}{R}\right)^{2}\left(\nabla\norm{x-\tilde{y}}\right)^{2}+\\
 & \qquad+\Gamma'\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\frac{\norm y}{R}\upDelta\norm{x-\tilde{y}}
\end{align*}
We know:
\begin{align*}
0 & =\upDelta\Gamma=\frac{1}{r^{n-1}}\partial_{r}\left(r^{n-1}\partial_{r}\Gamma\right)\\
\Rightarrow\qquad0 & =\Gamma''\left(r\right)+\frac{n-1}{r}\Gamma'\left(r\right)
\end{align*}
With this follows:
\begin{align*}
\upDelta\Gamma\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right) & =-\left(n-1\right)\frac{R}{\norm y\norm{x-\tilde{y}}}\Gamma'\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\left(\frac{\norm y}{R}\right)^{2}\left(\nabla\norm{x-\tilde{y}}\right)^{2}+\\
 & \qquad+\Gamma'\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\frac{\norm y}{R}\upDelta\norm{x-\tilde{y}}=\\
 & =\frac{\norm y}{R}\Gamma'\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\left(\frac{-\left(n-1\right)}{\norm{x-\tilde{y}}}\left(\nabla\norm{x-\tilde{y}}\right)^{2}+\upDelta\norm{x-\tilde{y}}\right)=\\
 & =\frac{\norm y}{R}\Gamma'\left(\frac{\norm y}{R}\norm{x-\tilde{y}}\right)\left(\frac{-\left(n-1\right)}{\norm{x-\tilde{y}}}\left(\frac{x-\tilde{y}}{\norm{x-\tilde{y}}}\right)^{2}+\frac{n-1}{\norm{x-\tilde{y}}}\right)=0
\end{align*}
Thus $G\left(x,y\right)$ is the desired Green's function. From (\ref{eq:G-symmetric})
one sees explicitly:
\begin{align*}
G\left(x,y\right) & =G\left(y,x\right)
\end{align*}
We hope that the solution of the Dirichlet problem
\begin{align*}
\upDelta u & =f & u\big|_{\partial B_{R}\left(0\right)} & =\varphi
\end{align*}
is given by the Green's representation:
\begin{align*}
u\left(y\right) & =\int_{B_{R}\left(0\right)}G\left(x,y\right)f\left(x\right)\dd\mu\left(x\right)+\int_{B_{R}\left(0\right)}\nabla_{\nu}G\left(x,y\right)\varphi\left(x\right)\dd\mu_{\partial B_{R}\left(0\right)}\left(x\right)
\end{align*}
Computing $\nabla_{\nu}G\left(x,y\right)$ for $x\in\partial B_{R}\left(0\right)$
gives:
\begin{align*}
\nabla_{\nu}G\left(x,y\right) & =\frac{R^{2}-y^{2}}{n\omega_{n}R}\cdot\frac{1}{\norm{x-y}^{n}}
\end{align*}



\subsection{Theorem \textmd{(Poisson representation)}}

The function
\begin{align}
u\left(y\right) & :=\begin{cases}
\frac{R^{2}-y^{2}}{n\omega_{n}R}\int_{\partial B_{R}\left(0\right)}\frac{\varphi\left(x\right)}{\norm{x-y}^{n}}\dd\mu_{\partial B_{R}\left(0\right)} & \text{if }y\in B_{R}\left(0\right)\\
\varphi\left(y\right) & \text{if }y\in\partial B_{R}\left(0\right)
\end{cases}
\end{align}
with $\varphi\in C^{0}\left(\partial B_{R}\left(0\right)\right)$
has the following properties:
\begin{itemize}
\item $u\in C^{0}\left(\overline{B_{R}\left(0\right)}\right)$
\item $u\in C^{2}\left(B_{R}\left(0\right)\right)$
\item $u$ is harmonic in $B_{R}\left(0\right)$.
\end{itemize}

\subsubsection*{Proof}

This can be shown using Green's representation, computing the boundary
values and justifying that the $y$-derivative may be taken inside
the integral.\qqed


\section{The Mean Value Theorem and the Maximum Principle for Harmonic Functions}


\subsection{Theorem \textmd{(Mean Value Formulas)}}

A continuous function $u:\Omega\to\mathbb{R}$ with $\Omega\subseteq\mathbb{R}^{n}$
is harmonic, i.e. $u\in C^{2}\left(\Omega\right)$ and $\upDelta u=0$,
if and only if for all $x_{0}\in\Omega$ and all balls $B_{r}\left(x_{0}\right)\subseteq\Omega$
one of the following mean value formulas holds:
\begin{align}
u\left(x_{0}\right) & =\frac{1}{n\omega_{n}r^{n-1}}\int_{\partial B_{r}\left(x_{0}\right)}u\left(x\right)\dd\mu_{\partial B_{R}}\left(x\right) &  & \text{(spherical mean)}\\
u\left(x_{0}\right) & =\frac{1}{\omega_{n}r^{n}}\int_{B_{r}\left(x_{0}\right)}u\left(x\right)\dd\mu\left(x\right) &  & \text{(mean over ball)}
\end{align}
In this case, the other formula holds as well.


\subsubsection*{Proof}

``$\Rightarrow$'': Let $u\in C^{2}\left(\Omega\right)$ be harmonic
and $B_{r}\left(x_{0}\right)\subseteq\Omega$. Then $B_{r}\left(y\right)\subseteq\Omega$
holds also for all $y$ in a neighborhood of $x_{0}$. The function
\begin{align*}
H\left(x,y\right) & =\Gamma\left(x,y\right)-\Gamma\left(r\right)
\end{align*}
coincides up to a constant with the fundamental solution and it holds
$H\left(x,y\right)=0$ for $x\in\partial B_{r}\left(y\right)$. Using
Green's representation gives: 
\begin{align*}
u\left(y\right) & =\int_{B_{r}\left(y\right)}\underbrace{\left(\upDelta u\right)\left(x\right)}_{=0}\cdot H\left(x,y\right)\dd\mu\left(x\right)+\\
 & \qquad+\int_{\partial B_{r}\left(y\right)}\bigg(u\left(x\right)\nabla_{\nu}H\left(x,y\right)-\underbrace{H\left(x,y\right)}_{=0}\nabla_{\nu}u\left(x\right)\bigg)\dd\mu_{\partial B_{r}\left(y\right)}\left(x\right)=\\
 & =\int_{\partial B_{r}\left(y\right)}u\left(x\right)\nabla_{\nu}H\left(x,y\right)\dd\mu_{\partial B_{r}\left(y\right)}\left(x\right)
\end{align*}
\begin{align*}
\nabla_{\nu}H\left(x,y\right) & =\nabla_{\nu,x}\Gamma\left(\norm{x-y}\right)\sr ={r:=\norm{x-y}}{}\frac{\partial}{\partial r}\Gamma\left(r\right)=\frac{1}{n\omega_{n}r^{n-1}}
\end{align*}
For $y:=x_{0}$ follows the spherical mean formula:
\begin{align*}
u\left(x_{0}\right) & =\frac{1}{n\omega_{n}r^{n-1}}\int_{\partial B_{r}\left(x_{0}\right)}u\left(x\right)\dd\mu_{\partial B_{R}}\left(x\right)
\end{align*}
Now follows:
\begin{align}
\int_{B_{r}\left(y\right)}u\left(x\right)\dd\mu\left(x\right) & \stackrel{\text{Fubini}}{=}\int_{0}^{r}\dd\rho\int_{\partial B_{\rho}\left(y\right)}u\left(x\right)\dd\mu_{\partial B_{\rho}\left(y\right)}\left(x\right)=\int_{0}^{r}u\left(y\right)n\omega_{n}\rho^{n-1}\dd\rho=\nonumber \\
 & =u\left(y\right)n\omega_{n}\cdot\frac{1}{n}r^{n}=\omega_{n}r^{n}u\left(y\right)\label{eq:comp-spherical-mean}
\end{align}
``$\Leftarrow$'': Let $u\in C^{0}\left(\Omega\right)$ be continuous.
If the formula for the spherical mean holds, the computation (\ref{eq:comp-spherical-mean})
gives the formula for means over balls. Let us show that the formula
for means over balls implies the formula for spherical means. So assume
for fixed $y$ and all $r<r_{0}$:
\begin{align*}
u\left(y\right) & =\frac{1}{\omega_{n}r^{n}}\int_{B_{r}\left(y\right)}u\left(x\right)\dd\mu\left(x\right)
\end{align*}
Thus using Fubini's theorem follows:
\begin{align*}
r^{n}u\left(y\right) & =\frac{1}{\omega_{n}}\overbrace{\int_{0}^{r}\dd\rho\underbrace{\int_{\partial B_{\rho}\left(y\right)}u\left(x\right)\dd\mu_{\partial B_{\rho}\left(y\right)}\left(x\right)}_{\text{continuous in }\rho}}^{C^{1}\text{ in }r}
\end{align*}
Differentiation on both sides with respect to $r$ gives the formula
for spherical means:
\begin{align*}
nr^{n-1}u\left(y\right) & =\frac{1}{\omega_{n}}\int_{\partial B_{r}\left(y\right)}u\left(x\right)\dd\mu_{\partial B_{r}\left(y\right)}\left(x\right)
\end{align*}
Next we show, that $u$ is smooth in $\Omega$. The idea is to mollify
by convolution. Choose:
\begin{align*}
\varrho\left(t\right) & :=\begin{cases}
c_{n}e^{\frac{1}{t^{2}-1}} & \text{if }-1<t<1\\
0 & \text{otherwise}
\end{cases}\in C_{0}^{\infty}\left(\mathbb{R}\right)
\end{align*}


\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
 \begin{axis}[width=9cm, axis x line=middle, axis y line=middle, ytick={0}, xlabel=$t$, ylabel=$\varrho$,xmin=-1.5, xmax=1.5, ymax=0.4 ,samples=300]
  \addplot[mark=none,domain=-0.9999:0.9999] {exp(1/(x^2-1))};
 \end{axis}
\end{tikzpicture} \caption{$\varrho\left(t\right)$ is smooth and has compact support $\left[-1,1\right]$.}
\end{figure}


Then holds $\varrho\left(\norm x\right)\in C_{0}^{\infty}\left(\mathbb{R}^{n}\right)$.
Choose $c_{n}$ such that holds:
\begin{align*}
1 & =\int_{\mathbb{R}^{n}}\varrho\left(\norm x\right)\dd^{n}x
\end{align*}
Now for $\varepsilon\in\mathbb{R}_{>0}$ set:
\begin{align*}
\varrho_{\varepsilon}\left(x,y\right) & :=\frac{1}{\varepsilon^{n}}\varrho\left(\frac{\norm{x-y}}{\varepsilon}\right)
\end{align*}
Then still holds:
\begin{align*}
1 & =\int_{\mathbb{R}^{n}}\varrho_{\varepsilon}\left(x,y\right)\dd^{n}x
\end{align*}
Also we know $\varrho_{\varepsilon}\left(x,.\right)\in C_{0}^{\infty}\left(B_{2\varepsilon}\left(x\right)\right)$.
Choose $\varepsilon$ so small that $B_{2\varepsilon}\left(y\right)\subseteq\Omega$
and use $\varrho_{\varepsilon}$ as our convolution kernel to define:
\begin{align*}
u_{\varepsilon}\left(y\right) & :=\int_{\Omega}\varrho_{\varepsilon}\left(x,y\right)u\left(x\right)\dd\mu\left(x\right)
\end{align*}
Now $u_{\varepsilon}$ is a smooth function, because $\varrho_{\varepsilon}\left(x,.\right)$
is smooth and any derivative with respect to $y$ can be exchanged
with the integral, since the integration volume is compact.
\begin{align*}
u_{\varepsilon}\left(y\right) & =\frac{1}{\varepsilon^{n}}\int_{\Omega}\varrho\left(\frac{\norm{x-y}}{\varepsilon}\right)u\left(x\right)\dd\mu\left(x\right)
\end{align*}
Choose polar coordinates around $y$ and use Fubini to get:
\begin{align*}
u_{\varepsilon}\left(y\right) & =\frac{1}{\varepsilon^{n}}\int_{0}^{2\varepsilon}\dd r\int_{\partial B_{r}\left(y\right)}\varrho\left(\frac{r}{\varepsilon}\right)u\left(x\right)\dd\mu_{\partial B_{r}\left(y\right)}\left(x\right)=\\
 & \stackrel{\text{spherical mean}}{=}\frac{1}{\varepsilon^{n}}\int_{0}^{2\varepsilon}\dd r\varrho\left(\frac{r}{\varepsilon}\right)u\left(y\right)n\omega_{n}r^{n-1}=\\
 & =u\left(y\right)\frac{1}{\varepsilon^{n}}\int_{0}^{2\varepsilon}\dd r\int_{\partial B_{r}\left(y\right)}1\cdot\varrho\left(\frac{r}{\varepsilon}\right)\dd\mu_{\partial B_{r}\left(y\right)}\left(x\right)=\\
 & =u\left(y\right)\int_{\mathbb{R}^{n}}\varrho_{\varepsilon}\left(x,y\right)\dd^{n}x=u\left(y\right)
\end{align*}
Thus $u$ is smooth.

Compute $\upDelta u$ using the theorem of Gauss:
\begin{align*}
\int_{B_{r}\left(y\right)}\left(\upDelta u\left(x\right)\right)\dd\mu\left(x\right) & =\int_{\partial B_{r}\left(y\right)}\nabla_{\nu}u\dd\mu_{\partial B_{r}\left(y\right)}
\end{align*}
With
\begin{align*}
\omega & :=\frac{x-y}{\norm{x-y}}\in S^{n-1}
\end{align*}
we get:
\begin{align*}
\int_{B_{r}\left(y\right)}\left(\upDelta u\left(x\right)\right)\dd\mu\left(x\right) & =r^{n-1}\int_{\partial B_{1}\left(0\right)}\frac{\partial}{\partial r}u\left(y+r\omega\right)\dd\mu_{\partial B_{1}\left(0\right)}\left(\omega\right)=\\
 & =r^{n-1}\frac{\partial}{\partial r}\left(\frac{1}{r^{n-1}}\int_{\partial B_{r}\left(y\right)}u\left(x\right)\dd\mu_{\partial B_{r}\left(y\right)}\left(x\right)\right)=\\
 & \sr ={\text{mean value}}{\text{property}}r^{n-1}\frac{\partial}{\partial r}\left(\frac{1}{r^{n-1}}u\left(y\right)n\omega_{n}r^{n-1}\right)=0
\end{align*}
In the limes $r\to0$ follows $\upDelta u\left(y\right)=0$ since
$u$ is smooth. Thus $u$ is harmonic.\qqed

%DATE: Mi 15.5.13


\subsection{Theorem \textmd{(Strong Maximum Principle)}}

Let $u\in C^{2}\left(\Omega\right)$ be a harmonic function on an
open and connected subset $\Omega\subseteq\mathbb{R}^{n}$. If a point
$x_{0}\in\Omega$ with
\begin{align*}
u\left(x_{0}\right) & =\sup_{x\in\Omega}\left(u\right)
\end{align*}
or
\begin{align*}
u\left(x_{0}\right) & =\inf_{x\in\Omega}\left(u\right)
\end{align*}
exists, then $u$ is constant.


\subsubsection*{Proof}

Since $-u$ is also harmonic, it suffices to consider the case $u\left(x_{0}\right)=\sup_{\Omega}\left(u\right)$,
so assume
\begin{align*}
u\left(x_{0}\right) & =\sup_{\Omega}\left(u\right)=:M
\end{align*}
and define:
\begin{align*}
\Omega_{M} & :=\left\{ x\in\Omega\big|u\left(x\right)=M\right\} 
\end{align*}
Clearly, $\Omega_{M}$ is not empty, because $x_{0}\in\Omega_{M}$.
By continuity, $\Omega_{M}$ is closed in $\Omega$ with respect to
the relative topology. Thus it suffices to show that $\Omega_{M}$
is open. Consider $y\in\Omega_{M}$, i.e. $u\left(y\right)=M$, and
choose $r\in\mathbb{R}_{>0}$ with $B_{r}\left(y\right)\subseteq\Omega$.
\begin{align*}
0 & =u\left(y\right)-M\sr ={\text{mean value}}{\text{property}}\frac{1}{\omega_{n}r^{n}}\int_{B_{r}\left(y\right)}\underbrace{\left(u\left(x\right)-M\right)}_{\le0}\dd\mu\left(x\right)
\end{align*}
Since $u\left(x\right)\le M$, the integrand is non-positive and continuous.
So $u\left(x\right)=M$ for all $x\in B_{r}\left(y\right)$ and thus
follows $B_{r}\left(y\right)\subseteq\Omega_{M}$. This means that
$\Omega_{M}$ is open. Since it is also non-empty, closed and connected,
the only possibility is $\Omega_{M}=\Omega$.\qqed


\subsection{Theorem \textmd{(Weak Maximum Principle)\label{sub:Thm-Weak-Maximum-Principle}}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be open and bounded and $u\in C^{0}\left(\overline{\Omega}\right)$
be harmonic. Then holds for all $x\in\Omega$:
\begin{align*}
\min_{y\in\partial\Omega}\left(u\left(y\right)\right) & \le u\left(x\right)\le\max_{y\in\partial\Omega}\left(u\left(y\right)\right)
\end{align*}



\subsubsection*{Proof}

If the statement were false, there would be an interior maximum or
minimum at point $x_{0}\in\Omega$, since $\overline{\Omega}$ is
compact, which ensures that the maximum and minimum of $u$ are attained
in $\overline{\Omega}$.\\
Let $\tilde{\Omega}\subseteq\Omega$ be the connected component of
$\Omega$ which contains $x_{0}$.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw plot[smooth cycle,tension=1] coordinates{(0,0) (1,0) (0,1) (-1,0)};
  \draw (0.3,0.5) node[left]{$x_0$} +(-0.1,-0.1) -- +(0.1,0.1) +(-0.1,0.1) -- +(0.1,-0.1);
  \node at (0,1.3) {$\tilde{\Omega}$};
  \draw plot[smooth cycle,tension=1] coordinates{(2,-0.5) (4,0) (3,2)};
  \node (a) at (1.5,1) {$\Omega$};
  \draw (a.-130) -- (0.9,0.5) (a.-70) -- (2,0.5);
\end{tikzpicture}\caption{$\tilde{\Omega}$ is the connected component of $\Omega$ that contains
$x_{0}$.}
\end{figure}


The strong maximum principle implies $u=u\left(x_{0}\right)$ on $\tilde{\Omega}$.
Thus follows:
\begin{align*}
\min_{\partial\tilde{\Omega}}\left(u\right) & =u\left(x_{0}\right)=\max_{\partial\tilde{\Omega}}\left(u\right)\\
\Rightarrow\qquad\min_{\partial\Omega}\left(u\right)\le\min_{\partial\tilde{\Omega}}\left(u\right) & =u\left(x_{0}\right)=\max_{\partial\tilde{\Omega}}\left(u\right)\le\max_{\partial\Omega}\left(u\right)
\end{align*}
Therefore $x_{0}$ cannot be an interior maximum or minimum in contradiction
to our assumption.\qqed


\subsection{Corollary \textmd{(Uniqueness of Solutions of the Poisson Equation)}}

Let $u,v\in C^{0}\left(\overline{\Omega}\right)\cap C^{2}\left(\Omega\right)$
be solutions of the Poisson equation:
\begin{align*}
\upDelta u & =f=\upDelta v
\end{align*}
If moreover holds $u\le v$ on $\partial\Omega$, then follows $u\le v$
in $\Omega$.\\
In particular, $u=v$ on $\partial\Omega$ implies $u=v$ in $\Omega$.


\subsubsection*{Proof}

The function $h:=u-v$ is harmonic
\begin{align*}
\upDelta h & =\upDelta u-\upDelta v=f-f=0
\end{align*}
and on $\partial\Omega$ holds $h\le0$. The weak maximum principle
gives $h\le0$ in $\Omega$.\qqed


\subsection{Corollary \textmd{(Liouville's Theorem)}}

If $u:\mathbb{R}^{n}\to\mathbb{R}$ is harmonic and bounded, then
$u$ is constant.


\subsubsection*{Proof}

Choose $y_{1},y_{2}\in\mathbb{R}^{n}$. Then for any $r\in\mathbb{R}_{>0}$
holds for $i\in\left\{ 1,2\right\} $.
\begin{align*}
u\left(y_{i}\right) & =\frac{1}{\omega_{n}r^{n}}\int_{B_{r}\left(y_{i}\right)}u\left(x\right)\dd\mu\left(x\right)
\end{align*}
The difference is:
\begin{align*}
u\left(y_{1}\right)-u\left(y_{2}\right) & =\frac{1}{\omega_{n}r^{n}}\left(\int_{B_{r}\left(y_{1}\right)}u\left(x\right)\dd\mu\left(x\right)-\int_{B_{r}\left(y_{2}\right)}u\left(x\right)\dd\mu\left(x\right)\right)
\end{align*}


\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw (0,0) node[left]{$y_1$} +(-0.1,-0.1) -- +(0.1,0.1) +(-0.1,0.1) -- +(0.1,-0.1);
  \draw (0.5,0) node[right]{$y_2$} +(-0.1,-0.1) -- +(0.1,0.1) +(-0.1,0.1) -- +(0.1,-0.1);
  \draw[dashed] (0,0) -- node[above]{$r$} +({2*cos(150)},{2*sin(150)});
  \draw[dashed] (0.5,0) -- node[below right=-3pt]{$r$} +({2*cos(60)},{2*sin(60)});
  \draw[red,dashed] (0,0) -- node[right]{$r_-$} +({1.5*cos(100)},{1.5*sin(100)});
  \draw[red,dashed] (0,0) -- node[right]{$r_+$} +({2.5*cos(260)},{2.5*sin(260)});
\begin{scope}
  \draw[clip] (2,0) arc (0:360:2)  (2.5,0) arc (0:-360:2);
  \foreach \x in {-4.9,-3.8,...,3}     \draw (-4,\x) -- (4,{\x +4});
\end{scope}
  \draw[red,thick] (0,0) circle(1.5) circle (2.5);
\end{tikzpicture}\caption{$r_{-}:=r-\norm{y_{1}-y_{2}}$, $r_{+}:=r+\norm{y_{1}-y_{2}}$}
\end{figure}


So we get:
\begin{align*}
u\left(y_{1}\right)-u\left(y_{2}\right) & \le\frac{1}{\omega_{n}r^{n}}\underbrace{\sup_{\mathbb{R}^{n}}\left(\abs u\right)}_{<\infty}\mu\left(B_{r}\left(y_{1}\right)\setminus B_{r}\left(y_{2}\right)\cup B_{r}\left(y_{2}\right)\setminus B_{r}\left(y_{1}\right)\right)\le\\
 & \le\frac{1}{\omega_{n}r^{n}}\sup_{\mathbb{R}^{n}}\left(\abs u\right)\left(\mu\left(B_{r+\norm{y_{2}-y_{1}}}\left(y_{1}\right)\right)-\mu\left(B_{r-\norm{y_{2}-y_{1}}}\left(y_{1}\right)\right)\right)\le\\
 & \le\frac{1}{\omega_{n}r^{n}}\sup_{\mathbb{R}^{n}}\left(\abs u\right)\left(\omega_{n}\left(r+\norm{y_{2}-y_{1}}\right)^{n}-\omega_{n}\left(r-\norm{y_{2}-y_{1}}\right)^{n}\right)=\\
 & =\sup_{\mathbb{R}^{n}}\left(\abs u\right)\frac{1}{r^{n}}\left(2n\norm{y_{2}-y_{1}}r^{n-1}+\ldots\right)\xrightarrow{r\to\infty}0
\end{align*}
This gives $u\left(y_{1}\right)=u\left(y_{2}\right)$ and thus $u$
is constant.\qqed


\section{The Harnack Inequality for Harmonic Functions}


\subsection{Theorem \textmd{(Harnack inequality)}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be open and $u:\Omega\to\mathbb{R}$
harmonic and non-negative. Then for every compact, (for simplicity)
connected subset $\Omega'\subseteq\Omega$ there is a constant $c=c\left(\Omega,\Omega'\right)\in\mathbb{R}$,
which is independent of $u$, such that holds:
\begin{align*}
\sup_{\Omega'}\left(u\right) & \le c\cdot\inf_{\Omega'}\left(u\right)
\end{align*}



\subsubsection{Proof}

We begin with the case $\Omega'=B_{r}\left(x_{0}\right)$ and $B_{4r}\left(x_{0}\right)\subseteq\overline{\Omega}$.
Choose $y_{1},y_{2}\in\Omega'$.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}[scale=0.8]
  \draw plot[smooth cycle] coordinates {(-4,-4) (4,-3) (5,4) (-5,4)};
  \node at (-4,4) {$\overline{\Omega}$};
  \node at (1.3,0) {$\Omega '$};
  \node (x0) at (0,0){};
  \node (y1) at (-0.3,0.5){};
  \node (y2) at (0,-0.7){};
  \draw (x0) node[left] {$x_0$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (y1) node[right] {$y_1$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (y2) node[right] {$y_2$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (x0) circle(1) circle(4);
  \draw[red] (y1) circle(1);
  \draw[blue] (x0) circle(2);
  \draw[green] (y2) circle(3);
\end{tikzpicture}\caption{${\color{red}B_{r}\left(y_{1}\right)}\subseteq{\color{blue}B_{2r}\left(x_{0}\right)}\subseteq{\color{green}B_{3r}\left(y_{2}\right)}\subseteq B_{4r}\left(x_{0}\right)\subseteq\overline{\Omega}$}
\end{figure}


It holds:
\begin{align*}
u\left(y_{1}\right) & =\frac{1}{\omega_{n}r^{n}}\int_{{\color{red}B_{r}\left(y_{1}\right)}}u\left(x\right)\dd\mu\left(x\right)\sr{\le}{u\ge0}{}\frac{1}{\omega_{n}r^{n}}\int_{{\color{blue}B_{2r}\left(x_{0}\right)}}u\left(x\right)\dd\mu\left(x\right)\le\\
 & \le\frac{1}{\omega_{n}r^{n}}\int_{{\color{green}B_{3r}\left(y_{2}\right)}}u\left(x\right)\dd\mu\left(x\right)\le\frac{3^{n}}{\omega_{n}\left(3r\right)^{n}}\int_{{\color{green}B_{3r}\left(y_{2}\right)}}u\left(x\right)\dd\mu\left(x\right)=3^{n}u\left(y_{2}\right)
\end{align*}
Taking the supremum over $y_{1}$ and the infimum over $y_{2}$ we
get:
\begin{align*}
\sup_{B_{r}\left(x_{0}\right)}\left(u\right) & \le3^{n}\inf_{B_{r}\left(x_{0}\right)}\left(u\right)
\end{align*}
Now consider a general compact subset $\Omega'\subseteq\Omega$. If
$\Omega'$ is empty, we have nothing to show, so consider a non-empty
$\Omega'$. We want to find a $r\in\mathbb{R}_{>0}$ such that for
every $x\in\Omega'$ holds $B_{4r}\left(x\right)\subseteq\Omega$.
For $\Omega=\mathbb{R}^{n}$ we can choose any $r$. Otherwise $\partial\Omega\subseteq\mathbb{R}^{n}$
is non-empty and closed. Since $\Omega'$ is also non-empty and even
compact, the distance $\text{dist}\left(\Omega',\partial\Omega\right)$
is attained. This must be larger than zero, because from $\partial\Omega\cap\Omega=\emptyset$
and $\Omega'\subseteq\Omega$ follows $\partial\Omega\cap\Omega'=\emptyset$.
So we can choose any $r\in\mathbb{R}_{>0}$ with:
\begin{align*}
r & <\frac{1}{4}\text{dist}\left(\Omega',\partial\Omega\right)
\end{align*}
We cover $\Omega'$ by a finite number $\Omega_{1},\ldots,\Omega_{N}$
of balls of radius $r.$ On each of the sets $\Omega_{l}$ holds:
\begin{align*}
\sup\left(u\right) & \le3^{n}\text{inf}\left(u\right)
\end{align*}
One can proceed inductively as follows: Without loss of generality
holds $\sup_{\Omega'}\left(u\right)\le\sup_{\Omega_{1}}\left(u\right)$,
otherwise change the numbering of the $\Omega_{i}$. Since $\Omega'$
is connected, one can choose a finite number $\Omega_{2},\ldots,\Omega_{k}$
from the balls of the covering such that for $i\in\left\{ 1,\ldots,k-1\right\} $
holds
\begin{align*}
\Omega_{i}\cap\Omega_{i+1} & \not=\emptyset
\end{align*}
and $\inf_{\Omega_{k}}\left(u\right)\le\inf_{\Omega'}\left(u\right)$.
If $\inf_{\Omega_{1}}\left(u\right)\le\inf_{\Omega'}\left(u\right)$,
we can choose $k=1$. It holds:
\begin{align*}
\sup_{\Omega_{i}}\left(u\right) & \le3^{n}\inf_{\Omega_{i}}\left(u\right)\sr{\le}{z_{i}\in\Omega_{i}\cap\Omega_{i+1}}{}3^{n}u\left(z_{i}\right)\le3^{n}\sup_{\Omega_{i+1}}\left(u\right)
\end{align*}
Inductively one gets after $k-1$ steps:
\begin{align*}
\sup_{\Omega'}\left(u\right)\le\sup_{\Omega_{1}}\left(u\right) & \le3^{\left(k-1\right)n}\sup_{\Omega_{k}}\left(u\right)\le3^{kn}\inf_{\Omega_{k}}\left(u\right)\le3^{kn}\inf_{\Omega'}\left(u\right)
\end{align*}
In the worst case of $u$, we have $k=N$ and therefore for any $u$
holds:
\begin{align*}
\sup_{\Omega'}\left(u\right) & \le3^{Nn}\inf_{\Omega'}\left(u\right)
\end{align*}
Thus the theorem is proven.\qqed


\subsection{Corollary \textmd{(Harnack's Convergence Theorem)}}

Let $u_{n}:\Omega\to\mathbb{R}^{n}$ be a monotonically increasing
sequence of harmonic functions. Assume that there is a $y\in\Omega$,
where $u_{n}\left(y\right)$ is a bounded sequence.\\
Then the functions $u_{n}$ converge locally uniformly, i.e. uniformly
in a small neighborhood of $y$, to a harmonic function $u$.


\subsubsection*{Proof}

Since the sequence $u_{n}\left(y\right)$ is bounded and monotonically
increasing, it is convergent, i.e. for all $\varepsilon\in\mathbb{R}_{>0}$
there exists a $N\in\mathbb{N}$ such that for all $k>m>N$ holds:
\begin{align*}
u_{k}\left(y\right)-u_{m}\left(y\right) & <\varepsilon
\end{align*}
The function $u_{k}-u_{m}$ is non-negative and harmonic. Now choose
a bounded and connected neighborhood $\Omega'$ of $y$ with $\overline{\Omega'}\subseteq\Omega$.
According to the Harnack inequality holds:
\begin{align*}
\sup_{\overline{\Omega'}}\left(u_{k}-u_{m}\right) & \le c\left(\Omega,\overline{\Omega'}\right)\cdot\inf_{\overline{\Omega'}}\left(u_{k}-u_{m}\right)\le c\left(u_{k}\left(y\right)-u_{m}\left(y\right)\right)\xrightarrow{k,m\to\infty}0
\end{align*}
Hence $u_{k}$ converges \emph{uniformly} in $\Omega'$ to a function
$u\in C^{0}\left(\Omega'\right)$. The function $u$ is harmonic,
because for all $x_{0}\in\Omega'$ and $r\in\mathbb{R}_{>0}$ such
that $B_{r}\left(x_{0}\right)\subseteq\Omega'$ holds:
\begin{align*}
u_{k}\left(x_{0}\right) & =\frac{1}{\omega_{n}r^{n}}\int_{B_{r}\left(x_{0}\right)}u_{k}\left(x\right)\dd\mu\left(x\right)
\end{align*}
From $u_{k}\left(x_{0}\right)\to u\left(x_{0}\right)$ and $u_{k}\left(x\right)\rightrightarrows u\left(x\right)$
follows:
\begin{align*}
u\left(x_{0}\right) & =\frac{1}{\omega_{n}r^{n}}\int_{B_{r}\left(x_{0}\right)}u\left(x\right)\dd\mu\left(x\right)
\end{align*}
So $u$ has the mean-value property and is thus harmonic.\qqed


\chapter{Perron Method of Sub- and Supersolutions}

Consider the Dirichlet problem on a bounded $\Omega\subseteq\mathbb{R}^{n}$
with $\varphi\in C^{2}\left(\partial\Omega\right)$:
\begin{align*}
\upDelta u & =0 & u\big|_{\partial\Omega} & =\varphi
\end{align*}
The Perron method, named after Oskar Perron (1880-1972), yields the
existence of a solution if $\partial\Omega$ is sufficiently smooth.
This is a classical method, in the sense that it works always with
continuous functions and no weak solutions, but only classical solutions.
The idea is to find the solution as a supremum of subsolutions:
\begin{align*}
u\left(x\right) & :=\sup_{v\text{ subsolution}}\left(v\left(x\right)\right)
\end{align*}
For any domain this gives a harmonic function, but $\partial\Omega$
has to be sufficiently smooth, for the boundary condition to be satisfied.

%DATE: Fr 17.5.13


\section{Definition \textmd{(Sub- and Superharmonic Functions, Compactly Contained,
Subsolution)}}

A function $u\in C^{0}\left(\Omega\right)$ is called \emph{subharmonic}
(\emph{superharmonic}) in $\Omega$, if for every ball $B\Subset\Omega$
(\emph{compactly contained} in $\Omega$, i.e. $\overline{B}\subseteq\Omega$)
and every harmonic function $h\in C^{0}\left(\overline{B}\right)$,
$h\big|_{B}\in C^{2}\left(B\right)$ with $u\le h$ (respectively
$u\ge h$) in $\partial B$ follows $u\le h$ (respectively $u\ge h$)
in all of $B$.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw plot [smooth cycle] coordinates {(-2,0) (-1,2) (1,1) (2,-1)};
  \draw (0,0) node{$h$} circle (0.5);
  \node at (0,0.7) {$B$};
  \node at (-1,0.5) {$u$};
  \node at (0.3,1.9) {$\Omega$};
\end{tikzpicture}\caption{$h\in C^{0}\left(\overline{B}\right)$}
\end{figure}


Note that every harmonic function is subharmonic and superharmonic
by the weak maximum principle.

For $\varphi\in C^{2}\left(\partial\Omega\right)$ we consider the
family $S_{\varphi}$ of functions defined as:
\begin{align*}
S_{\varphi} & :=\left\{ v\in C^{0}\left(\overline{\Omega}\right)\big|v\text{ subharmonic},v\big|_{\partial\Omega}\le\varphi\big|_{\partial\Omega}\right\} 
\end{align*}
Because the constant $v:=\min_{\partial\Omega}\varphi$ is subharmonic
and satisfies $v\big|_{\partial\Omega}\le\varphi\big|_{\partial\Omega}$,
we know $S_{\varphi}\not=\emptyset$. The constant is bigger than
minus infinity, because $\Omega$ is bounded and thus $\partial\Omega$
is compact. A function $v\in S_{\varphi}$ is called \emph{subsolution}.
Now define the function $u$ by:
\begin{align*}
u\left(x\right) & :=\sup_{v\in S_{\varphi}}\left(v\left(x\right)\right)\in\mathbb{R}\cup\left\{ \infty\right\} 
\end{align*}
We hope, that $u$ is the solution of the Dirichlet problem.


\section{Lemma}

If $u_{1}$ and $u_{2}$ are subharmonic then $\text{max}\left(u_{1},u_{2}\right)$
is also subharmonic.


\subsubsection*{Proof}

Let $B\Subset\Omega$ and $h\in C^{2}\left(\Omega\right)$ be harmonic.
\begin{align*}
\max\left(u_{1},u_{2}\right)\big|_{\partial B} & \le h\big|_{\partial B}
\end{align*}
Then follows $u_{1}\big|_{\partial B}\le h\big|_{\partial B}$ and
$u_{2}\big|_{\partial B}\le h\big|_{\partial B}$ in $B$, since $u_{1}$
and $u_{2}$ are subharmonic. Therefore holds also for the maximum
in $B$:
\begin{align*}
\max\left(u_{1},u_{2}\right)\big|_{B} & \le h\big|_{B}
\end{align*}
\qqed


\section{Theorem \textmd{(Supremum of Subsolutions is Harmonic)}\label{sec:Thm-u-harmonic}}

The function
\begin{align*}
u\left(x\right) & :=\sup_{v\in S_{\varphi}}\left(v\left(x\right)\right)\in\mathbb{R}\cup\left\{ \infty\right\} 
\end{align*}
is a harmonic function in $\Omega$.

After proving this theorem, we still need to verify that $u$ satisfies
the boundary condition.


\section{Definition \textmd{(Harmonic Lift)}}

Let $u\in C^{0}\left(\Omega\right)$ be subharmonic in $\Omega$ and
$B\Subset\Omega$ a ball. The function $\overline{v}\in C^{0}\left(\Omega\right)$
defined by
\begin{align*}
\overline{v}\big|_{\Omega\setminus B} & =v\big|_{\Omega\setminus B} & \upDelta\overline{v}\big|_{B} & =0
\end{align*}
is referred to as the \emph{harmonic lift} of $v$ in $B$.


\section{Lemma \textmd{(Harmonic Lift Exists)}}

The harmonic lift exists.


\subsubsection*{Proof}

Consider the Dirichlet problem on the ball $B$:
\begin{align*}
\upDelta u\big|_{B} & =0 & u\big|_{\partial B} & =v\big|_{\partial B}
\end{align*}
This Dirichlet problem has an explicit solution $u$ given by the
Poisson representation. Set:
\begin{align*}
\overline{v}\left(x\right) & =\begin{cases}
v\left(x\right) & \text{if }x\in\Omega\setminus B\\
u\left(x\right) & \text{if }x\in B
\end{cases}
\end{align*}
\qqed

Note that $\overline{v}\ge v$ follows by the definition of subharmonic,
because $u\ge v$ holds in $B$ and thus $\overline{v}\ge v$ holds
in $\Omega$.


\section{Lemma \textmd{(Harmonic Lift is Subharmonic)}}

The harmonic lift is again subharmonic.


\subsubsection*{Proof}

Let $\overline{v}$ be the harmonic lift of $v$ in $B$.

We want to prove, that for all balls $B'\Subset\Omega$ and for all
harmonic functions $u$ with $u\ge\overline{v}$ on $\partial B'$
holds $u\ge\overline{v}$ in $B'$.

If $B'\cap B=\emptyset$ this follows, because $\overline{v}\big|_{\Omega\setminus B}=v$
and $v$ is subharmonic.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}[scale=1.5]
  \draw plot [smooth cycle] coordinates {(-2,0) (-1,2) (2,1) (2,-1)};
  \draw(0,0.5) circle (0.5) (0.8,0.3) circle (1);
\begin{scope}
  \clip (0,0.5) circle (0.5);
  \clip (0.8,0.3) circle (1);
  \draw[thick, orange] (0,0.5) circle (0.485);
  \draw[thick, orange] (0.8,0.3) circle (0.985);
  \foreach \x in {-0.45,-0.25,...,0.35}     \draw[orange] (-1,\x) -- (1,{\x+1});
\end{scope}
  \node[orange] at (0,1.4) {$B\cap B'$};
  \node at (1.15,0.56) {$v=\overline{v}$};
\begin{scope}
  \clip (0.8,0.3) +(1,0) arc (0:360:1) (0,0.5) +(0.5,0) arc (0:-360:0.5);
  \foreach \x in {-1,-0.25,...,0.5}     \draw (2,\x) -- (0.1,{\x+1});
\end{scope}
  \node at (-0.6,0.9) {$B$};
  \node at (1.6,-0.7) {$B'$};
  \node at (0.3,2.2) {$\Omega$};
\end{tikzpicture}\caption{$B'\cap B\not=\emptyset$}
\end{figure}


Otherwise let $u\in C^{2}\left(B'\right)$ be harmonic and $u\ge\overline{v}$
on $\partial B'$. We know $v\le\overline{v}$ in $\Omega$ thus follows:
\begin{align*}
v\big|_{\partial B'} & \le\overline{v}\big|_{\partial B'}\le u\big|_{\partial B'}\\
\sr{\Rightarrow}{\text{subharmonic}}{}v\big|_{B'} & \le u\big|_{B'}
\end{align*}
Therefore we have $\overline{v}=v\le u$ in $B'\setminus B$ and thus
$\overline{v}\le u$ on $\partial\left(B\cap B'\right)$. Since the
functions $\overline{v}$ and $u$ are both harmonic in $B\cap B'$,
the weak maximum principle implies $\overline{v}\le u$ in $B\cap B'$.\qqed


\section{Proposition \textmd{(Maximum Principle for Subharmonic Functions)}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be open and connected and $u\in C^{0}\left(\overline{\Omega}\right)$
subharmonic.
\begin{enumerate}[label=\alph*)]
\item Strong maximum principle: If there is a $x_{0}\in\Omega$ with $u\left(x_{0}\right)=\sup_{\Omega}\left(u\right)$,
then $u$ is constant.
\item Weak maximum principle: ${\displaystyle u\left(x\right)\le\sup_{\partial\Omega}\left(u\right)}$
\end{enumerate}

\subsubsection*{Proof}

b) follows from a) just as in the proof of Theorem \ref{sub:Thm-Weak-Maximum-Principle}.

To prove a), suppose that $u\left(x_{0}\right)=\sup_{\Omega}\left(u\right)$
holds, but $u$ is \emph{not} constant. Then there is a $y\in\Omega$
and a $r\in\mathbb{R}_{>0}$ such that $u\left(y\right)=\sup_{x\in\Omega}u\left(x\right)$,
but $u\big|_{\partial B_{r}\left(y\right)}$ is not constant.

Let $c\left(\tau\right)$ be a curve joining $c\left(0\right)=x_{0}$
with $c\left(1\right)=z$.
\begin{align*}
\tilde{\tau} & :=\sup\left\{ \tau\big|u\left(c\left(\tau\right)\right)=u\left(x_{0}\right)\right\} <1
\end{align*}
Choose $y:=c\left(\tilde{\tau}\right)$ and $r$ small enough, so
that $B_{r}\left(y\right)\subseteq\Omega$.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}[scale=1.5]
  \draw plot [smooth cycle] coordinates {(-2,0) (-1,2) (2,1) (2,-1)};
  \node at (0.3,2) {$\Omega$};
  \draw (-1,1) node[left]{$x_0$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (1.5,0) node[right]{$z$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw plot [smooth] coordinates {(-1,1) (-0.5,0) (0.5,0.5) (1.5,0)};
  \draw[red] (0.5,0.5) node[above]{$y$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1) +(0,0) circle (0.5) node[above right=10pt]{$B_r(y)$};
\end{tikzpicture}\caption{$u\left(z\right)<u\left(x_{0}\right)=u\left(y\right)$; $u\big|_{\partial B_{r}\left(y\right)}$
is not constant.}
\end{figure}


Let $\overline{u}$ be the harmonic lift of $u$ in $B_{r}$. Then
holds:
\begin{align*}
\overline{u}\left(y\right) & \ge u\left(y\right)\ge\sup_{\partial B_{r}\left(y\right)}\left(u\right)=\sup_{\partial B_{r}\left(y\right)}\left(\overline{u}\right)
\end{align*}
By the weak maximum principle, which states $\overline{u}\left(y\right)\le\sup_{\partial B_{r}\left(y\right)}\left(\overline{u}\right)$,
follows that $\overline{u}$ is constant in $B_{r}\left(y\right)$.
This implies that $\overline{u}\big|_{\partial B_{r}}=u\big|_{\partial B_{r}}$
is constant, which is a contradiction to $u$ not being constant on
$\partial B_{r}$.\qqed


\section{Proposition\label{sec:Prop-sub-super-equals}}

If $v$ is subharmonic and $u$ superharmonic in $\Omega$ and $v\le u$
holds on $\partial\Omega$, then in $\Omega$ holds either $v<u$
or $v=u$, in which case $u$ and $v$ are harmonic.


\subsubsection*{Proof}

This is proven in the exercises.\qqed


\subsubsection*{Proof of Theorem \ref{sec:Thm-u-harmonic}}

Define $u\left(x\right):=\sup_{v\in S_{\varphi}}\left(v\left(x\right)\right)$.
According to Proposition \ref{sec:Prop-sub-super-equals} we know,
since the constant function $\sup_{\partial\Omega}\left(\varphi\right)$
is superharmonic:
\begin{align*}
v\left(x\right) & \le\sup_{\partial\Omega}\left(\varphi\right)\qquad\fall_{v\in S_{\varphi}}\\
\Rightarrow\qquad u\left(x\right) & \le\sup_{\partial\Omega}\left(\varphi\right)
\end{align*}
From $\inf_{\partial\Omega}\left(\varphi\right)\in S_{\varphi}$ and
because $u$ is the supremum of the functions in $S_{\varphi}$, we
also know:
\begin{align*}
\inf_{\partial\Omega}\left(\varphi\right) & \le u\left(x\right)
\end{align*}
Hence $u$ is a bounded function on $\Omega$. Consider $y\in\Omega$.
By definition of the supremum, there is a sequence $v_{n}\in S_{\varphi}$
such that $v_{n}\left(y\right)\to u\left(y\right)$ converges. We
can assume that the functions $v_{n}$ are bounded, because $v_{n}\le u$
bounds from above and if $v_{n}$ is not bounded from below, we can
replace it by $\max\left(v_{n},\inf_{\partial\Omega}\left(\varphi\right)\right)$.
This is again a subsolution.

Choose $r\in\mathbb{R}_{>0}$ such that $B_{r}\left(y\right)\Subset\Omega$
and let $\overline{v}_{n}$ be the harmonic lift of $v_{n}$ in $B_{r}\left(y\right)$.
Then $v_{n}\le\overline{v}_{n}$ holds in $\Omega$ and $\overline{v}_{n}\left(y\right)\le u\left(y\right)$,
since $\overline{v}_{n}$ is again a subsolution. Hence follows:
\begin{align*}
\underbrace{v_{n}\left(y\right)}_{\to u\left(y\right)} & \le\overline{v}_{n}\left(y\right)\le u\left(y\right)
\end{align*}
Thus we have $\overline{v}_{n}\left(y\right)\to u\left(y\right)$.
Possibly after choosing a subsequence, the sequence $\overline{v}_{n}\left(y\right)$
is monotonically increasing. Moreover, we can assume that the sequence
$v_{n}$ is monotonically increasing in $\Omega$. Namely, we can
replace $v_{n}$ by $\tilde{v}_{n}$ defined by:
\begin{align*}
\tilde{v}_{1} & =v_{1}\\
\tilde{v}_{2} & :=\max\left(v_{1},v_{2}\right)\\
\vdots\ \\
\tilde{v}_{n} & :=\max\left(v_{1},\ldots,v_{n}\right)
\end{align*}
These are again subharmonic and they are monotonically increasing.
Then the $\overline{v}_{n}$ are also monotonically increasing in
$B_{r}\left(y\right)$ using the maximum principle:
\begin{align*}
\left(\overline{v}_{n+1}-\overline{v}_{n}\right)\big|_{\partial B_{r}\left(y\right)} & =\left(v_{n+1}-v_{n}\right)\big|_{\partial B_{r}\left(y\right)}\ge0
\end{align*}
The weak maximum principle implies $\overline{v}_{n+1}-\overline{v}_{n}\ge0$
in $B_{r}\left(y\right)$.

Applying Harnack's convergence theorem, we conclude
\begin{align*}
\overline{v}_{n} & \rightrightarrows\tilde{v}\text{ in }B_{r}\left(y\right)
\end{align*}
and $\overline{v}$ is harmonic.

Now we show $u=\tilde{v}$ in $B_{r}\left(y\right)$. Obviously holds
$\tilde{v}\le u,$ because $\overline{v}_{n}\le u$ holds for all
$n\in\mathbb{N}$.\\
Thus assume that there is a point $x\in B_{r}\left(y\right)$ with
$\tilde{v}\left(x\right)<u\left(x\right)$. Then there exists a subsolution
$s\in S_{\varphi}$ with $\tilde{v}\left(x\right)<s\left(x\right)<u\left(x\right)$,
because $u\left(x\right)=\sup_{v\in S_{\varphi}}\left(v\left(x\right)\right)$.
Thus we also have:
\begin{align*}
v_{n}\left(x\right) & \le\tilde{v}\left(x\right)<s\left(x\right)
\end{align*}
Now form the sequence:
\begin{align*}
w_{n} & :=\max\left(s,v_{n}\right)
\end{align*}
These are again subharmonic, from $v_{n}\left(x\right)<s\left(x\right)$
follows $w_{n}\left(x\right)=s\left(x\right)$, and for the harmonic
lifts holds $\overline{w}_{n}\ge\overline{v}_{n}$. Then repeating
the above construction for $v_{n}$ replaced by $w_{n}$, we find
$\overline{w}_{n}\rightrightarrows\tilde{w}$ converges uniformly
to a harmonic $\tilde{w}$. Then follows $\tilde{v}\le\tilde{w}\le u$
just as before and thus $\tilde{v}-\tilde{w}\le0$ in $B$. Additionally
$\tilde{v}$ and $\tilde{w}$ are harmonic and we know:
\begin{align*}
\tilde{v}\left(y\right) & =\tilde{w}\left(y\right)=u\left(y\right)
\end{align*}
So $\tilde{v}-\tilde{w}$ attains its maximum in $y\in B$ and the
strong maximum principle gives $\tilde{v}=\tilde{w}$ in $B$. From
$w_{n}\left(x\right)>s\left(x\right)$ follows $\tilde{w}\left(x\right)\ge s\left(x\right)>\tilde{v}\left(x\right)$.
This is a contradiction.\qqed[\ref{sec:Thm-u-harmonic}]

Question: Is $u$ also a solution of the Dirichlet problem?


\section{Proposition \textmd{(Existing Solution is Perron Solution)}}

If the Dirichlet problem has a solution, then
\begin{align*}
u\left(x\right) & =\sup_{v\in S_{\varphi}}\left(v\left(x\right)\right)
\end{align*}
equals this solution.


\subsubsection*{Proof}

Let $\tilde{u}$ be a solution of the Dirichlet problem. Then $\tilde{u}$
is also a subsolution, i.e. $\tilde{u}\in S_{\varphi}$. Therefore
holds $u\left(x\right)\ge\tilde{u}\left(x\right)$ for all $x\in\Omega$.
Hence $u-\tilde{u}$ is harmonic and $\left(u-\tilde{u}\right)\big|_{\partial\Omega}=0$.
By the weak maximum principle follows $u=\tilde{u}$ in $\Omega$.\qqed

%DATE: Mi 22.5.13


\section{Definition \textmd{(Barrier, Local Barrier)}}

A function $w\in C^{0}\left(\overline{\Omega}\right)$ is called \emph{barrier}
at $\xi\in\partial\Omega$, if holds:
\begin{enumerate}[label=\roman*)]
\item $w$ is superharmonic.
\item In $\overline{\Omega}\setminus\left\{ \xi\right\} $ holds $w>0$
and $w\left(\xi\right)=0$.
\end{enumerate}
We show first that the barrier is a local concept.

A \emph{local barrier} in $\xi\in\partial\Omega$ is a function $w\in C^{0}\left(\overline{\Omega}\cap B_{\varepsilon}\left(\xi\right)\right)$
for $\varepsilon\in\mathbb{R}_{>0}$ with:
\begin{enumerate}[label=\roman*)]
\item $w$ is superharmonic in $\Omega\cap B_{\varepsilon}\left(\xi\right)$.
\item In $\overline{\Omega\cap B_{\varepsilon}\left(\xi\right)}\setminus\left\{ \xi\right\} $
holds $w>0$ and $w\left(\xi\right)=0$.
\end{enumerate}

\section{Theorem \textmd{(Barrier is Local Concept)}}

If there is a local barrier, there is also a barrier.


\subsubsection*{Proof}

Let $w\in C^{0}\left(\overline{\Omega\cap B_{\varepsilon}\left(\xi\right)}\right)$
be a local barrier. With
\begin{align*}
m & :=\inf_{B_{\varepsilon}\left(\xi\right)\setminus B_{\frac{\varepsilon}{2}}\left(\xi\right)}\left(w\right)>0
\end{align*}
we choose:
\begin{align*}
\overline{w}\left(x\right) & :=\begin{cases}
\min\left(w\left(x\right),m\right) & x\in\overline{\Omega\cap B_{\frac{\varepsilon}{2}}\left(\xi\right)}\\
m & x\not\in\overline{\Omega\cap B_{\frac{\varepsilon}{2}}\left(\xi\right)}
\end{cases}
\end{align*}
This is a barrier.\qqed


\section{Definition \textmd{(regular point)}}

A border-point $\xi\in\partial\Omega$ is called \emph{regular} if
there is a barrier at $\xi$.


\section{Theorem \textmd{(Solution of Dirichlet Problem with Regular Border)\label{sec:Thm-Perron-Solution}}}

If $\xi$ is a regular border-point, then holds for the function $u\left(x\right):=\sup_{v\in S_{\varphi}}\left(v\left(x\right)\right)$:
\begin{align*}
\lim_{\Omega\ni x\to\xi}u\left(x\right) & =\varphi\left(\xi\right)
\end{align*}
So if every border point is regular, the Perron solution is the solution
of the Dirichlet problem.


\subsubsection*{Proof}

We construct a supersolution. Because $\xi$ is a regular border-point,
there is a barrier $w$ at $\xi$. Consider $\varepsilon\in\mathbb{R}_{>0}$
and define:
\begin{align*}
M & :=\sup_{\partial\Omega}\left(\abs{\varphi}\right)
\end{align*}
Now choose $\delta,k\in\mathbb{R}_{>0}$ such that holds:
\begin{align*}
\abs{\varphi\left(x\right)-\varphi\left(\xi\right)} & <\varepsilon &  & \fall_{x\in B_{\delta}\left(\xi\right)\cap\partial\Omega}\\
kw\left(x\right) & \ge2M &  & \fall_{x\in\partial\Omega\setminus B_{\delta}\left(\xi\right)}
\end{align*}
Then
\begin{align*}
u_{+}\left(x\right) & :=\varphi\left(\xi\right)+\varepsilon+kw\left(x\right)
\end{align*}
is a supersolution and
\begin{align*}
u_{-}\left(x\right) & :=\varphi\left(\xi\right)-\varepsilon-kw\left(x\right)
\end{align*}
a subsolution. (These are sub-/superharmonic, because $w\left(x\right)$
is superharmonic and thus $-w\left(x\right)$ subharmonic and constants
are harmonic.)

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}[scale=2]
  \draw[blue] plot[smooth,tension=1] coordinates {(0.8,0.8) (0.7,1.2) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.6) (0.6,0)};
  \node[blue] at (0.2,1){$\partial\Omega \setminus B_\delta(\xi)$};
  \draw[red,thick] (0.8,0.8) .. controls +(-0.1,-0.2) and +(0.1,0.2) .. (0.6,0) node[midway](a){};
  \node[red] at (1.1,-0.15){$B_\delta(\xi)\cap\partial\Omega$};
  \draw (a) node[right]{$\xi$} +(0.05,0.05) -- +(-0.05,-0.05) +(0.05,-0.05) -- +(-0.05,0.05);
  \draw (a) circle (0.41) node[above right=13pt]{$B_\delta (\xi)$};
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{Construction of $u_{+}$ and $u_{-}$}
\end{figure}


The $\varepsilon$ ensures $u_{-}\left(x\right)\le\varphi\left(x\right)\le u_{+}\left(x\right)$
for $x\in B_{\delta}\left(\xi\right)\cap\partial\Omega$ and for $x\in\partial\Omega\setminus B_{\delta}\left(\xi\right)$
this follows from $kw\ge2M$. Now the maximum principle implies:
\begin{align*}
\varphi\left(\xi\right)-\varepsilon-kw\left(x\right) & \le u\left(x\right)\le\varphi\left(\xi\right)+\varepsilon+kw\left(x\right)
\end{align*}
Due to $\lim_{x\to\xi}\left(w\left(x\right)\right)=0$ and because
$\varepsilon$ can be chosen arbitrarily, follows:
\begin{align*}
\lim_{x\to\xi}u\left(x\right) & =\varphi\left(\xi\right)
\end{align*}
\qqed


\section{Theorem \textmd{(Existence of Solution for Dirichlet Problem)}}

The following statements are equivalent:
\begin{enumerate}
\item The Dirichlet problem has for every $\varphi\in C^{0}\left(\partial\Omega\right)$
a solution in $C^{2}\left(\Omega\right)\cap C^{0}\left(\overline{\Omega}\right)$.
\item Every border-point of $\Omega$ is regular.
\end{enumerate}

\subsubsection*{Proof}

``2. $\Rightarrow$ 1.'': If every border-point is regular, then
the Perron solution $u\left(x\right)$ is the solution of the Dirichlet
problem after Theorem \ref{sec:Thm-Perron-Solution}.

``1. $\Rightarrow$ 2.'': For $\xi\in\partial\Omega$ let $u$ be
the solution of the following Dirichlet problem:
\begin{align*}
\upDelta u\big|_{\Omega} & =0\\
u\left(x\right)\big|_{\partial\Omega} & =\norm{x-\xi}\big|_{\partial\Omega}
\end{align*}


\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}[scale=2]
  \draw plot[smooth cycle,tension=1] coordinates {(0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.6) (0.6,0)};
  \node at (0,1.6) {$\partial\Omega$};
  \draw (0.8,0.8) node[right]{$\xi$} +(0.05,0.05) -- +(-0.05,-0.05) +(0.05,-0.05) -- +(-0.05,0.05);
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$u\left(\xi\right)=0$, $u\big|_{\partial\Omega\setminus\left\{ \xi\right\} }>0$}
\end{figure}


Were $u\left(y\right)=0$ for a $y\in\Omega$, we had an interior
minimum, and thus the maximum principle would give the contradiction
$u=0$. So this $u$ is a barrier.\qqed

How can one construct barriers? Under what geometrical conditions
do barriers exist?

Consider the dimension $n=2$. Without loss of generality we assume
$\xi=0$. We consider this as a problem in $\mathbb{C}\stackrel{\sim}{=}\mathbb{R}^{2}$
and define:
\begin{align*}
w\left(z\right) & =-\text{Re}\left(\frac{1}{\ln\left(z\right)}\right)
\end{align*}


\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}[scale=2]
  \draw plot[smooth,tension=1] coordinates {(0.4,0.4) (0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.6)  (0.8,0.25) (0.4,0.4)};
  \node at (0,1.6) {$\Omega$};
  \draw (0.4,0.4) node[below]{$\xi :=0$} +(0.05,0.05) -- +(-0.05,-0.05) +(0.05,-0.05) -- +(-0.05,0.05);
  \draw[red] (0.4,0.4) -- node[below right]{Strahl $S$} (1.4,0.63);
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$\Omega$ under consideration}
\end{figure}


Choose the branch cut of the logarithm on $S$. Such an $S$ with
$\Omega\cap S=\emptyset$ and $\xi\in S$ can always be chosen locally.
Thus $\ln\left(z\right)$ is holomorphic in $\mathbb{C}\setminus S$
and therefore $\left(\ln\left(z\right)\right)^{-1}$ is holomorphic
on $\mathbb{C}\setminus S$, which means that $w$ is harmonic on
$\mathbb{C}\setminus S$ and especially continuous on $\mathbb{C}\setminus S$.

To study the behaviour at the origin, we choose the polar representation:
\begin{align*}
z & =re^{\ii\varphi}\\
\ln\left(z\right) & =\ln\left(r\right)+\ii\varphi+2\pi\ii n
\end{align*}
Here $n\in\mathbb{Z}$ depends on the branch of the logarithm.
\begin{align*}
-\frac{1}{\ln\left(z\right)} & =-\frac{1}{\ln\left(r\right)+\ii\varphi+2\pi\ii n}\\
-\text{Re}\left(\frac{1}{\ln\left(z\right)}\right) & =-\frac{\ln\left(r\right)}{\left(\ln\left(r\right)\right)^{2}+\left(\varphi+2\pi n\right)^{2}}\xrightarrow{r\to0}0
\end{align*}
This converges locally uniformly in $\varphi$.

Alternatively, this follows from the Riemann mapping theorem.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \draw plot[smooth cycle,tension=1] coordinates {(0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.6)};
  \node at (0,1.7) {$\Omega$};
  \draw[decoration={markings,mark=at position 1 with {\arrow[scale=2]{>}};},postaction={decorate}] (1,0.5) -- (2,0.5);
  \draw (3,0.5) circle (0.8);
  \node at (3,1.6) {$B_1(0)\subseteq \mathbb{C}$};
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{Mapping from $\Omega$ to $B_{1}\left(0\right)$}
\end{figure}


Just transform from any simply connected $\Omega$ to $B_{1}\left(0\right)\subseteq\mathbb{C}$,
solve the Dirichlet problem in the ball and transform back.

Consider now the case $n\ge3$: One gets barriers if additional conditions
are satisfied.


\section{Definition \textmd{(Exterior Sphere/Cone Condition)}}

An open subset $\Omega\subseteq\mathbb{R}^{n}$ satisfies in $\xi\in\partial\Omega$
the \emph{exterior sphere condition} if there is a closed ball $K\subseteq\mathbb{R}^{n}$
with radius $R$ such that $K\cap\overline{\Omega}=\left\{ \xi\right\} $.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
    \node at (0,3) {satisfied};
    \draw[yshift=1cm] plot[smooth cycle,tension=1] coordinates {(0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.3)  (0.6,0.1)};
    \draw[yshift=-1cm] plot[smooth,tension=1] coordinates {(0.8,0.9) (0,1) (-0.5,0.7) (-0.3,0) ( 0,-0.6)  (0.4,0.6) (0.8,0.9)};
  \begin{scope}[xshift=3cm]
    \node at (0,2) {not satisfied};
    \draw plot[smooth,tension=1] coordinates {(0.4,0.4) (0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.6)  (0.8,0.25) (0.4,0.4)};
  \end{scope}
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{Exterior Sphere Condition}
\end{figure}


Now choose:
\begin{align*}
w\left(x\right) & =-\Gamma\left(\norm{x-y}\right)+\Gamma\left(\norm{\xi-y}\right)
\end{align*}
This function is harmonic and non-negative. Furthermore $w\left(x\right)=0$
implies $x=\xi$. So $w$ is a barrier and we can solve the Dirichlet
problem.

An improvement is the so called \emph{exterior cone condition}.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
    \node at (0,3) {satisfied};
    \draw[yshift=1cm] plot[smooth cycle,tension=1] coordinates {(0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.3)  (0.6,0.1)};
  \draw (0.8,1.8) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw[blue] (0.8,1.8) -- +(0.5,1) +(0,0) -- +(1,0.2);
  \node[blue] at (1.6,2.3) {cone};
    \draw[yshift=-1cm] plot[smooth,tension=1] coordinates {(0,0) (0,0.7) (-0.6,0.8) (-1,0) (0,-1) (1,0) (0,0)};
  \draw[blue] (0,-1) -- +(0.5,1) +(0,0) -- +(1,0.3);
  \begin{scope}[xshift=4cm]
    \node at (0,2) {not satisfied};
    \draw plot[smooth,tension=1] coordinates {(0.4,0.4) (0.8,0.8) (0,1.5) (-0.5,1) (-1,0.7) (-0.7,0) ( 0,-0.6)  (0.8,0.25) (0.4,0.4)};
  \end{scope}
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{Exterior Cone Condition}
\end{figure}


Here one can also construct a barrier.


\section{Example \textmd{(Dirichlet Problem without Solution, Lebesgue Spine)}}

A counter example for the solvability of the Dirichlet problem with
continuous boundary values was found in 1912 by Lebesgue. The details
can be found in the book of Courant and Hilbert.

We use the Ansatz:
\begin{align*}
u\left(x\right) & =\int\Gamma\left(\norm{x-y}\right)\varrho\left(y\right)\dd^{n}y
\end{align*}
Here $\varrho\left(y\right)$ can be viewed as ``charge density''.
Then follows:
\begin{align*}
\upDelta u\left(x\right) & =\varrho\left(x\right)\\
\upDelta u\left(x\right) & =0\qquad\text{if }\varrho\left(x\right)=0
\end{align*}
Consider the charge distribution on the $x$-axis:
\begin{align*}
\varrho\left(x,y,z\right) & =\begin{cases}
\delta\left(y\right)\delta\left(z\right)\left(-4\pi x\right) & x\in\left[0,1\right]\\
0 & \text{otherwise}
\end{cases}
\end{align*}
This is the so-called Lebesgue spine in $\mathbb{R}^{3}$.
\begin{align*}
u\left(x,y,z\right) & =\int_{0}^{1}\Gamma\left(\norm{\left(\xi,0,0\right)-\left(x,y,z\right)}\right)\left(-4\pi\xi\right)\dd\xi=\\
 & =\int_{0}^{1}\frac{\xi\dd\xi}{\sqrt{\left(\xi-x\right)^{2}+y^{2}+z^{2}}}\dd\xi
\end{align*}
Define $\zeta:=\sqrt{y^{2}+z^{2}}$. The integral gives:
\begin{align*}
u\left(x,\zeta\right) & =A\left(x,\zeta\right)-2x\ln\left(\zeta\right)\\
A\left(x,\zeta\right) & =\sqrt{\left(1-x\right)^{2}+\zeta^{2}}-\sqrt{x^{2}+\zeta^{2}}+x\ln\left(\abs{\left(1+x+\sqrt{\left(1-x\right)^{2}+\zeta^{2}}\right)\left(x+\sqrt{x^{2}+\zeta^{2}}\right)}\right)
\end{align*}
$A$ is continuous at the origin:
\begin{align*}
1+x+\sqrt{\left(1-x\right)^{2}+\zeta^{2}} & \to2\\
\lim_{\left(x,\zeta\right)\to0}\left(x+\sqrt{x^{2}+\zeta^{2}}\right) & =\lim_{x\to0}\left(2x\right)\\
\lim_{x\to0}x\ln\left(x\right) & =0\\
\Rightarrow\qquad\lim_{\left(x,\zeta\right)\to0}A\left(x,\zeta\right) & =1
\end{align*}
But $x\ln\left(\zeta\right)$ is not continuous at the origin. Consider
for example:
\begin{align*}
\zeta & =\abs x^{\alpha} & x\ln\left(\zeta\right) & =x\alpha\ln\left(\abs x\right)\xrightarrow{x\to0}0\\
\zeta & =e^{-\frac{c}{x}} & x\ln\left(\zeta\right) & =x\frac{-c}{x}\xrightarrow{x\to0}-c
\end{align*}


\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
 \begin{axis}[width=10cm, axis x line=middle, axis y line=middle, xtick={0}, ytick={0}, xlabel=$q$,domain=0:.3,samples=300]
  \addplot[mark=none] {x^2};
  \addlegendentry{$\zeta=\abs{x}^2$}
  \addplot[blue,mark=none] {exp(-1/x)};
  \addlegendentry{$\zeta=\exp (-\frac{1}{x})$}
 \end{axis}
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$\lim_{\left(x,\zeta\right)\to0}A$$\left(x,\zeta\right)$ has different
values for different path.}
\end{figure}


%DATE: Fr 24.5.13

So the function $-2x\ln\left(\zeta\right)$ is constant along the
curve $\left\{ \zeta=e^{-\frac{c}{2x}}\Big|x>0\right\} $, but is
not continuous at the origin. Consider the level set $u\left(x,\zeta\right)=1+C$
for $C\in\mathbb{R}_{>0}$.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \foreach \x in {-3,-2,...,2}     \draw (-1,\x) -- (4,{\x + 1.5});
  \draw[scale=1.5,fill=white] plot[smooth, tension=0.7] coordinates {(0,0) (0.5,0.18) (1,1) (1.5,1) (2,0) (1.5,-1) (1,-1) (0.5,-0.18) (0,0)};
  \draw[->] (0,0) -- (0,3) node[above]{$\zeta$};
  \draw[->] (0,0) -- (4,0) node[right]{$x$};
  \node at (1,3) {$\Omega $};
  \draw[red] (1.5,0) circle (2);
  \draw[red] (1.5,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw[red] (1.5,0) -- node[above left=-2pt]{$R$} (2.9,1.4);
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{The level set $u\left(x,\zeta\right)=1+C$ defines the region $\Omega$.}
\end{figure}


We have:
\begin{align*}
\upDelta u\big|_{\Omega} & =0\\
u\big|_{\partial\Omega} & =1+C
\end{align*}
But $u$ is \emph{not} continuous at $\left(x,y,z\right)=0$. Let
$B$ be a ball such that $\partial B\cap\partial\Omega=\emptyset$.
Now perform an inversion:

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \node at (1,2.25) {$\Omega $};
  \draw[red] (-0.75,0) -- (-4,0);
  \draw[scale=1.5] plot[smooth, tension=0.7] coordinates {(-0.5,0) (-1,0.3) (0,1.7) (1,2) (3,1) (3.5,0) (3,-1) (1,-2) (0,-1.7) (-1,-0.3) (-0.5,0)};
  \draw (1.5,0) circle (2);
  \draw (1.5,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (1.5,0) -- node[above left=-2pt]{$R$} (2.9,1.4);
\begin{scope}
  \clip[scale=1.5] plot[smooth, tension=0.7] coordinates {(-0.5,0) (-1,0.3) (0,1.7) (1,2) (3,1) (3.5,0) (3,-1) (1,-2) (0,-1.7) (-1,-0.3) (-0.5,0)};
  \foreach \x in {-3,-2,...,2}     \draw (-2,\x) -- (6,{\x + 1.5});
\end{scope}
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$\Omega$ inverted}
\end{figure}


For formulas see the book of Courant and Hilbert.
\begin{align*}
\upDelta u\big|_{\Omega} & =0\\
u\big|_{\partial\Omega} & =1+C
\end{align*}
$u$ is not continous in $\overline{\Omega}$.


\chapter{The Maximum Principle for Linear Elliptic Equations}

Again $\Omega\subseteq\mathbb{R}^{n}$ is an open domain and $u\in C^{2}\left(\Omega,\mathbb{R}\right)$
a scalar function. Let $L$ be a differential operator of second order,
linear and elliptic.
\begin{align*}
Lu\left(x\right) & =\sum_{i,j=1}^{n}a^{ij}\left(x\right)\partial_{ij}u\left(x\right)+\sum_{i=1}^{n}b^{i}\left(x\right)\partial_{i}u\left(x\right)+c\left(x\right)u\left(x\right)
\end{align*}
Usually we use the summation convention to omit the sums. We make
the following assumpions:
\begin{enumerate}[label=\roman*)]
\item \emph{Uniform ellipticity}: There exists a $\lambda\in\mathbb{R}_{>0}$
such that for all $\xi\in\mathbb{R}^{n}$ and all $x\in\Omega$ holds:
\begin{align*}
a^{ij}\left(x\right)\xi_{i}\xi_{j} & \ge\lambda\cdot\abs{\xi}^{2}
\end{align*}

\item The coefficients are \emph{uniformly bounded}: There exists a $K\in\mathbb{R}_{>0}$
such that for all $x\in\Omega$ and $i,j\in\left\{ 1,\ldots,n\right\} $
holds:
\begin{align*}
\abs{a^{ij}\left(x\right)},\abs{b^{i}\left(x\right)},\abs{c\left(x\right)} & \le K
\end{align*}

\end{enumerate}
Consider solutions of the homogeneous equation $Lu=0$. Our goal is
to derive a maximum principle.


\section{Example \textmd{(One-Dimensional)}}

We begin with a one-dimensional example.
\begin{align*}
u''\left(x\right)+cu\left(x\right) & =0
\end{align*}

\begin{enumerate}[label=\alph*)]
\item $c<0$: There are two fundamental solutions, i.e. every solution
is a linear combination:
\begin{align*}
u_{1}\left(x\right) & =e^{\sqrt{\abs c}x}\\
u_{2}\left(x\right) & =e^{-\sqrt{\abs c}x}
\end{align*}
Consider $\Omega=\left(0,1\right)$ and impose Dirichlet boundary
conditions $u\left(0\right)=0=u\left(1\right)$.
\begin{align*}
u & =\alpha u_{1}+\beta u_{2}
\end{align*}
\begin{align*}
u\left(0\right) & =0 & \Rightarrow\qquad\alpha & =-\beta & \Rightarrow\qquad u & =\alpha\left(e^{\sqrt{\abs c}x}-e^{-\sqrt{\abs c}x}\right)\\
u\left(1\right) & =0 & \Rightarrow\qquad\alpha\left(e^{\sqrt{\abs c}}-e^{-\sqrt{\abs c}}\right) & =0 & \Rightarrow\qquad\alpha & =0
\end{align*}
Thus the Dirichlet problem has a unique solution $u=0$, in accordance
with the maximum principle.
\item $c>0$: The functions 
\begin{align*}
u_{1}\left(x\right) & =\sin\left(\sqrt{c}x\right)\\
u_{2}\left(x\right) & =\cos\left(\sqrt{c}x\right)
\end{align*}
are fundamental solutions.
\begin{align*}
u & =\alpha u_{1}+\beta u_{2}\\
u\left(0\right) & =0\qquad\Rightarrow\qquad\beta=0\\
u\left(1\right) & =0=\alpha\sin\left(\sqrt{c}\right)
\end{align*}
In the case of $\sin\left(\sqrt{c}\right)=0$, the parameter $\alpha$
is arbitrary, otherwise follows $\alpha=0$ and thus $u=0$.\\
For example for $c=\pi^{2}$ with $\alpha=1$ we have:
\begin{align*}
u\left(x\right) & =\sin\left(\pi x\right)
\end{align*}
This is a non-trivial solution of the Dirichlet problem and it has
an interior maximum. So the maximum principle is violated.\\
\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
 \begin{axis}[width=12cm, axis x line=middle, axis y line=middle, xlabel=$q$,domain=0:1,samples=300, ymax=1.08, xmax=1.08]
  \addplot[mark=none] {sin(x*180)};
  \addlegendentry{$u(x)=\sin (\pi x)$}
  \draw[dashed] (axis cs:0.5,0) -- (axis cs:0.5,1.1);
 \end{axis}
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$u\left(x\right)$ has an interior maximum.}
\end{figure}

\end{enumerate}

\section{Theorem \textmd{(Weak Maximum Principle)\label{sec:Thm-Weak-Maximum_elliptic}}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be bounded and $c\left(x\right)=0$.
Assume $Lu\ge0$ (or $Lu\le0$) in all of $\Omega$ and $u\in C^{2}\left(\Omega\right)\cap C^{0}\left(\overline{\Omega}\right)$.
Then follows:
\begin{align*}
\sup_{\Omega}\left(u\right) & =\sup_{\partial\Omega}\left(u\right) &  & \left(\inf_{\Omega}\left(u\right)=\inf_{\partial\Omega}\left(u\right)\right)
\end{align*}



\subsubsection*{Proof}

Only consider the case $Lu\ge0$, since for $Lu\le0$ holds $L\left(-u\right)\ge0$.
\begin{enumerate}[label=\alph*)]
\item Assume $Lu>0$ and $\sup_{\Omega}\left(u\right)>\sup_{\partial\Omega}\left(u\right)$.
Then the function $u$ has a maximum at $x_{0}\in\Omega$. (Here we
use that $\Omega$ is bounded and thus $\overline{\Omega}$ is compact.)
This implies
\begin{align*}
\partial_{i}u\left(x_{0}\right) & =0
\end{align*}
and that $\partial_{ij}u\left(x_{0}\right)$ is negative semi-definite.
\begin{align*}
Lu\left(x_{0}\right) & =\underbrace{a^{ij}\left(x_{0}\right)}_{\text{pos. def.}}\underbrace{\partial_{ij}u\left(x_{0}\right)}_{\text{neg. sem.-def.}}\le0
\end{align*}
This is a contradiction to $Lu>0$.
\item Assume only $Lu\ge0$. The idea is to modify $u$ such that a) applies.
\begin{align*}
Le^{\gamma x_{1}} & =a^{11}\partial_{x_{1}}\partial_{x_{1}}e^{\gamma x_{1}}+b^{1}\partial_{x_{1}}e^{\gamma x_{1}}=\left(\gamma^{2}a^{11}+\gamma b^{1}\right)e^{\gamma x_{1}}
\end{align*}
We know $a^{11}\left(x\right)>\lambda$ and $\abs{b^{1}}<K$. For
$\gamma>\frac{K}{\lambda}$ we have:
\begin{align*}
Le^{\gamma x_{1}} & >0
\end{align*}
Now consider for $\varepsilon\in\mathbb{R}_{>0}$:
\begin{align*}
L\left(u+\varepsilon e^{\gamma x_{1}}\right) & =Lu+\varepsilon Le^{\gamma x_{1}}>0
\end{align*}
Thus one can apply a) to conclude:
\begin{align*}
\sup_{\Omega}\left(u+\varepsilon e^{\gamma x_{1}}\right) & =\sup_{\partial\Omega}\left(u+\varepsilon e^{\gamma x_{1}}\right)
\end{align*}
Since this holds for all $\varepsilon\in\mathbb{R}_{>0}$, we get
also:
\begin{align*}
\sup_{\Omega}\left(u\right) & =\sup_{\partial\Omega}\left(u\right)
\end{align*}

\end{enumerate}
\qqed


\section{Corollary\label{sec:Cor-Weak-Maximum-c-non-positive}}

For $c\left(x\right)\le0$ and $Lu\ge0$ (or $Lu\le0$) holds with
$u^{+}:=\max\left(u,0\right)$ (and $u^{-}:=\min\left(u,0\right)$):
\begin{align*}
\sup_{\Omega}\left(u\right) & \le\sup_{\partial\Omega}\left(u^{+}\right) & \big(\inf_{\Omega}\left(u\right) & \le\inf_{\partial\Omega}\left(u\right)\big)
\end{align*}



\subsubsection*{Proof}

We show this only for $Lu\ge0$, since for $Lu\le0$ one can consider
$-u$. $L$ without the zero order term is:
\begin{align*}
L_{0} & :=a^{ij}\left(x\right)\partial_{ij}+b^{i}\left(x\right)\partial_{i}
\end{align*}
Now consider the domain:
\begin{align*}
\Omega^{+} & :=\left\{ x\in\Omega\big|u\left(x\right)>0\right\} 
\end{align*}
$\Omega^{+}$ is again an open, bounded domain and in $\Omega^{+}$
holds:
\begin{align*}
L_{0}u & \ge L_{0}u+\underbrace{cu}_{\le0}=Lu\ge0
\end{align*}
Then \ref{sec:Thm-Weak-Maximum_elliptic} yields:
\begin{align*}
\sup_{\Omega^{+}}\left(u\right) & =\sup_{\partial\Omega^{+}}\left(u\right)\le\sup_{\partial\Omega}\left(u^{+}\right)
\end{align*}
If in $\Omega$ holds $u\le0$, there is nothing to prove. Otherwise
holds:
\begin{align*}
\sup_{\Omega}\left(u\right) & =\sup_{\Omega^{+}}\left(u\right)=\sup_{\partial\Omega^{+}}\left(u\right)\le\sup_{\partial\Omega}\left(u^{+}\right)
\end{align*}
\qqed


\section{Theorem}

Let $c\in\mathbb{R}_{\le0}$ and $u,v\in C^{2}\left(\Omega\right)\cap C^{0}\left(\overline{\Omega}\right)$
satisfy the inequalities $Lu\ge Lv$ in $\Omega$ and $u\le v$ on
$\partial\Omega$. Then follows $u\le v$ in $\Omega$.


\subsubsection*{Proof}

\begin{align*}
L\left(u-v\right)\big|_{\Omega} & \ge0\\
\left(u-v\right)\big|_{\partial\Omega} & \le0
\end{align*}
Apply Corollary \ref{sec:Cor-Weak-Maximum-c-non-positive} to obtain:
\begin{align*}
\sup_{\Omega}\left(u-v\right) & \le0 & \Rightarrow\qquad u\big|_{\Omega} & \le v\big|_{\Omega}
\end{align*}
\qqed

Now we want to show the strong maximum principle, i.e. $u\left(x\right)=\max_{\Omega}\left(u\right)$
for $x\in\Omega$ implies that $u$ is constant.


\section{Definition \textmd{(Interior Sphere Condition)}}

A boundary point $x_{0}\in\partial\Omega$ satisfies the \emph{interior
sphere condition} if there is a ball $B\subseteq\Omega$ such that
$x_{0}=\partial B$.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}[scale=0.7]
  \draw[scale=1.5] plot[smooth cycle, tension=0.7] coordinates {(-1,0.3) (1,2) (3,1) (3.5,0) (3,-1)  (0,-1.7)};
  \draw (2.5,2.88) node[above]{$x_0$} +(0.1,0.1) -- +(-0.1,-0.1)  +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (2.5,2.88) +({cos(250)},{sin(250)}) circle (1);
  \node at (2,2) {$B$};
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{Interior sphere condition}
\end{figure}



\section{Lemma}

Assume $c=0$ and $Lu\ge0$ in $\Omega$. Moreover, at the boundary
point $x_{0}\in\partial\Omega$ the following conditions should hold:
\begin{enumerate}[label=\roman*)]
\item $u$ is continuous at $x_{0}$.
\item $u\left(x_{0}\right)>u\left(x\right)$ for all $x\in\Omega$.
\item The interior sphere condition is satisfied.
\end{enumerate}
Then the normal derivative of $u$ at $x_{0}$, if it exists, satisfies
the inequality:
\begin{align*}
\frac{\partial u}{\partial\nu}\left(x_{0}\right) & >0
\end{align*}
We take the \emph{outer} normal.

\emph{Note}: The statement $\frac{\partial u}{\partial\nu}\left(x_{0}\right)\ge0$
is obvious. The point is to prove the \emph{strict} inequality.


\subsubsection*{Proof}

Let $B=B_{R}\left(y\right)$ be the sphere of the interior sphere
condition.
\begin{align*}
v\left(x\right) & :=e^{-\alpha r^{2}}-e^{-\alpha R^{2}} & r & :=\norm{x-y} & r^{2} & =\left(x-y\right)_{i}\left(x-y\right)^{i}
\end{align*}
\begin{align*}
\partial_{i}r^{2} & =2\left(x-y\right)_{i}\\
\partial_{ij}r^{2} & =2\delta_{ij}
\end{align*}
\begin{align*}
\partial_{i}e^{-\alpha r^{2}} & =e^{-\alpha r^{2}}\left(-2\alpha\right)\left(x-y\right)_{i}\\
\partial_{ij}e^{-\alpha r^{2}} & =e^{-\alpha r^{2}}4\alpha^{2}\left(x-y\right)_{i}\left(x-y\right)_{j}+e^{-\alpha r^{2}}\left(-2\alpha\right)\delta_{ij}
\end{align*}
\begin{align*}
Lv & =e^{-\alpha r^{2}}\Big(4\alpha^{2}\underbrace{a^{ij}\left(x\right)\left(x-y\right)_{i}\left(x-y\right)_{j}}_{\le\lambda\norm{x-y}^{2}}-2\alpha\underbrace{\left(a^{ij}\delta_{ij}+b^{i}\left(x-y\right)_{i}\right)}_{\abs{\ldots}\le n^{2}K+nKR}\Big)
\end{align*}
For $\varrho<R$ consider $B_{R}\left(y\right)\setminus B_{\varrho}\left(y\right)$.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \draw plot[smooth cycle, tension=0.7] coordinates { (-7,0) (-5,2) (-1,2.5) ({2*cos(60)},{2*sin(60)}) (3,0.5) (3,-1)  (0,-3) (-2,-3) (-7,-2)};
  \draw (0,0) node[left]{$y$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw[blue] (0,0) circle(2);
  \draw (0,0) -- node[above right]{$R$} (2,0);
  \draw[red] (0,0) circle (1);
  \draw (0,0) -- node[left]{$\varrho$} (0,1);
  \draw (0.5,-1.2) node[left]{$x$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw ({2*cos(60)},{2*sin(60)}) node[right]{$x_0$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$Lv\ge0$ in $B_{R}\left(y\right)\setminus B_{\varrho}\left(y\right)$}
\end{figure}


By choosing $\alpha$ sufficiently large, we can arrange that $Lv\ge0$
in $B_{R}\left(y\right)\setminus B_{\varrho}\left(y\right)$. It holds
$u\left(x\right)-u\left(x_{0}\right)<0$ on ${\color{red}\partial B_{\varrho}\left(y\right)}$.

On ${\color{blue}\partial B_{R}\left(y\right)}$ holds $v=0$ and
$u\left(x\right)-u\left(x_{0}\right)\le0$. Thus follows 
\begin{align*}
u-u\left(x_{0}\right)+\varepsilon v & \le0
\end{align*}
on $\partial\left(B_{R}\left(y\right)\setminus B_{\varrho}\left(y\right)\right)$
for small enough $\varepsilon\in\mathbb{R}_{>0}$ and:
\begin{align*}
L\left(u-u\left(x_{0}\right)+\varepsilon v\right) & \ge0
\end{align*}
The weak maximum principle implies
\begin{align*}
u-u\left(x_{0}\right)+\varepsilon v & \le0
\end{align*}
in $B_{R}\left(y\right)\setminus B_{\varrho}\left(y\right)$. Now
we compute the normal derivative.
\begin{align*}
0\le\frac{\partial}{\partial\nu}\left(u-u\left(x_{0}\right)+\varepsilon v\right)\big|_{x_{0}} & =\frac{\partial u}{\partial\nu}\bigg|_{x_{0}}+\varepsilon\frac{\partial v}{\partial\nu}
\end{align*}
\begin{align*}
\frac{\partial v}{\partial\nu} & =v'\left(r\right)=-2\alpha re^{-\alpha r^{2}}\big|_{r=R}<0
\end{align*}
Therefore follows:
\begin{align*}
\frac{\partial u}{\partial\nu}\left(x_{0}\right) & >0
\end{align*}
\qqed

%DATE: Mi 29.5.13


\section{Theorem \textmd{(Hopf's Maximum Principle)}}

Assume that $L$ is uniformly elliptic with uniformly bounded coefficients
and $c=0$. Furthermore assume $Lu\ge0$ (or $Lu\le0$) in $\Omega$,
but not necessarily bounded. If $u$ attains a maximum (respectively
minimum) in $\Omega$, then $u$ is constant.

For $c\le0$, the function $u$ cannot attain a non-negative maximum
(respectively a non-positively minimum) in $\Omega$, unless $u$
is constant.

This theorem can be viewed as a generalization of Liouville's theorem
to $\mathbb{R}^{n}$.

There were two important mathematicians Hopf:
\begin{itemize}
\item Heinz Hopf (1894-1971): Hopf fibration, Hopf algebra, $\ldots$
\item Ebehard Hopf (1902-1983): Hopf's maximum principle is named after
him.\\
1936 he was Professor in Leipzig, in 1944 he went to Munich as a successor
of Caratheodory. After the war, he went to Bloomington in the USA.
\end{itemize}

\subsubsection*{Proof}

Assume that $Lu\ge0$ and that $u$ attains a maximum $M$ in $\Omega$,
but $u$ is not constant. Define:
\begin{align*}
\Omega^{-} & :=\left\{ x\in\Omega\big|u\left(x\right)<M\right\} \not=\emptyset
\end{align*}
Since $\Omega^{-}\subsetneq\Omega$ is a proper subset, $\partial\Omega^{-}\cap\Omega$
is not empty. For $y\in\Omega^{-}$ let $B_{R}\left(y\right)$ be
the largest ball, which does not intersect $\Omega\setminus\Omega^{-}$.
By choosing a $x_{0}\in\Omega^{-}$ sufficiently close to $\partial\Omega^{-}\cap\Omega$,
one can arrange $B:=B_{R}\left(x_{0}\right)\subseteq\Omega$.

\begin{figure}[H]
\noindent \begin{centering}
\begin{tikzpicture}
  \node at (-0.5,2.75) {$\Omega$};
  \draw plot[smooth cycle, tension=0.7] coordinates { (-7,0) (-5,2) (-1,2.5) (3,0.5) (3,-1)  (0,-3) (-2,-3) (-7,-2)};
  \draw[red] plot[smooth, tension=0.7] coordinates { (-5,2) (-4,1) (-3,0) (-2,0.5) (0,-2)  (1,-2.5)};
  \node at (2,0) {$\Omega ^-$};
  \node at (2,-0.5) {$u<M$};
  \node at (-6,-1) {$\Omega \setminus \Omega ^-$};
  \node at (-6,-1.5) {$u=M$};
  \draw (-1.5,0) node[right]{$y$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (-1.5,0) ++({cos(40)},{sin(40)}) node[right]{$x_0$} circle (1) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \node at (-1,1.2) {$B$};
\end{tikzpicture}
\par\end{centering}

\noindent \centering{}\caption{$B_{R}\left(x_{0}\right)\subseteq\Omega$}
\end{figure}


It holds $\overline{B}\cap\partial\Omega^{-}\not=\emptyset$, since
otherwise the radius would not be maximal. Choose $y\in\overline{B}\cap\partial\Omega^{-}$,
i.e. $u\left(y\right)=M$. We want to apply the previous Lemma to
$\Omega^{-}$ and the boundary point $y$. We check the conditions:
\begin{enumerate}
\item $u$ is continuous in $y$, because we have $u\in C^{2}\left(\Omega\right)$.
\item We have $u\left(y\right)=M$, but $u<M$ in $\Omega^{-}$.
\item The interior sphere condition holds by construction.
\end{enumerate}
So we get:
\begin{align*}
\frac{\partial u}{\partial\nu}\bigg|_{y} & >0
\end{align*}
On the other hand, $\DD u\big|_{y}$ vanishes, because $u$ has a
maximum at $y$. This is a contradiction.\qqed


\chapter{Weak Solutions, Sobolev Spaces}

We can write the elliptic operator
\begin{align*}
L & =a^{ij}\partial_{ij}+b^{i}\partial_{i}+c
\end{align*}
in divergence form:
\begin{align*}
0 & =Lu=a^{ij}\partial_{ij}u+b^{i}\partial_{i}u+cu=\\
 & =\partial_{i}\left(a^{ij}\partial_{j}u\right)+\underbrace{\left(-\partial_{i}a^{ij}+b^{j}\right)}_{=:\tilde{b}^{j}}\partial_{j}u+cu=\\
 & =\partial_{i}\left(a^{ij}\partial_{j}u\right)+\tilde{b}^{j}\partial_{j}u+cu
\end{align*}
We consider $u\in C^{2}\left(\Omega\right)$. Let $\eta\in C_{0}^{\infty}\left(\Omega\right)$
be a ``test function''. We evaluate weakly:
\begin{align*}
\int_{\Omega}\left(Lu\right)\eta\dd^{n}x & =\int_{\Omega}\left(\partial_{i}\left(a^{ij}\partial_{j}u\right)+\tilde{b}^{i}\partial_{i}u+cu\right)\eta\dd^{n}x
\end{align*}
Now we apply Gauß' divergence theorem. The boundary terms vanish due
to $\eta\big|_{\partial\Omega}=0$.
\begin{align*}
\int_{\Omega}\left(Lu\right)\eta\dd^{n}x & =\int_{\Omega}\left(-a^{ij}\left(\partial_{j}u\right)\left(\partial_{i}\eta\right)+\left(\tilde{b}^{i}\partial_{i}u\right)\eta+cu\eta\right)\dd^{n}x
\end{align*}
Now $Lu=0$ is equivalent to $\int_{\Omega}\left(Lu\right)\eta\dd^{n}x=0$,
since $\left(Lu\right)\eta$ is continuous for any $\eta\in C_{0}^{\infty}\left(\Omega\right)$.
\begin{align*}
0 & =\int_{\Omega}\left(-a^{ij}\left(\partial_{j}u\right)\left(\partial_{i}\eta\right)+\left(\tilde{b}^{i}\partial_{i}u\right)\eta+cu\eta\right)\dd^{n}x
\end{align*}
This works just as well for $u\in C^{1}\left(\Omega\right)$. One
can do even better, by only demanding $\partial_{i}u\in L^{1}\left(\Omega\right)$.
The partial derivative is defined weakly. For $u\in C^{1}\left(\Omega\right)$
assume:
\begin{align*}
\partial_{i}u & =v
\end{align*}
Equivalently holds:
\begin{align*}
\int_{\Omega}v\eta\dd^{n}x & =\int_{\Omega}\left(\partial_{i}u\right)\eta\dd^{n}x=-\int_{\Omega}u\left(\partial_{i}\eta\right)\dd^{n}x\qquad\fall_{\eta\in C_{0}^{\infty}\left(\Omega\right)}
\end{align*}
The last condition can be stated for $u\in L^{2}\left(\Omega\right)$.

What are the resulting function spaces?

These are Sobolov space spaces $H^{n,2}\left(\Omega\right)=W^{n,2}\left(\Omega\right)$
(Hilbert spaces, in this case $H^{1,2}\left(\Omega\right)$) or more
general the Sobolov spaces of $n$-times weakly differentiable functions
$W^{n,p}\left(\Omega\right)$ with $\DD^{\beta}f\in L^{p}$ for all
multi-indices $\beta$ with $\abs{\beta}\le n$ (Banach spaces).

Writing the equation in divergence form is also of advantage for using
\emph{variational methods}. For simplicity for $\upDelta u=0$ in
$\Omega$ holds equivalently:
\begin{align*}
\int_{\Omega}\left(\partial_{i}u\right)\left(\partial^{i}\eta\right)\dd^{n}x & =0\qquad\fall_{\eta\in C_{0}^{\infty}\left(\Omega\right)}
\end{align*}
The Dirichlet energy is defined as:
\begin{align*}
E\left(u\right) & :=\frac{1}{2}\int_{\Omega}\norm{\nabla u}^{2}\dd^{n}x
\end{align*}
Minimize $E$ in a suitable Sobolev space ($H^{1,2}\left(\Omega\right)\cap\ldots$).
Suppose a minimizer $u$ exists.
\begin{align*}
E\left(u+\tau\eta\right) & \ge E\left(u\right)\qquad\fall_{\eta\in C_{0}^{\infty}}\fall_{\tau\in\mathbb{R}}
\end{align*}
If $E\left(u+\tau\eta\right)$ is differentiable, now follows:
\begin{align*}
0 & =\frac{\dd}{\dd\tau}E\left(u+\tau\eta\right)\bigg|_{\tau=0}
\end{align*}
For the Dirichlet energy this gives for all $\eta\in C_{0}^{\infty}$:
\begin{align*}
0 & =\frac{\dd}{\dd\tau}\frac{1}{2}\int_{\Omega}\partial_{i}\left(u+\tau\eta\right)\partial^{i}\left(u+\tau\eta\right)\dd^{n}x\bigg|_{\tau=0}=\\
 & =\int_{\Omega}\partial_{i}\left(u+\tau\eta\right)\partial^{i}\eta\dd^{n}x\bigg|_{\tau=0}=\\
 & =\int_{\Omega}\left(\partial_{i}u\right)\left(\partial^{i}\eta\right)\dd^{n}x
\end{align*}
Thus holds $\upDelta u=0$ in the weak sense.

With regularity theory one can proof, that $u$ is smooth.


\section{Hölder Spaces}

Let $\Omega\subseteq\mathbb{R}^{n}$ be open.


\subsection{Definition \textmd{(Hölder Continuous, Hölder Norm)}}

A function $u:\Omega\to\mathbb{R}^{m}$ is called \emph{Hölder continuous}
with exponent $\alpha\in\mathbb{R}_{\ge0}$, if the \emph{Hölder norm}
is finite:
\begin{align*}
\norm u_{C^{0,\alpha}\left(\Omega\right)} & :=\norm u_{C^{0}\left(\Omega\right)}+\sup_{x\not=y\in\Omega}\left(\frac{\norm{u\left(x\right)-u\left(y\right)}}{\norm{x-y}^{\alpha}}\right)<\infty
\end{align*}
This implies:
\begin{align*}
\norm{u\left(x\right)-u\left(y\right)} & \le\sup_{x\not=y\in\Omega}\left(\frac{\norm{u\left(x\right)-u\left(y\right)}}{\norm{x-y}^{\alpha}}\right)\cdot\norm{x-y}^{\alpha}
\end{align*}
We often use the short notation:
\begin{align*}
\norm ._{C^{0,\alpha}\left(\Omega\right)} & =:\norm ._{0,\alpha}
\end{align*}
Obviously holds $C^{0,\alpha}\left(\Omega\right)\subseteq C^{0}\left(\Omega\right)$.

$C^{0,0}\left(\Omega\right)$ are bounded continuous functions.

$C^{0,1}\left(\Omega\right)$ are bounded, Lipschitz continuous functions.

This can also be defined locally: $f\in C_{\text{loc}}^{0,\alpha}\left(\Omega\right)$
means that for all $x\in\Omega$ exists a neighborhood $U\subseteq\Omega$
such that $f\big|_{U}\in C^{0,\alpha}\left(U\right)$.

A typical example is $u\left(x\right)=\sqrt{\abs x}$ on $\left[-1,1\right]$
with $f\in C^{0,\frac{1}{2}}\left(\Omega\right)$.
\begin{align*}
C^{k,\alpha}\left(\Omega\right) & :=\left\{ f\in C^{k}\bigg|\DD^{\beta}f\in C^{0,\alpha}\fall_{\text{multi-index }\beta,\abs{\beta}\le k}\right\} \\
\norm f_{k,\alpha} & =\sum_{\beta,\abs{\beta}\le k}\norm{\DD^{\beta}f}_{0,\alpha}
\end{align*}



\subsection{Proposition \textmd{(Hölder Continuous Functions are Complete)}}

$\left(C^{k,\alpha}\left(\Omega\right),\norm ._{k,\alpha}\right)$
is a Banach space.


\subsubsection*{Proof}

Let $u_{n}\in C^{k,\alpha}\left(\Omega\right)$ be a Cauchy sequence.
Then $u_{n}$ is also a Cauchy sequence in $C^{k}\left(\Omega\right)$.
Since $C^{k}\left(\Omega\right)$ is complete, $\DD^{\beta}u_{n}\rightrightarrows\DD^{\beta}u$
converges uniformly for all multi-indices $\beta$ with $\abs{\beta}\le k$.

Next we have by uniform convergence:
\begin{align*}
\frac{\DD^{\beta}u_{n}\left(x\right)-\DD^{\beta}u_{n}\left(y\right)}{\norm{x-y}^{\alpha}} & \xrightarrow{n\to\infty}\frac{\DD^{\beta}u\left(x\right)-\DD^{\beta}u\left(y\right)}{\norm{x-y}^{\alpha}}
\end{align*}
Therefore the Hölder norms also converge.\qqed


\section{\texorpdfstring{$L^{p}$}{Lp}-Spaces, Approximate Theorems}

\begin{align*}
L^{p}\left(\Omega\right) & :=\left\{ f:\Omega\to\mathbb{R}\text{ measurable}\bigg|\int_{\Omega}\abs{f\left(x\right)}^{p}\dd^{n}x<\infty\right\} 
\end{align*}
\begin{align*}
\norm f_{p} & :=\left(\int\abs{f\left(x\right)}^{p}\dd^{n}x\right)^{\frac{1}{p}}
\end{align*}



\subsection{Definition and Lemma \textmd{(Mollifier)}}

A \emph{mollifier} is a function $\eta\in C_{0}^{\infty}\left(\mathbb{R}^{n}\right)$
with $\eta\left(x\right)\in\mathbb{R}_{\ge0}$ and $\int_{\mathbb{R}^{n}}\eta\left(x\right)\dd^{n}x=1$.
Example:
\begin{align*}
\eta\left(x\right) & =\begin{cases}
ce^{-\frac{1}{\norm x^{2}-1}} & \text{if }\norm x<1\\
0 & \text{otherwise}
\end{cases}
\end{align*}


\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
 \begin{axis}[width=9cm, axis x line=middle, axis y line=middle, ytick={0}, xlabel=$x$, ylabel=$\eta$,xmin=-1.5, xmax=1.5, ymax=0.4 ,samples=300]
  \addplot[mark=none,domain=-0.9999:0.9999] {exp(1/(x^2-1))};
 \end{axis}
\end{tikzpicture} \caption{$\eta\left(x\right)$ is smooth and has compact support $\left[-1,1\right]$.}
\end{figure}


For $u\in L_{\text{loc}}^{1}\left(\Omega\right)$ and $\varepsilon\in\mathbb{R}_{>0}$
define the mollified function as:
\begin{align*}
u_{\varepsilon}\left(x\right) & =\int_{\Omega}\underbrace{\frac{1}{\varepsilon^{n}}\eta\left(\frac{x-y}{\varepsilon}\right)}_{=:\eta_{\varepsilon}\left(x-y\right)}u\left(y\right)\dd^{n}y=\left(\eta_{\varepsilon}*u\right)\left(x\right)
\end{align*}
Now holds $u_{\varepsilon}\in C^{\infty}\left(\Omega'\right)$ for
every $\Omega'\Subset\Omega$.


\subsubsection*{Proof}

The support satisfies $\text{supp}\left(\eta_{\varepsilon}\right)\subseteq B_{\varepsilon}\left(0\right)$.
Define $\varepsilon_{0}:=\text{dist}\left(\Omega',\partial\Omega\right)>0$.
For $\varepsilon<\varepsilon_{0}$, we have for $x\in\Omega'$:
\begin{align*}
u_{\varepsilon}\left(x\right) & =\int_{\Omega'}\eta_{\varepsilon}\left(x-y\right)u\left(y\right)\dd^{n}y
\end{align*}
\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \node at (-0.5,2.75) {$\Omega$};
  \draw plot[smooth cycle, tension=0.7] coordinates { (-7,0) (-5,2) (-1,2.5) (3,0.5) (3,-1)  (0,-3) (-2,-3) (-7,-2)};
  \draw plot[smooth cycle, tension=0.7,scale=0.5] coordinates { (-10,0) (-5,3) (-1,2) (3,0.5) (3,-1)  (0,-4) (-2,-3) (-7,-2)};
  \node at (0,0) {$\Omega ' \Subset \Omega$};
  \draw (-4,0) circle (1.5) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (-4,0) -- node[above]{$\varepsilon$} (-2.5,0);
\end{tikzpicture}\caption{$\text{supp}\left(\eta_{\varepsilon}\right)\subseteq B_{\varepsilon}\left(0\right)\subseteq\Omega$}
\end{figure}


\begin{align*}
\frac{u_{\varepsilon}\left(x+he_{i}\right)-u_{\varepsilon}\left(x\right)}{h} & =\int_{\Omega'}\underbrace{\frac{1}{h}\left(\eta_{\varepsilon}\left(x+he_{i}-y\right)-\eta_{\varepsilon}\left(x-y\right)\right)}_{\xrightarrow{h\to0}\partial_{i}\eta_{\varepsilon}\left(x-y\right)}u\left(y\right)\dd^{n}y
\end{align*}
The limit $h\searrow0$ can be taken inside the integral by Lebesgue's
dominated convergence theorem, since both $\abs u$ and $\eta_{\varepsilon}$
are integrable.\qqed


\subsection{Lemma \textmd{(Mollified Function Converges Locally Uniformly)}\label{sub:Lem-local-convergence}}

For $u\in C^{0}\left(\Omega\right)$ the functions $u_{\varepsilon}\sr{\rightrightarrows}{\text{loc}}{}u$
converge locally uniformly.


\subsubsection*{Proof}

\begin{align*}
u_{\delta}\left(x\right) & =\frac{1}{\delta^{n}}\int_{\Omega}\eta\left(\frac{x-y}{\delta}\right)u\left(y\right)\dd^{n}y
\end{align*}
For $x_{0}\in\Omega$ we choose $\varrho\in\mathbb{R}_{>0}$ such
that $B_{2\varrho}\left(x_{0}\right)\subseteq\Omega$. Let us show
the uniform convergence $u_{\delta}\rightrightarrows u$ in $B_{\varrho}\left(x_{0}\right)$.
We always assume $\delta<\varrho$. Then for all $x\in B_{\varrho}\left(x_{0}\right)$
holds:
\begin{align*}
u_{\delta}\left(x\right) & =\frac{1}{\delta^{n}}\int_{\Omega}\eta\left(\frac{x-y}{\delta}\right)u\left(y\right)\dd^{n}y=\frac{1}{\delta^{n}}\int_{\mathbb{R}^{n}}\eta\left(\frac{x-y}{\delta}\right)u\left(y\right)\dd^{n}y=\\
 & \sr ={z:=\frac{x-y}{\delta}}{\dd^{n}z=\frac{\dd^{n}y}{\delta^{n}}}\frac{1}{\delta^{n}}\int_{\mathbb{R}^{n}}\eta\left(z\right)u\left(x-\delta z\right)\delta^{n}\dd^{n}z=\\
 & =\int_{\mathbb{R}^{n}}\eta\left(z\right)u\left(x-\delta z\right)\dd^{n}z
\end{align*}
$u$ is uniformly continuous on the compact set $\overline{B_{\delta}\left(x\right)}$.
For all $\varepsilon\in\mathbb{R}_{>0}$, there exists a $\delta\in\mathbb{R}_{>0}$
such that for all $y,y'\in\overline{B_{\delta}\left(x\right)}$ holds:
\begin{align*}
\norm{y-y'}<\delta & \qquad\Rightarrow\qquad\abs{u\left(y\right)-u\left(y'\right)}<\varepsilon
\end{align*}
\begin{align*}
\Rightarrow\qquad\left(u_{\delta}-u\right)\left(x\right) & =\int_{\mathbb{R}^{n}}\eta\left(z\right)\left(u\left(x-\delta z\right)-u\left(x\right)\right)\dd^{n}z\\
\abs{\left(u_{\delta}-u\right)\left(x\right)} & \le\int_{\mathbb{R}^{n}}\eta\left(z\right)\abs{u\left(x-\delta z\right)-u\left(x\right)}\dd^{n}z\le\varepsilon\int_{\mathbb{R}^{n}}\eta=\varepsilon
\end{align*}
\qqed

%DATE: Fr 31.5.13


\subsection{Lemma \textmd{(Mollified Function Converges)\label{sub:Lem-Mollified-Function-Converges}}}

For $u\in L_{\text{loc}}^{p}\left(\Omega\right)$ (or $u\in L^{p}\left(\Omega\right)$)
converges $u_{\varepsilon}\xrightarrow{\varepsilon\to0}u$ in $L_{\text{loc}}^{p}\left(\Omega\right)$
(respectively in $L^{p}\left(\Omega\right)$).


\subsubsection*{Proof}

For $\Omega'\Subset\Omega$ holds for small enough $\varepsilon\in\mathbb{R}_{>0}$:
\begin{align*}
\text{dist}\left(\Omega',\partial\Omega\right) & >2\varepsilon
\end{align*}
The definition of $u_{\varepsilon}\left(x\right)$ for $x\in\Omega'$
is:
\begin{align*}
u_{\varepsilon}\left(x\right) & =\int_{\Omega}\frac{1}{\varepsilon^{n}}\eta\left(\frac{x-y}{\varepsilon}\right)u\left(y\right)\dd^{n}y
\end{align*}
By a variable transformation $z:=\frac{x-y}{\varepsilon}$ follows
due to $\text{supp}\left(\eta\right)=B_{1}\left(0\right)$:
\begin{align*}
u_{\varepsilon}\left(x\right) & =\int_{\norm z\le1}\eta\left(z\right)u\left(y+\varepsilon z\right)\dd^{n}z
\end{align*}
Now define the measure $\dd\varrho:=\eta\left(z\right)\dd z$ and
apply Hölder's inequality:
\begin{align*}
\abs{u_{\varepsilon}\left(x\right)} & =\abs{\int_{\norm z\le1}u\left(x+\varepsilon z\right)\dd\varrho}\sr{\le}{\text{Hölder}}{}\underbrace{\left(\int_{\norm z\le1}1\dd\varrho\right)^{\frac{1}{q}}}_{=1}\left(\int_{\norm z\le1}\abs u^{p}\left(x+\varepsilon z\right)\dd\varrho\right)^{\frac{1}{p}}
\end{align*}
This implies:
\begin{align*}
\abs{u_{\varepsilon}\left(x\right)}^{p} & \le\int_{\norm z\le1}\abs u^{p}\left(x+\varepsilon z\right)\eta\left(z\right)\dd z
\end{align*}
Integrating over $x$ yields:
\begin{align*}
\int_{\Omega'}\abs{u_{\varepsilon}\left(x\right)}^{p}\dd x & \le\int_{\Omega'}\dd x\int_{\norm z\le1}\abs u^{p}\left(x+\varepsilon z\right)\eta\left(z\right)\dd z=\\
 & \sr ={\text{Fubini}}{}\int_{\norm z\le1}\dd z\eta\left(z\right)\int_{\Omega'}\dd x\abs u^{p}\left(x+\varepsilon z\right)\le\\
 & \le\underbrace{\int_{\norm z\le1}\dd z\eta\left(z\right)}_{=1}\int_{\Omega''}\dd x\abs u^{p}\left(x\right)=\int_{\Omega''}\dd x\abs u^{p}\left(x\right)
\end{align*}


\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw plot[smooth cycle, xshift=-2.5cm,yshift=2cm] coordinates {(0,0) (1,1) (4,1) (5,0) (5,-3) (4,-5) (1,-4) };
  \node at (0,3.3){$\Omega$};
  \draw[rotate=30] (0,0) circle (1 and 1.5);
  \node at (0,0){$\Omega '$};
  \draw[rotate=30,dashed,scale=1.3] (0,0) circle (1 and 1.5);
  \node at (0,2){$\Omega ''$};
  \draw (0.75,-1.3) -- node[above right=-2pt]{$\varepsilon$} +(0.21,-0.37) ;
\end{tikzpicture}\caption{$\Omega'':=\left\{ x+\varepsilon z\big|x\in\Omega',\norm z\le1\right\} $}
\end{figure}


This means:
\begin{align*}
\norm{u_{\varepsilon}}_{L^{p}\left(\Omega'\right)} & \le\norm u_{L^{p}\left(\Omega''\right)}
\end{align*}
We want to apply the Lemma \ref{sub:Lem-local-convergence}. For $u\in L_{\text{loc}}^{p}\left(\Omega\right)$
and $\varepsilon\in\mathbb{R}_{>0}$ choose $\omega\in C^{0}\left(\Omega\right)$
with:
\begin{align*}
\norm{u-\omega}_{L^{p}\left(\Omega''\right)} & \le\varepsilon
\end{align*}
This is possible after Urysohn's lemma (cf. the book of Rudin). With
\begin{align*}
\norm{\omega_{\varepsilon}-u_{\varepsilon}}_{L^{p}\left(\Omega'\right)} & \le\norm{\omega-u}_{L^{p}\left(\Omega''\right)}\le\varepsilon
\end{align*}
this gives:
\begin{align*}
\norm{u-u_{\varepsilon}}_{L^{p}\left(\Omega'\right)} & \le\norm{u-\omega}_{L^{p}\left(\Omega'\right)}+\norm{\omega-\omega_{\varepsilon}}_{L^{p}\left(\Omega'\right)}+\norm{\omega_{\varepsilon}-u_{\varepsilon}}_{L^{p}\left(\Omega'\right)}\le\\
 & \le\norm{u-\omega}_{L^{p}\left(\Omega''\right)}+\norm{\omega-\omega_{\varepsilon}}_{L^{p}\left(\Omega'\right)}+\norm{\omega-u}_{L^{p}\left(\Omega''\right)}\le\\
 & \le2\norm{u-\omega}_{L^{p}\left(\Omega''\right)}+\norm{\omega-\omega_{\varepsilon}}_{L^{p}\left(\Omega'\right)}\le\\
 & \le2\varepsilon+\norm{\omega-\omega_{\varepsilon}}_{L^{p}\left(\Omega'\right)}
\end{align*}
By Lemma \ref{sub:Lem-local-convergence} holds, since $\omega$ is
continuous:
\begin{align*}
\norm{\omega-\omega_{\varepsilon}}_{L^{p}\left(\Omega'\right)} & \xrightarrow{\varepsilon\to0}0
\end{align*}
Therefore converges $u_{\varepsilon}\to u$ in $L_{\text{loc}}^{p}\left(\Omega\right)$.

To get the statement without ``loc'', extend $u$ by 0 to all of
$\mathbb{R}^{n}$. If $\Omega$ is bounded, $u_{\varepsilon}\to u$
converges in $L^{p}\left(\Omega\right)$ as above, as $\overline{\Omega}$
is compact, so $\overline{\Omega}\Subset\mathbb{R}^{n}$. If $\Omega$
is not bounded, we choose a ball $B_{R}\left(0\right)$ such that
$\norm u_{L^{p}\left(\Omega\setminus B_{R}\left(0\right)\right)}\le\varepsilon$.
This is possible, since $u$ is integrable on $\Omega$.\qqed


\subsection{Corollary \textmd{(Dense Subsets of Integrable Functions)}}

$C^{\infty}\left(\Omega\right)$ and $C^{k,\alpha}\left(\Omega\right)$
are dense in $L_{\text{loc}}^{p}\left(\Omega\right)$ and $C_{0}^{\infty}\left(\Omega\right)$
is dense in $L^{p}\left(\Omega\right)$ for $1\le p<\infty$.


\section{Sobolev Spaces}


\subsection{Definition \textmd{(Weak Derivative)}}

Consider $u,v\in L_{\text{loc}}^{1}\left(\Omega\right)$ and a multi-index
$\alpha$. The function $u$ is the $\alpha$-th \emph{weak derivative}
of $ $$v$ if and only if for all test functions $\eta\in C_{0}^{\infty}\left(\Omega\right)$
holds:
\begin{align*}
\int_{\Omega}v\left(x\right)\left(-1\right)^{\abs{\alpha}}\DD^{\alpha}\eta\left(x\right)\dd x & =\int_{\Omega}u\left(x\right)\eta\left(x\right)\dd x
\end{align*}
This reminds of the definition of the distributional derivative $T_{u}=\DD^{\alpha}T_{v}$
by
\begin{align*}
T_{u}\left(\eta\right) & =\left(\DD^{\alpha}T_{v}\right)\left(\eta\right):=\left(-1\right)^{\abs{\alpha}}T_{v}\left(\DD^{\alpha}\eta\right)
\end{align*}
for all Schwartz functions $\eta$.

Here it is more general, because we have less test functions $\eta\in C_{0}^{\infty}$,
but on the other hand less general due to $v\in L_{1}^{\text{loc}}$,
so $T_{v}$ is a kind of ``regular distribution''.


\subsection{Definition \textmd{(Sobolev Space)}}

The \emph{Sobolev space} $H^{k,p}\left(\Omega\right)$ for $k\in\mathbb{N}_{\ge0}$
and $1\le p\le\infty$ consists of $L_{\text{loc}}^{1}$ functions
$u:\Omega\to\mathbb{C}$ such that for all multi-indices $\alpha$
with $\abs{\alpha}\le k$, the weak derivative $\DD^{\alpha}u$ exists
and is in $L^{p}\left(\Omega\right)$.
\begin{align*}
\norm u_{H^{k,p}\left(\Omega\right)} & :=\left(\sum_{\abs{\alpha}\le k}\int_{\Omega}\abs{\DD^{\alpha}u}^{p}\dd x\right)^{\frac{1}{p}}
\end{align*}
A special case of interest is $p=2$, where we have an associated
scalar product.
\begin{align*}
\left\langle u,v\right\rangle _{H^{k,2}\left(\Omega\right)} & :=\sum_{\abs{\alpha}\le k}\int_{\Omega}\overline{\DD^{\alpha}u}\cdot\DD^{\alpha}v\dd x
\end{align*}



\subsection{Theorem \textmd{(Sobolev Spaces are Complete)\label{sub:Thm-Sobolev-Spaces-Complete}}}

The spaces $H^{k,p}\left(\Omega\right)$ are Banach spaces and Hilbert
spaces for $p=2$.


\subsubsection*{Proof}

We have to show completeness. Let $u_{n}$ be a Cauchy sequence in
$H^{k,p}\left(\Omega\right)$. Completeness of $L^{p}\left(\Omega\right)$
implies that $u_{n}^{\alpha}:=\DD^{\alpha}u_{n}\xrightarrow{n\to\infty}v^{\alpha}$
converges in $L^{p}\left(\Omega\right)$. Then holds
\begin{align*}
v & :=\lim_{n\to\infty}u_{n}\in H^{k,p}\left(\Omega\right)
\end{align*}
and $\partial^{\alpha}v=v^{\alpha}$, because for all $\eta\in C_{0}^{\infty}\left(\Omega\right)$
holds:
\begin{align*}
\int_{\text{supp}\left(\eta\right)}v\left(-1\right)^{\abs{\alpha}}\DD^{\alpha}\eta\dd x\leftarrow\int u_{n}\left(-1\right)^{\abs{\alpha}}\DD^{\alpha}\eta\dd x & =\int u_{n}^{\alpha}\eta\dd x\to\int_{\text{supp}\left(\eta\right)}v^{\alpha}\eta\dd x
\end{align*}
This is true, because convergence in $L^{p}$ implies convergence
in $L_{\text{loc}}^{1}$.\qqed

What is the relation between the weak derivative and mollifying?


\subsection{Lemma \textmd{(Weak Derivative and Mollifying Commute)\label{sub:Lemma-Weak-Derivative-Mollifying-Commute}}}

Let $u\in L_{\text{loc}}^{1}\left(\Omega\right)$ be $\alpha$-times
\emph{weakly differentiable}, i.e. $\DD^{\alpha}u$ exists for the
multi-index $\alpha$. Let $\Omega'\Subset\Omega$ be a compact subset
with $\text{dist}\left(\Omega',\partial\Omega\right)>\varepsilon\in\mathbb{R}_{>0}$.
Then holds:
\begin{align*}
\left(\DD^{\alpha}u\right)_{\varepsilon} & =\DD^{\alpha}u_{\varepsilon}
\end{align*}



\subsubsection*{Proof}

\begin{align*}
\DD^{\alpha}u_{\varepsilon}\left(x\right) & =\int_{\mathbb{R}^{n}}\DD_{x}^{\alpha}\eta_{\varepsilon}\left(x-y\right)u\left(y\right)=\int_{\mathbb{R}^{n}}\left(-1\right)^{\abs{\alpha}}\DD_{y}^{\alpha}\eta_{\varepsilon}\left(x-y\right)u\left(y\right)\dd y=\\
 & \sr ={\text{integration}}{\text{by parts}}\int_{\mathbb{R}^{n}}\eta_{\varepsilon}\left(x-y\right)\DD^{\alpha}u\left(y\right)\dd y=\left(\DD^{\alpha}u\right)_{\varepsilon}
\end{align*}
\qqed


\subsection{Theorem \textmd{(Dense Subset of Sobolev Space)}}

$C^{\infty}\left(\Omega\right)\cap H^{k,p}\left(\Omega\right)$ is
dense in $H^{k,p}\left(\Omega\right)$.


\subsubsection*{Proof}

Let $\left(\Omega_{n}\right)_{n\in\mathbb{N}}$ be an exhaustion of
$\Omega$ by relatively compact sets $\Omega_{n}$, i.e. $\Omega_{n}\Subset\Omega_{n+1}\Subset\Omega$
and $\bigcup_{n\in\mathbb{N}}\Omega_{n}=\Omega$, with $\Omega_{0}\not=\emptyset$.
Define $A_{n}:=\Omega_{n+1}\setminus\overline{\Omega}_{n}$. The $A_{n}$
form a locally finite open covering.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw plot[smooth cycle] coordinates {(0,0) (1,1) (4,1) (5,0) (5,-3) (4,-5) (1,-4) };
  \node at (1,0.25){$\Omega_3 $};
  \draw plot[smooth cycle,scale=0.7,xshift=1cm,yshift=-1cm] coordinates {(0,-0.5) (1,1) (3,1) (5,1.5) (5,-3.5) (4,-4.5) (1,-4) };
  \node at (2,-0.25){$\Omega_2 $};
  \node at (3.5,-3.6){$A_1 $};
  \draw plot[smooth cycle,scale=0.5,xshift=2.5cm,yshift=-2.5cm] coordinates {(0,-1) (1,0) (3,0.5) (5,1) (5,-3.5) (4,-3) (1,-4) };
  \node at (2.5,-2){$\Omega_1 $};
  \node at (4.3,-4.1){$A_2 $};
\path[even odd rule] plot[smooth cycle,scale=0.7,xshift=1cm,yshift=-1cm] coordinates {(0,-0.5) (1,1) (3,1) (5,1.5) (5,-3.5) (4,-4.5) (1,-4) }
plot[smooth cycle,scale=0.5,xshift=2.5cm,yshift=-2.5cm] coordinates {(0,-1) (1,0) (3,0.5) (5,1) (5,-3.5) (4,-3) (1,-4) }
[path picture={
  \foreach \x in {-5,-4,...,5}
    \draw (0,\x) -- (5,{\x +1});
}];
\path[even odd rule] plot[smooth cycle,scale=0.7,xshift=1cm,yshift=-1cm] coordinates {(0,-0.5) (1,1) (3,1) (5,1.5) (5,-3.5) (4,-4.5) (1,-4) }
plot[smooth cycle] coordinates {(0,0) (1,1) (4,1) (5,0) (5,-3) (4,-5) (1,-4) }
[path picture={
  \foreach \x in {-5,-4,...,5}
    \draw (-1,\x) -- (6,{\x -1});
}];
\end{tikzpicture}\caption{$A_{n}$ form locally finite open covering}
\end{figure}


Let $\left(\varphi_{n}\right)_{n\in\mathbb{N}}$ be a subordinate
partition of unity, i.e. $\varphi_{n}\in C_{0}^{\infty}\left(A_{n}\right)$
and $\sum_{n}\varphi_{n}=1$. For $u\in H^{k,p}\left(\Omega\right)$
holds $\varphi_{n}u\in H^{k,p}\left(A_{n}\right)$.

Now $\left(\varphi_{n}u\right)_{\varepsilon_{n}}\xrightarrow{\varepsilon_{n}\to0}\varphi_{n}u$
converges in $L^{p}\left(A_{n}\right)$ after Lemma \ref{sub:Lem-Mollified-Function-Converges}
due to $\varphi_{n}u\in L^{p}\left(A_{n}\right)$. Since weak derivative
and mollifying commute (Lemma \ref{sub:Lemma-Weak-Derivative-Mollifying-Commute}),
follows in $L^{p}\left(A_{n}\right)$:
\begin{align*}
\DD^{\alpha}\left(\left(\varphi_{n}u\right)_{\varepsilon_{n}}\right) & =\left(\DD^{\alpha}\varphi_{n}u\right)_{\varepsilon_{n}}\xrightarrow{\varepsilon_{n}\to0}\DD^{\alpha}\varphi_{n}u
\end{align*}
So $\left(\varphi_{n}u\right)_{\varepsilon_{n}}\xrightarrow{\varepsilon_{n}\to0}\varphi_{n}u$
converges in $H^{k,p}\left(\Omega\right)$. Therefore one can choose
$\varepsilon_{n}\in\mathbb{R}_{>0}$ such that for every $\delta\in\mathbb{R}_{>0}$
holds:
\begin{align*}
\norm{\left(\varphi_{n}u\right)_{\varepsilon_{n}}-\varphi_{n}u}_{H^{k,p}\left(\Omega\right)} & \le\frac{\delta}{2^{n}}
\end{align*}
Define:
\begin{align*}
\tilde{u} & =\sum_{n=1}^{\infty}\left(\varphi_{n}u\right)_{\varepsilon_{n}}\in C^{\infty}\left(\Omega\right)
\end{align*}
This $\tilde{u}$ is smooth, because the sum is locally finite and
the $\left(\varphi_{n}u\right)_{\varepsilon_{n}}$ are smooth. So
we get:
\begin{align*}
\norm{u-\tilde{u}}_{H^{k,p}\left(\Omega\right)} & \le\sum_{n=1}^{\infty}\frac{\delta}{2^{n}}=\delta
\end{align*}
\qqed

Warning: $C_{0}^{\infty}\left(\Omega\right)$ is in general \emph{not}
dense in $H^{k,p}\left(\Omega\right)$. This motivates the following
definition:


\subsection{Definition \textmd{(Sobolev Spaces for Dirichlet Boundary)}}

The \emph{Sobolev spaces for Dirichlet boundary} condition are defined
as:
\begin{align*}
H_{0}^{k,p}\left(\Omega\right) & :=\overline{C_{0}^{\infty}\left(\Omega\right)}^{H^{k,p}}\subseteq H^{k,p}
\end{align*}
We have:
\begin{align*}
C^{\infty}\left(\Omega\right)\cap H^{k,p}\left(\Omega\right) & \sr{\subseteq}{\text{dense}}{}H^{k,p}\left(\Omega\right)\subseteq L^{p}\left(\Omega\right)\\
\overset{\Rotate{\subseteq}}{C_{0}^{\infty\vphantom{k}}\left(\Omega\right)} & \sr{\subseteq}{\text{dense}}{}\overset{\Rotate{\subseteq}}{H_{0}^{k,p}\left(\Omega\right)}\overset{\left(\text{in general not dense}\right)}{\vphantom{\bigg|}}
\end{align*}
Is $C^{\infty}\left(\overline{\Omega}\right)\cap H^{k,p}\left(\Omega\right)$
dense in $H_{0}^{k,p}\left(\Omega\right)$? In general it is not,
but for smooth boundary of $\Omega$, it is.


\subsection{Theorem \textmd{(Elementary Properties of Sobolev Functions)}}
\begin{enumerate}[label=\roman*)]
\item For $u\in H^{k,p}\left(\Omega\right)$ holds $\DD^{\alpha}u\in H^{k-\abs{\alpha},p}\left(\Omega\right)$.
\item For $\Omega'\subseteq\Omega$ and $u\in H^{k,p}\left(\Omega\right)$
holds $u\big|_{\Omega'}\in H^{k,p}\left(\Omega'\right)$.
\item Leibniz rule: For $u\in H^{k,p}\left(\Omega\right)$, $v\in C_{0}^{\infty}$
and $\abs{\alpha}\le k$ holds:
\begin{align*}
vu & \in H^{k,p}
\end{align*}
\begin{align*}
\DD^{\alpha}\left(vu\right) & =\sum_{\beta\le\alpha}\left(\begin{array}{c}
\alpha\\
\beta
\end{array}\right)\DD^{\beta}v\DD^{\alpha-\beta}u
\end{align*}
For multi-indices $\alpha=\left(i_{1},\ldots,i_{r}\right)$ and $\beta=\left(j_{1},\ldots,j_{s}\right)$
with $i_{l},j_{l}\in\left\{ 1,\ldots,n\right\} $ let $\alpha_{m}$
be the number indices $i_{l}=m$ and analog $\beta_{m}$. The Binomial
coefficient is defined as follows: 
\begin{align*}
\left(\begin{array}{c}
\alpha\\
\beta
\end{array}\right) & :=\left(\begin{array}{c}
\alpha_{1}\\
\beta_{1}
\end{array}\right)\cdot\left(\begin{array}{c}
\alpha_{2}\\
\beta_{2}
\end{array}\right)\cdot\ldots\cdot\left(\begin{array}{c}
\alpha_{n}\\
\beta_{n}
\end{array}\right)
\end{align*}

\item Chain rule: For $f\in C^{1}$, $f'\in L^{\infty}$ and $u$ weakly
differentiable holds:
\begin{align*}
\DD\left(f\circ u\right) & =f'\left(u\right)\DD u
\end{align*}

\end{enumerate}

\subsubsection*{Proof}

i) and ii) are obvious.
\begin{enumerate}
\item [iii)]For brevity consider the first derivatives only.
\begin{align*}
\int\left(vu\right)\left(-1\right)\left(\DD\eta\right)\dd x & \sr ={\text{Leibniz}}{\text{for }C^{\infty}}\int u\left(-1\right)\left(\DD\left(\eta v\right)-\eta\left(\DD v\right)\right)\dd x=\\
 & \sr ={\text{integtation}}{\text{by parts}}\int\left(\left(\DD u\right)\eta v+u\eta\DD v\right)\dd x=\int\left(v\DD u+u\DD v\right)\eta\dd x\\
\Rightarrow\qquad\DD\left(uv\right) & =\left(\DD u\right)v+u\left(\DD v\right)
\end{align*}
Be careful: A product of two $H^{k,p}$ functions need not be in $H^{k,p}$
in general!
\item [iv)]We use an approximation argument: Consider $\Omega'\Subset\Omega$,
$u_{n}\in C^{\infty}\left(\Omega'\right)$ with $u_{n}\to u$ and
$\DD u_{n}\to\DD u$ converging in $L^{1}\left(\Omega'\right)$. The
chain rule in $C^{1}$ gives $\DD\left(f\circ u_{n}\right)=f'\left(u_{n}\right)\DD u_{n}$.
Now evaluate in the weak sense with a test function $\eta\in C_{0}^{\infty}\left(\Omega\right)$:
\begin{align*}
\int\left(f\circ u_{n}\right)\left(-\DD\eta\right)\dd x & =\int f'\left(u_{n}\right)\left(\DD u_{n}\right)\eta\dd x
\end{align*}
We have to show, that we can take the limit on both sides.
\begin{align*}
\int_{\Omega'}\abs{f\left(u_{n}\right)-f\left(u\right)}\abs{\DD\eta}\dd x & \le\sup\abs{\DD\eta}\sup\abs{f'}\int_{\Omega'}\left(u_{n}-u\right)\dd x\xrightarrow{n\to\infty}0
\end{align*}
The general technique
\begin{align*}
\abs{a_{n}b_{n}-ab} & =\abs{a_{n}b_{n}-ab_{n}+ab_{n}-ab}\le\abs{a_{n}-a}\abs{b_{n}}+\abs a\abs{b_{n}-b}
\end{align*}
gives for the other side:
\begin{align*}
 & \abs{\int_{\Omega'}f'\left(u_{n}\right)\DD u_{n}-f'\left(u\right)\left(\DD u\right)\eta\dd x}\le\\
 & \qquad\qquad\le\int_{\Omega'}\abs{f'\left(u_{n}\right)-f'\left(u\right)}\abs{\DD u}\abs{\eta}\dd x+\sup\abs{f'}\cdot\int_{\Omega'}\abs{\DD u_{n}-\DD u}\abs{\eta}\dd x
\end{align*}
The second term converges to zero, because $\DD u_{n}\to\DD u$ converges
in $L^{1}\left(\Omega'\right)$.\\
The first term converges to zero, because as $u_{n}\to u$ converges
almost everywhere, $f'\left(u_{n}\right)\to f'\left(u_{n}\right)$
converges almost everywhere and due to$f'\in C^{0}$ we can apply
the dominated convergence theorem to take the limit under the integral.\qqed
\end{enumerate}
%DATE: Mi 5.6.13


\section{Traces of Sobolev Functions at the Boundary}

Note: These traces have nothing to do with traces in linear algebra.

Consider $u\in H^{k,p}\left(\Omega\right)$.

Questions: What is $u\big|_{\partial\Omega}$? In which space is it?

Assume for simplicity that $\partial\Omega$ is $C^{1}$, i.e. $\overline{\Omega}$
is a manifold with $C^{1}$-boundary $\partial\Omega$.

\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw plot[smooth cycle] coordinates {(2,0) (1.5,1) (0,1.5) (-2,1) (-1,-2) (1,-1)};
  \node at (0,1) {$\Omega$};
  \draw (0.5,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (0.5,0) circle (0.5);
  \draw[thick,->] (0.95,0.45) .. controls +(1,1) and +(-1,0.5) .. (4.7,1);
  \draw (5.5,0.5) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (5.5,0.5) circle (0.5);
  \node at (5.5,1.25) {$\mathbb{R}^n$};
  \draw (0.5,-1.35) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (0.5,-1.35) +({0.5*cos(35)},{0.5*sin(35)}) arc (35:215:0.5);
  \draw[thick,red] (0.5,-1.35) +({0.5*cos(35)},{0.5*sin(35)}) -- +({-0.5*cos(35)},{-0.5*sin(35)});
  \draw[thick,->] (0.7,-1.5) .. controls +(1,-0.6) and +(-1,0) .. (4,-2);
  \draw[->] (4.5,-2) -- (6.5,-2) node[right]{$\mathbb{R}$};
  \draw[->] (5.5,-3) -- (5.5,-1) node[above]{$\mathbb{R}^{n-1}$};
  \draw (5.5,-1.5) arc (90:-90:0.5);
  \draw (5.5,-2) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \foreach \x in {-2,-1,...,2}     \draw (5.5,{0.3*\x - 2}) -- (6.25,{0.3*\x+0.2 -2});
  \draw[very thick,red] (5.5,-2.5) -- (5.5,-1.5);
\end{tikzpicture}\caption{$\Omega$ is a manifold with $C^{1}$-boundary.}
\end{figure}


Thus for every $p\in\partial\Omega$ exists a neighborhood $p\in U\subseteq\overline{\Omega}$
open in the relative topology and a mapping
\begin{align*}
\varphi:U & \to\mathbb{R}^{n}
\end{align*}
with $\varphi\left(U\right)\subseteq\mathbb{R}_{+}^{n}\left\{ x\in\mathbb{R}^{n}\big|x^{n}\ge0\right\} $
and $\varphi\left({\color{red}\partial\Omega\cap U}\right)\subseteq\left\{ x\in\mathbb{R}^{n}\big|x^{n}=0\right\} $
such that the mapping $\varphi:U\to\varphi\left(U\right)$ is a $C^{1}$-Diffeomorphism.

Moreover we assume that $\Omega$ is bounded. Thus $\partial\Omega$
is compact and therefore we can work with a finite number of charts
$\left(\varphi_{i},U_{i}\right)$ with $i\in\left\{ 1,\ldots,N\right\} $
and $\partial\Omega\subseteq\bigcup_{i=1}^{N}U_{i}$.


\subsection{Theorem and Definition \textmd{(Trace)}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be a bounded domain with $C^{1}$-boundary.
Then there is a unique bounded operator $T:H^{1,p}\left(\Omega\right)\to L^{p}\left(\partial\Omega\right)$,
called \emph{trace}, such that for all $u\in H^{1,p}\left(\Omega\right)\cap C^{0}\left(\overline{\Omega}\right)$
holds for all $x\in\partial\Omega$:
\begin{align*}
\left(Tu\right)\left(x\right) & =u\left(x\right)
\end{align*}



\subsubsection*{Proof}

First consider $u\in C^{1}\left(\overline{\Omega}\right)$. ($C^{1}\left(\overline{\Omega}\right)\subseteq H^{1,p}\left(\Omega\right)$
holds, because $\Omega$ is bounded.) We show that $T$ is bounded,
i.e.
\begin{align*}
\norm{Tu}_{L^{p}\left(\partial\Omega\right)}\le & K\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
with a $C\in\mathbb{R}_{>0}$ independent of $u$. Then $T$ has a
unique continuation to $\overline{C^{1}\left(\overline{\Omega}\right)}^{H^{1,p}}=H^{1,p}\left(\Omega\right)$,
because $C^{1}$ is dense in $H^{1,p}$.
\begin{enumerate}
\item Let $u\in C^{1}\left(\overline{\Omega}\right)$, $y\in\partial\Omega$
and assume that $\partial\Omega$ is flat in a neighborhood $U$ of
$y$, i.e. a hyperplane. By a rotation in $\mathbb{R}^{n}$, we can
arrange that $U\cap\Omega=U\cap\mathbb{R}_{+}^{n}$. Set $\tilde{x}=\left(x^{1},\ldots,x^{n-1}\right)$.\\
\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}[scale=2]
  \draw[->] (0,0) -- (3,0) node[right]{$\tilde x=(x^1,\ldots , x^{n-1})$};
  \draw[->] (0,0) -- (0,1.5) node[above]{$x^n$};
  \draw[scale=0.5] (3,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (1.5,0) node[below]{$y$} +(1,0) arc (0:180:1);
  \node at (1,1.2) {$U$};
  \draw[red] (1.5,0) +(0.8,0) arc (0:180:0.8);
  \draw[red] (1.5,0) -- node[left=-2pt]{$2r$} +({0.8*cos(60)},{0.8*sin(60)});
\end{tikzpicture}\caption{$U\cap\Omega=U\cap\mathbb{R}_{+}^{n}$}
\end{figure}
Choose $r\in\mathbb{R}_{>0}$ with $B_{2r}\left(y\right)\cap\mathbb{R}_{+}^{n}\subseteq U$
and $\eta\in C_{0}^{\infty}\left(B_{2r}\left(y\right)\right)$ with
$\eta\big|_{B_{r}\left(y\right)}=1$ and $\eta\ge0$. Define $\Gamma:=B_{r}\left(y\right)\cap\partial\Omega$.
\begin{align*}
\int_{\Gamma}\abs{u\left(\tilde{x},0\right)}^{p}\dd^{n-1}\tilde{x} & \le\int_{\mathbb{R}^{n-1}}\underbrace{\eta\left(\tilde{x},0\right)\abs{u\left(\tilde{x},0\right)}^{p}}_{\ge0}\dd^{n-1}\tilde{x}=\\
 & =\int_{\mathbb{R}^{n-1}}\left(\int_{0}^{\infty}-\frac{\partial}{\partial x^{n}}\left(\eta\left(x\right)\abs{u\left(x\right)}^{p}\right)\dd x^{n}\right)\dd^{n-1}\tilde{x}=\\
 & \sr ={\text{Fubini}}{}-\int_{\mathbb{R}_{+}^{n}}\frac{\partial}{\partial x^{n}}\left(\eta\left(x\right)\abs{u\left(x\right)}^{p}\right)\dd^{n}x=\\
 & =\int_{\mathbb{R}_{+}^{n}}-\frac{\partial\eta}{\partial x^{n}}\abs{u\left(x\right)}^{p}-\text{sgn}\left(u\left(x\right)\right)\eta pu^{p-1}\left(\frac{\partial}{\partial x^{n}}u\right)\dd^{n}x
\end{align*}
The derivative of $\abs{u\left(x\right)}^{p}$ does not exist for
$p=1$ and $u=0$, but any point with $u=0$ does not contribute to
the original integral. We estimate:
\begin{align*}
\int_{\Gamma}\abs{u\left(\tilde{x},0\right)}^{p}\dd^{n-1}\tilde{x} & \le\abs{\frac{\partial\eta}{\partial x^{n}}}_{C^{0}}\int_{\mathbb{R}_{+}^{n}\cap\Omega}\abs u^{p}\dd^{n}x+\abs{\eta}_{C^{0}}p\cdot\int_{\mathbb{R}_{+}^{n}\cap\Omega}\abs u^{p-1}\abs{\partial_{n}u}\dd^{n}x
\end{align*}
Now we use Young's inequality for $1\le p\le\infty$ with $\frac{1}{p}+\frac{1}{q}=1$:
\begin{align*}
ab & \le\frac{a^{p}}{p}+\frac{b^{q}}{q}
\end{align*}
This yields for $q=\frac{1}{1-\frac{1}{p}}=\frac{p}{p-1}$:
\begin{align*}
\abs{\partial_{n}u}\abs u^{p-1} & \le\frac{\abs{\partial_{n}u}^{p}}{p}+\frac{\abs u^{q\left(p-1\right)}}{q}=\frac{\abs{\partial_{n}u}^{p}}{p}+\frac{\abs u^{p}}{q}
\end{align*}
So we get:
\begin{align*}
\int_{\Gamma}\abs{u\left(\tilde{x},0\right)}^{p}\dd^{n-1}\tilde{x} & \le\abs{\frac{\partial\eta}{\partial x^{n}}}_{C^{0}}\int_{\mathbb{R}_{+}^{n}\cap\Omega}\abs u^{p}\dd^{n}x+\abs{\eta}_{C^{0}}p\cdot\int_{\mathbb{R}_{+}^{n}\cap\Omega}\left(\frac{\abs{\partial_{n}u}^{p}}{p}+\frac{\abs u^{p}}{q}\right)\dd^{n}x\le\\
 & \le\abs{\frac{\partial\eta}{\partial x^{n}}}_{C^{0}}\int_{\mathbb{R}_{+}^{n}\cap\Omega}\left(\abs u^{p}+\abs{\partial_{n}u}^{p}\right)\dd^{n}x+\\
 & \qquad+\abs{\eta}_{C^{0}}\left(1+\frac{p}{q}\right)\int_{\mathbb{R}_{+}^{n}\cap\Omega}\left(\abs{\partial_{n}u}^{p}+\abs u^{p}\right)\dd^{n}x=\\
 & =\left(\abs{\frac{\partial\eta}{\partial x^{n}}}_{C^{0}}+\abs{\eta}_{C^{0}}\left(1+\frac{p}{q}\right)\right)\int_{\mathbb{R}_{+}^{n}\cap\Omega}\left(\abs u^{p}+\abs{\partial_{n}u}^{p}\right)\dd^{n}x\le\\
 & \le C\norm u_{H^{1,p}\left(\Omega\right)}^{p}
\end{align*}
All in all follows:
\begin{align*}
\norm{Tu}_{L^{p}\left(\Gamma\right)} & \le C^{\frac{1}{p}}\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}

\item Now consider a general $\Omega$. We cover the boundary by a finite
number $N\in\mathbb{N}$ of balls with charts $\left(\varphi_{i},U_{i}\right)$.
Let $\eta_{i}\left(\tilde{x}\right)$ be a subordinate partition of
unity to get:
\begin{align*}
\int_{\partial\Omega}\abs u^{p}\dd\mu_{\partial\Omega} & =\sum_{i=1}^{N}\int_{\varphi_{i}\left(U_{i}\right)}\underbrace{\eta_{i}\left(\tilde{x}\right)}_{\le C\left(\Omega\right)}\underbrace{\abs{\left(u\circ\varphi_{i}^{-1}\right)\left(\tilde{x}\right)}^{p}}_{=\abs{u\left(\tilde{y}\right)}^{p}}\underbrace{\abs{\det\left(\frac{\partial\varphi}{\partial\tilde{x}}\right)}\dd^{n-1}\tilde{x}}_{=\dd^{n-1}\tilde{y}}\le\\
 & \le C\left(\Omega\right)\sum_{i=1}^{N}\int_{\Gamma_{i}}\abs{u\left(\tilde{y}\right)}^{p}\dd^{n-1}\tilde{y}
\end{align*}
With the first step follows:
\begin{align*}
\int_{\partial\Omega}\abs u^{p}\dd\mu_{\partial\Omega} & \le C\left(\Omega\right)\sum_{i=1}^{N}\int_{\Gamma_{i}}\left(\abs u^{p}+\abs{\partial^{n}u}^{p}\right)\dd^{n-1}\tilde{x}\le\\
 & \le\tilde{C}\left(\Omega\right)\int_{\Omega}\left(\abs u^{p}+\abs{\partial^{n}u}^{p}\right)\dd^{n}x
\end{align*}
This gives for all $u\in C^{1}\left(\overline{\Omega}\right)$:
\begin{align*}
\norm{u|_{\partial\Omega}}_{L^{p}\left(\partial\Omega\right)} & \le\underbrace{\left(\tilde{C}\left(\Omega\right)\right)^{\frac{1}{p}}}_{=:K}\norm u_{H^{1,p}}
\end{align*}

\item Now we use an approximation argument for $u\in H^{1,p}\left(\Omega\right)$.
Since $C^{1}\left(\overline{\Omega}\right)$ is dense in $H^{1,p}\left(\Omega\right)$,
we can take a sequence $u_{n}\in C^{1}\left(\overline{\Omega}\right)$
with $u_{n}\to u$ converging in $H^{1,p}\left(\Omega\right)$. Thus
holds:
\begin{align*}
\norm{T\left(u_{n}-u_{m}\right)}_{L^{p}\left(\partial\Omega\right)} & =\norm{u_{n}\big|_{\partial\Omega}-u_{m}\big|_{\partial\Omega}}_{L^{p}\left(\partial\Omega\right)}\le K\norm{u_{n}-u_{m}}_{H^{1,p}\left(\Omega\right)}\xrightarrow{n,m\to\infty}0
\end{align*}
Thus $Tu_{n}$ converges in $L^{p}\left(\partial\Omega\right)$. So
we define:
\begin{align*}
Tu & :=\lim_{n\to\infty}Tu_{n}
\end{align*}
This is determined uniquely by the demand that $T$ be continuous.
Then the inequality also holds in the limit:
\begin{align*}
\norm{Tu}_{L^{p}\left(\partial\Omega\right)}\leftarrow\norm{Tu_{n}}_{L^{p}\left(\partial\Omega\right)} & \le K\norm{u_{n}}_{H^{1,p}\left(\Omega\right)}\to K\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
\qqed
\end{enumerate}

\subsection{Theorem}

Let $\Omega\subseteq\mathbb{R}^{n}$ be a bounded domain, $1\le p<\infty$.
Then for every $u\in H^{1,p}\left(\Omega\right)$ holds $u\in H_{0}^{1,p}\left(\Omega\right)$
if and only if $T\left(u\right)=0$.


\subsubsection*{Proof}

``$\Rightarrow$'': For $u\in H_{0}^{1,p}\left(\Omega\right)=\overline{C_{0}^{\infty}\left(\Omega\right)}^{H^{1,p}\left(\Omega\right)}$
exists a sequence $u_{n}\in C_{0}^{\infty}\left(\Omega\right)$ with
$u_{n}\to u$ in $H^{1,p}$ and $Tu_{n}=0$. So we get:
\begin{align*}
0 & =Tu_{n}\to Tu
\end{align*}
''$\Leftarrow$'': The proof is quite technical and can be found
in the book of Evans.\qqed


\section{The Gagliando-Nirenberg Inequalities}

Question: Consider $u\in C_{0}^{1}\left(\mathbb{R}^{n}\right)$. For
which $p$ and $q$ does the inequality
\begin{align*}
\norm u_{L^{q}\left(\mathbb{R}^{n}\right)} & \le c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}\qquad\fall_{u\in C_{0}^{1}\left(\mathbb{R}^{n}\right)}
\end{align*}
hold? This inequality does not holds for $u\in C^{1}\left(\mathbb{R}^{n}\right)$,
because $u$ can be an arbitrarily large constant with vanishing derivative.

We consider the scaling behavior, i.e. the behavior under the substitution
$x\leadsto\lambda x$ with $\lambda\in\mathbb{R}_{>0}$.
\begin{align*}
u_{\lambda}\left(x\right) & :=u\left(\lambda x\right) & \DD u_{\lambda}\left(x\right) & =\lambda\DD u\big|_{\lambda x}
\end{align*}
\begin{align*}
\norm{\DD u_{\lambda}}_{L^{p}\left(\mathbb{R}^{n}\right)} & =\left(\int_{\mathbb{R}^{n}}\norm{\DD u_{\lambda}}^{p}\dd^{n}x\right)^{\frac{1}{p}}=\left(\int_{\mathbb{R}^{n}}\norm{\lambda\DD u\big|_{\lambda x}}^{p}\dd^{n}x\right)^{\frac{1}{p}}=\lambda\left(\int_{\mathbb{R}^{n}}\norm{\DD u\big|_{\lambda x}}^{p}\dd^{n}x\right)^{\frac{1}{p}}=\\
 & \sr ={y:=\lambda x}{\dd^{n}y=\lambda^{n}\dd^{n}x}\lambda\left(\int_{\mathbb{R}^{n}}\norm{\DD u\left(y\right)}^{p}\frac{\dd^{n}y}{\lambda^{n}}\right)^{\frac{1}{p}}=\lambda^{1-\frac{n}{p}}\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
\begin{align*}
\norm{u_{\lambda}}_{L^{q}\left(\mathbb{R}^{n}\right)} & =\left(\int\abs{u\left(\lambda x\right)}^{q}\dd^{n}x\right)^{\frac{1}{q}}\sr ={y=\lambda x}{}\left(\int\abs{u\left(x\right)}^{q}\frac{\dd^{n}y}{\lambda^{n}}\right)^{\frac{1}{q}}=\lambda^{-\frac{n}{q}}\norm u_{L^{q}\left(\mathbb{R}^{n}\right)}
\end{align*}
The inequality can only be true, if both scalings are the same:
\begin{align*}
1-\frac{n}{p} & \sr =!{}-\frac{n}{q}\\
\frac{1}{q} & =\frac{1}{p}-\frac{1}{n}\\
q & =\frac{np}{n-p}
\end{align*}
$q$ is called the \emph{Sobolev conjugate}, usually denoted by $p^{*}$.


\subsection{Theorem \textmd{(Gagliando-Nirenberg-Sobolev Inequality)\label{sub:Thm-Gagliando-Nirenberg}}}

For $1\le p<n$ there is a constant $c\in\mathbb{R}_{>0}$ such that
holds:
\begin{align*}
\norm u_{L^{p^{*}}\left(\mathbb{R}^{n}\right)} & \le c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}\qquad\fall_{u\in C_{0}^{1}\left(\mathbb{R}^{n}\right)}
\end{align*}
One can choose:
\begin{align*}
c & =\frac{p\left(n-1\right)}{n-p}
\end{align*}



\subsection{Lemma \textmd{(Generalized Hölder Inequalities)}}

Consider $1\le p_{1},\ldots,p_{m}<\infty$ with:
\begin{align*}
\frac{1}{p_{1}}+\frac{1}{p_{2}}+\ldots+\frac{1}{p_{n}} & =1
\end{align*}
Then for any $u_{i}\in L^{p_{i}}\left(\Omega\right)$ holds:
\begin{align*}
\int_{\Omega}\abs{u_{1}\cdots u_{m}}\dd^{n}x & \le\norm{u_{1}}_{L^{p_{1}}\left(\Omega\right)}\cdots\norm{u_{m}}_{L^{p_{m}}\left(\Omega\right)}
\end{align*}



\subsubsection*{Proof}

We proceed inductively: $\frac{1}{p_{1}}+\frac{1}{q_{1}}=1$
\begin{align*}
\int\abs{u_{1}}\cdot\abs{u_{2}\cdots u_{m}}\dd^{n}x & \le\norm u_{p_{1}}\cdot\left(\int\abs{u_{2}\cdots u_{m}}^{q_{1}}\dd^{n}x\right)^{\frac{1}{q_{1}}}\le\\
 & \le\norm u_{p_{1}}\cdot\left(\int\abs{u_{2}}^{q_{1}}\abs{u_{3}\cdots u_{m}}^{q_{1}}\dd^{n}x\right)^{\frac{1}{q_{1}}}
\end{align*}
Applying Hölder's inequality with exponents $r,s$ with $\frac{1}{r}+\frac{1}{s}=1$
such that $rq_{1}=p_{2}$ gives:
\begin{align*}
\int\abs{u_{2}}^{q_{1}}\abs{u_{3}\cdots u_{m}}^{q_{1}}\dd^{n}x & \le\underbrace{\norm{u_{2}^{q_{1}}}_{r}}_{=\norm{u_{2}}_{rq_{1}}^{q_{1}}}\cdot\left(\int\abs{u_{3}}^{q_{1}s}\abs{u_{4}\cdots u_{m}}^{q_{1}s}\dd^{n}x\right)^{\frac{1}{s}}\\
\int\abs{u_{1}}\cdot\abs{u_{2}\cdots u_{m}}\dd^{n}x & \le\norm u_{p_{1}}\cdot\norm{u_{2}}_{p_{2}}\left(\int\abs{u_{3}}^{q_{1}s}\abs{u_{4}\cdots u_{m}}^{q_{1}s}\dd^{n}x\right)^{\frac{1}{q_{1}s}}
\end{align*}
Iteratively this gives the the generalized Hölder Inequality.\qqed

%DATE: Fr 7.6.13


\subsubsection*{Proof of Theorem \ref{sub:Thm-Gagliando-Nirenberg}}

We need to find a constant $c\in\mathbb{R}_{>0}$ such that for $u\in C_{0}^{1}\left(\mathbb{R}^{n}\right)$
holds:
\begin{align*}
\norm u_{L^{p^{*}}\left(\mathbb{R}^{n}\right)} & \le c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}

\begin{enumerate}
\item Consider the case $p=1$, i.e. $p^{*}=\frac{n\cdot1}{n-1}$. We have
to show:
\begin{align*}
\norm u_{L^{\frac{n}{n-1}}} & \le\norm{\DD u}_{L^{1}}
\end{align*}
For any $i\in\left\{ 1,\ldots,n\right\} $ the fundamental theorem
of calculus yields:
\begin{align*}
u\left(x\right) & =\int_{-\infty}^{x_{i}}\frac{\partial}{\partial x_{i}}u\left(x_{1},\ldots,x_{i-1},y_{i},x_{i+1}\ldots,x_{n}\right)\dd y_{i}\\
\abs{u\left(x\right)} & \le\int_{-\infty}^{\infty}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\dd y_{i}\\
\abs{u\left(x\right)}^{\frac{n}{n-1}} & \le\prod_{i=1}^{n}\left(\int_{-\infty}^{\infty}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\dd y_{i}\right)^{\frac{1}{n-1}}
\end{align*}
We proceed inductively. First we integrate over $x_{1}$:
\begin{align*}
\int_{-\infty}^{\infty}\abs{u\left(x\right)}^{\frac{n}{n-1}}\dd x_{1} & \le\int_{-\infty}^{\infty}\prod_{i=1}^{n}\left(\int_{-\infty}^{\infty}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\dd y_{i}\right)^{\frac{1}{n-1}}\dd x_{1}=\\
 & =\underbrace{\left(\int_{-\infty}^{\infty}\norm{\DD u\left(y_{1},x_{2},\ldots,x_{n}\right)}\dd y_{i}\right)^{\frac{1}{n-1}}}_{\text{independent of }x_{1}}\cdot\\
 & \qquad\cdot\int_{-\infty}^{\infty}\prod_{i=2}^{n}\left(\int_{-\infty}^{\infty}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\dd y_{i}\right)^{\frac{1}{n-1}}\dd x_{1}
\end{align*}
The generalized Hölder's inequality for the $n-1$ exponents
\begin{align*}
p_{2}=\ldots=p_{n} & =n-1\\
\Rightarrow\qquad\frac{1}{p_{2}}+\ldots+\frac{1}{p_{n}} & =1
\end{align*}
gives:
\begin{align*}
\int_{-\infty}^{\infty}\abs{u\left(x\right)}^{\frac{n}{n-1}}\dd x_{1} & \le\left(\int_{-\infty}^{\infty}\norm{\DD u\left(y_{1},x_{2},\ldots,x_{n}\right)}\dd y_{i}\right)^{\frac{1}{n-1}}\cdot\\
 & \qquad\cdot\prod_{i=2}^{n}\left(\int_{-\infty}^{\infty}\dd x_{1}\int_{-\infty}^{\infty}\dd y_{i}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\right)^{\frac{1}{n-1}}
\end{align*}
Next, we integrate over $x_{2}$ (and don't write the boundary of
the integral any more):
\begin{align*}
\int\abs{u\left(x\right)}^{\frac{n}{n-1}}\dd x_{1}\dd x_{2} & \le\underbrace{\left(\int\dd x_{1}\int\dd y_{i}\norm{\DD u\left(x_{1},y_{2},x_{3},\ldots,x_{n}\right)}\right)^{\frac{1}{n-1}}}_{\text{independent of }x_{2}}\cdot\\
 & \qquad\cdot\int\dd x_{2}\left(\int\dd y_{i}\norm{\DD u\left(y_{1},x_{2},\ldots,x_{n}\right)}\right)^{\frac{1}{n-1}}\cdot\\
 & \qquad\cdot\prod_{i=3}^{n}\left(\int\dd x_{1}\int\dd y_{i}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\right)
\end{align*}
We again have $n-1$ factors, so we can apply the generalized Hölder
inequality with exponents $n-1$ to get:
\begin{align*}
\int\abs{u\left(x\right)}^{\frac{n}{n-1}}\dd x_{1}\dd x_{2} & \le\left(\int\dd x_{1}\int\dd y_{i}\norm{\DD u\left(x_{1},y_{2},x_{3},\ldots,x_{n}\right)}\right)^{\frac{1}{n-1}}\cdot\\
 & \qquad\cdot\left(\int\dd x_{2}\int\dd y_{i}\norm{\DD u\left(y_{1},x_{2},\ldots,x_{n}\right)}\right)^{\frac{1}{n-1}}\cdot\\
 & \qquad\cdot\prod_{i=3}^{n}\left(\int\dd x_{2}\int\dd x_{1}\int\dd y_{i}\norm{\DD u\left(x_{1},\ldots,y_{i},\ldots,x_{n}\right)}\right)^{\frac{1}{n-1}}
\end{align*}
After also integrating over $x_{3},\ldots,x_{n}$ and then renaming
$y_{i}\to x_{i}$ we get:
\begin{align*}
\int_{\mathbb{R}^{n}}\abs{u\left(x\right)}^{\frac{n}{n-1}}\dd^{n}x & \le\prod_{i=1}^{n}\left(\int_{\mathbb{R}^{n}}\dd^{n}x\norm{\DD u\left(x\right)}\right)^{\frac{1}{n-1}}=\left(\int_{\mathbb{R}^{n}}\dd^{n}x\norm{\DD u\left(x\right)}\right)^{\frac{n}{n-1}}
\end{align*}
\begin{align*}
\norm u_{L^{\frac{n}{n-1}}}=\left(\int_{\mathbb{R}^{n}}\abs{u\left(x\right)}^{\frac{n}{n-1}}\dd^{n}x\right)^{\frac{n-1}{n}} & \le\int_{\mathbb{R}^{n}}\dd^{n}x\norm{\DD u\left(x\right)}=\norm{\DD u}_{L^{1}}
\end{align*}

\item The general case $1<p<n$ follows from the first case by using Hölder's
inequality:\\
We have to prove for $p^{*}=\frac{np}{n-p}$:
\begin{align*}
\norm u_{L^{p^{*}}\left(\mathbb{R}^{n}\right)}=\left(\int_{\mathbb{R}^{n}}\abs{u\left(x\right)}^{p^{*}}\dd x\right)^{\frac{1}{p^{*}}} & \le c\left(\int_{\mathbb{R}^{n}}\abs{\DD u\left(x\right)}^{p}\dd x\right)^{\frac{1}{p}}=c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
Now introduce $v:=\abs u^{c}$ such that $\abs u^{p^{*}}=v^{\frac{n}{n-1}}$:
\begin{align*}
c\frac{n}{n-1} & =p^{*}=\frac{np}{n-p}\\
\Rightarrow\qquad c & =\frac{\left(n-1\right)p}{n-p}
\end{align*}
Now follows:
\begin{align*}
\left(\int\abs u^{p^{*}}\dd^{n}x\right)^{\frac{n-1}{n}} & =\left(\int v^{\frac{n}{n-1}}\dd^{n}x\right)^{\frac{n-1}{n}}=\norm v_{L^{\frac{n}{n-1}}}\le\\
 & \sr{\le}{\text{1.}}{}\int\abs{\DD v}\dd^{n}x\le c\int\abs u^{c-1}\abs{\DD u}\dd^{n}x
\end{align*}
Applying Hölder's inequality with $\tilde{q}$ and $\tilde{p}$ gives:
\begin{align*}
\left(\int\abs u^{p^{*}}\dd^{n}x\right)^{\frac{n-1}{n}} & \le c\left(\int\abs u^{\tilde{q}\left(c-1\right)}\dd^{n}x\right)^{\frac{1}{\tilde{q}}}\left(\int\abs{\DD u}^{\tilde{p}}\dd^{n}x\right)^{\frac{1}{\tilde{p}}}
\end{align*}
We choose $\tilde{q}$ such that $\tilde{q}\left(c-1\right)=p^{*}$
holds.
\begin{align*}
\tilde{q} & =\frac{p^{*}}{c-1}=\frac{\frac{np}{n-p}}{\frac{\left(n-1\right)p}{n-p}-1}=\frac{np}{\left(n-1\right)p-\left(n-p\right)}=\frac{np}{np-n}=\frac{p}{p-1}\\
\tilde{p} & =\frac{1}{1-\frac{1}{\tilde{q}}}=\frac{1}{1-\frac{p-1}{p}}=\frac{p}{p-\left(p-1\right)}=p
\end{align*}
We get:
\begin{align*}
\left(\int\abs u^{p^{*}}\dd^{n}x\right)^{\frac{n-1}{n}} & \le c\left(\int\abs u^{p^{*}}\dd^{n}x\right)^{\frac{p-1}{p}}\left(\int\abs{\DD u}^{p}\dd^{n}x\right)^{\frac{1}{p}}\\
\left(\int\abs u^{p^{*}}\dd^{n}x\right)^{\frac{n-1}{n}-\frac{p-1}{p}} & \le c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
\begin{align*}
\frac{n-1}{n}-\frac{p-1}{p} & =\frac{p\left(n-1\right)-n\left(p-1\right)}{np}=\frac{np-p-np+n}{np}=\frac{n-p}{np}=\frac{1}{p^{*}}
\end{align*}
Finally follows:
\begin{align*}
\norm u_{L^{p^{*}}\left(\mathbb{R}^{n}\right)}=\left(\int\abs u^{p^{*}}\dd^{n}x\right)^{\frac{1}{p^{*}}} & \le c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
\qqed[\ref{sub:Thm-Gagliando-Nirenberg}]
\end{enumerate}

\subsection{Theorem\label{sub:Thm-Proof-with-extension}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be an open, bounded domain with
$C^{1}$-boundary. Then there is a constant $C\in\mathbb{R}_{>0}$
such that for all $u\in H^{1,p}\left(\Omega\right)$ holds:
\begin{align*}
\norm u_{L^{p^{*}}\left(\Omega\right)} & \le C\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
Recall:
\begin{align*}
\norm u_{H^{1,p}\left(\Omega\right)} & =\left(\norm u_{L^{p}\left(\Omega\right)}^{p}+\norm{\DD u}_{L^{p}\left(\Omega\right)}^{p}\right)^{\frac{1}{p}}
\end{align*}



\subsubsection*{Proof}

Consider $u\in H^{1,p}\left(\Omega\right)$ and choose $\Omega'$
with $\Omega\Subset\Omega'$. Then $u$ can be extended to a function
$\overline{u}\in H_{0}^{1,p}\left(\Omega'\right)$ with:
\begin{align*}
\norm{\overline{u}}_{H^{1,p}\left(\Omega'\right)} & \le\tilde{c}\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
Namely choose $u_{n}\in C^{1}\left(\Omega\right)$ with $u_{n}\to u$
in $H^{1,p}\left(\Omega\right)$ and extend $u_{n}$ to $C_{0}^{1}\left(\Omega'\right)$
to obtain functions $\overline{u}_{n}$ with:
\begin{align*}
\norm{\overline{u}_{n}}_{H^{1,p}\left(\Omega'\right)} & \le\tilde{c}\norm{u_{n}}_{H^{1,p}\left(\Omega\right)}
\end{align*}
A subsequence of the $\overline{u}_{n}$ converges to $\overline{u}$
in $H_{0}^{1,p}\left(\Omega'\right)$. The constant $\tilde{c}$ can
be chosen independently of $u$, because one can construct a linear
mapping
\begin{align*}
A:C^{1}\left(\Omega\right) & \to C_{0}^{1}\left(\Omega'\right)
\end{align*}
with:
\begin{align*}
\norm{Au}_{H^{1,p}\left(\Omega'\right)} & \le c\norm u_{H^{1,p}\left(\Omega\right)}\qquad\fall_{u\in C^{1}\left(\Omega\right)}
\end{align*}
Extending $\overline{u}$ by zero to $\mathbb{R}^{n}$ we get:
\begin{align*}
\norm{\overline{u}}_{H^{1,p}\left(\mathbb{R}^{n}\right)} & \le\tilde{c}\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
Now follows:
\begin{align*}
\norm u_{L^{p^{*}}\left(\Omega\right)} & \le\norm{\overline{u}}_{L^{p^{*}}\left(\mathbb{R}^{n}\right)}\sr{\le}{\ref{sub:Thm-Gagliando-Nirenberg}}{}c\norm{\DD\overline{u}}_{L^{p}\left(\mathbb{R}^{n}\right)}\le\\
 & \le c\norm{\overline{u}}_{H^{1,p}\left(\mathbb{R}^{n}\right)}\le\underbrace{c\tilde{c}}_{=:C}\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
\qqed


\subsection{Theorem \textmd{(Poincaré's inequality)}}

Let $\Omega$ be a bounded open domain, $1\le p<\infty$ and $1\le q\le p^{*}$.
Then there is a $C\left(\Omega\right)\in\mathbb{R}$ such that for
all $u\in H_{0}^{1,p}\left(\Omega\right)$ holds:
\begin{align*}
\norm u_{L^{q}\left(\Omega\right)} & \le C\left(\Omega\right)\norm{\DD u}_{L^{p}\left(\Omega\right)}
\end{align*}



\subsubsection{Proof}

For $u\in H_{0}^{1,p}\left(\Omega\right)$ there is by definition
a sequence $u_{k}\in C_{0}^{1}\left(\Omega\right)$ with $u_{k}\to u$
converging in $H_{0}^{1,p}\left(\Omega\right)$. We extend $u_{n}$
by zero to $\mathbb{R}^{n}$ and apply the Gagliando-Nirenberg inequality:
\begin{align*}
\norm{u_{k}}_{L^{p^{*}}\left(\Omega\right)} & =\norm{u_{k}}_{L^{p^{*}}\left(\mathbb{R}^{n}\right)}\le c\norm{\DD u_{k}}_{L^{p}\left(\mathbb{R}^{n}\right)}=c\norm{\DD u_{k}}_{L^{p}\left(\Omega\right)}\xrightarrow{k\to\infty}c\norm{\DD u}_{L^{p}\left(\Omega\right)}
\end{align*}
For the left sides holds:
\begin{align*}
\norm{u_{k}-u_{l}}_{L^{p^{*}}\left(\Omega\right)} & \le c\norm{\DD\left(u_{k}-u_{l}\right)}_{L^{p}\left(\mathbb{R}^{n}\right)}\le c\norm{u_{k}-u_{l}}_{H^{1,p}\left(\Omega\right)}\xrightarrow{k,l\to\infty}0
\end{align*}
Therefore $u_{k}\to u$ converges in $L^{p^{*}}$. In the limit we
get:
\begin{align*}
\norm u_{L^{p^{*}}\left(\Omega\right)} & \le c\norm{\DD u}_{L^{p}\left(\Omega\right)}
\end{align*}
Now we have:
\begin{align*}
\norm u_{L^{q}\left(\Omega\right)} & =\left(\int1\cdot\abs u^{q}\dd^{n}x\right)^{\frac{1}{q}}
\end{align*}
Finally, we choose Hölder exponents $r,s$ with
\begin{align*}
sq & =p^{*}\\
s & =\frac{p^{*}}{q}\ge1
\end{align*}
to get:
\begin{align*}
\norm u_{L^{q}\left(\Omega\right)} & \le\left(\mu\left(\Omega\right)\right)^{\frac{1}{r}}\norm u_{L^{p^{*}}\left(\Omega\right)}\le\underbrace{c\left(\mu\left(\Omega\right)\right)^{\frac{1}{r}}}_{=:C\left(\Omega\right)}\norm{\DD u}_{L^{p}\left(\Omega\right)}
\end{align*}
\qqed


\subsection{Corollary}

Let $\Omega\subseteq\mathbb{R}^{n}$ be a bounded domain and $1\le p<n$.
Then $\norm{\DD u}_{L^{p}\left(\Omega\right)}$ is a norm on $H_{0}^{1,p}\left(\Omega\right)$
which is equivalent to the $H^{1,p}$-norm.


\subsubsection*{Proof}

The $H^{1,p}$-norm
\begin{align*}
\norm u_{H^{1,p}} & =\left(\norm u_{L^{p}}^{p}+\norm{\DD u}_{L^{p}}^{p}\right)^{\frac{1}{p}}
\end{align*}
is equivalent to:
\begin{align*}
\norm u_{L^{p}\left(\Omega\right)}+\norm{\DD u}_{L^{p}\left(\Omega\right)}
\end{align*}
It holds:
\begin{align*}
\norm u_{L^{p^{*}}\left(\mathbb{R}^{n}\right)} & \le c\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
\begin{align*}
\norm{\DD u}_{L^{p}\left(\Omega\right)} & \le\norm u_{L^{p}\left(\Omega\right)}+\norm{\DD u}_{L^{p}\left(\Omega\right)}\sr{\le}{p<p^{*}}{}\tilde{c}\underbrace{\norm u_{L^{p^{*}}\left(\Omega\right)}}_{\le c\norm{\DD u}_{L^{p}\left(\Omega\right)}}+\norm{\DD u}_{L^{p}\left(\Omega\right)}\le\left(1+\tilde{c}c\right)\norm{\DD u}_{L^{p}\left(\Omega\right)}
\end{align*}
\qqed

%DATE: Mi 12.6.13


\section{The Morrey Inequalities}

This traces back to Charles Morrey (1907-1984).


\subsection{Theorem \textmd{(Morrey Inequalities)\label{sub:Thm-Morrey-Inequalities}}}

For $n<p\le\infty$ exists a constant $C\in\mathbb{R}_{>0}$ (depending
only on $n$ and $p$), such that for $u\in C^{1}\left(\mathbb{R}^{n}\right)\cap L^{p}\left(\mathbb{R}^{n}\right)$
and $\gamma:=1-\frac{n}{p}$ holds:
\begin{align*}
\norm u_{C^{0,\gamma}\left(\mathbb{R}^{n}\right)} & \le C\norm u_{H^{1,p}\left(\mathbb{R}^{n}\right)}
\end{align*}
The exponent $\gamma$ can be found with a scaling argument.


\subsubsection*{Proof}

We define:
\begin{align*}
\fint_{U}f\left(y\right)\dd^{n}y & :=\frac{1}{\mu\left(U\right)}\int_{U}f\left(y\right)\dd^{n}y
\end{align*}

\begin{enumerate}
\item  

\begin{description}
\item [{Claim:}] 
\begin{align*}
\fint_{B_{r}\left(x\right)}\abs{u\left(x\right)-u\left(y\right)}\dd^{n}y & \le C_{1}\left(n\right)\cdot\int\frac{\norm{\DD u\left(y\right)}}{\norm{x-y}^{n-1}}\dd^{n}y
\end{align*}

\item [{Proof:}] Choose $B_{r}\left(x\right)$, $s\in\left(0,r\right)$
and a unit vector $w\in\partial B_{1}\left(0\right)$.\\
\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw (0,0) +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (0,0) circle (2) circle (3.5);
  \draw[->] (0,0) -- node[below]{$s$} ({2*cos(160)},{2*sin(160)}) node[left]{$x+sw$};
  \draw (0,0) -- node[left]{$r$} ({3.5*cos(60)},{3.5*sin(60)});
\end{tikzpicture}\caption{$x+sw\in B_{r}\left(x\right)$}
\end{figure}
\begin{align*}
\abs{u\left(x+sw\right)-u\left(x\right)} & =\abs{\int_{0}^{s}\frac{\dd}{\dd t}u\left(x+tw\right)\dd t}\le\int_{0}^{s}\norm{\DD u\left(x+tw\right)}\cdot\underbrace{\norm w}_{=1}\dd t
\end{align*}
Integration gives:
\begin{align*}
 & \int_{\partial B_{1}\left(0\right)}\abs{u\left(x+sw\right)-u\left(x\right)}\dd\mu_{\partial B_{1}\left(0\right)}\left(w\right)\le\\
 & \qquad\qquad\qquad\qquad\le\int_{0}^{s}\dd t\int_{\partial B_{1}\left(0\right)}\norm{\DD u\left(x+tw\right)}\dd\mu_{\partial B_{1}\left(0\right)}\left(w\right)=\\
 & \qquad\qquad\qquad\qquad\sr ={z:=x+tw}{}\int_{0}^{s}\frac{\dd t}{t^{n-1}}\int_{\partial B_{t}\left(x\right)}\norm{\DD u\left(z\right)}\dd\mu_{\partial B_{t}\left(x\right)}\left(z\right)=\\
 & \qquad\qquad\qquad\qquad\sr ={t=\norm{x-z}}{}\int_{0}^{s}\dd t\int_{\partial B_{t}\left(x\right)}\frac{\norm{\DD u\left(z\right)}}{\norm{x-z}^{n-1}}\dd\mu_{\partial B_{t}\left(x\right)}\left(z\right)=\\
 & \qquad\qquad\qquad\qquad\sr ={\text{Fubini}}{}\int_{B_{s}\left(x\right)}\frac{\norm{\DD u\left(y\right)}}{\norm{x-y}^{n-1}}\dd\mu_{B_{s}\left(x\right)}\left(y\right)\le\int_{B_{r}\left(x\right)}\frac{\norm{\DD u\left(y\right)}}{\norm{x-y}^{n-1}}\dd^{n}y
\end{align*}
Multiplying by $s^{n-1}$ and integrating over $s$ gives:
\begin{align*}
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\int_{0}^{r}\dd s\cdot s^{n-1}\int_{\partial B_{1}\left(0\right)}\abs{u\left(x+sw\right)-u\left(x\right)}\dd\mu_{\partial B_{1}\left(0\right)}\left(w\right) & \le\int_{0}^{r}\dd s\cdot s^{n-1}\int_{B_{r}\left(x\right)}\frac{\norm{\DD u\left(y\right)}}{\norm{x-y}^{n-1}}\dd^{n}y\\
\negthickspace\negthickspace\negthickspace\negthickspace\negthickspace\sr{\Rightarrow}{\text{Fubini}}{}\qquad\qquad\int_{\partial B_{r}\left(x\right)}\abs{u\left(y\right)-u\left(x\right)}\dd\mu_{B_{r}\left(x\right)}\left(y\right) & \le\frac{r^{n}}{n}\int_{B_{r}\left(x\right)}\frac{\norm{\DD u\left(y\right)}}{\norm{x-y}^{n-1}}\dd^{n}y
\end{align*}
Dividing by $r^{n}$ gives the claim.\qqed[\arabic{enumi}.]
\end{description}
\item Now estimate $u$ point-wise:
\begin{align*}
\abs{u\left(x\right)} & =\abs{u\left(x\right)}\underbrace{\fint_{B_{1}\left(x\right)}\dd^{n}y}_{=1}\le\fint_{B_{1}\left(x\right)}\abs{u\left(x\right)-u\left(y\right)}\dd^{n}y+\fint_{B_{1}\left(x\right)}\abs{u\left(y\right)}\dd^{n}y\le\\
 & \sr{\le}{\text{1.}}{}C_{1}\cdot\int\frac{\norm{\DD u\left(y\right)}}{\norm{x-y}^{n-1}}\dd^{n}y+\frac{1}{\omega_{n}}\int_{B_{1}\left(x\right)}\abs{u\left(y\right)}\dd^{n}y\le\\
 & \sr{\le}{\text{Hölder}}{p^{-1}+q^{-1}=1}C_{1}\norm{\DD u}_{L^{q}\left(B_{1}\left(x\right)\right)}\left(\int_{B_{1}\left(x\right)}\left(\frac{1}{\norm{x-y}^{n-1}}\right)^{q}\dd^{n}y\right)^{\frac{1}{q}}+\\
 & \qquad\qquad\quad+\norm u_{L^{p}\left(B_{1}\left(x\right)\right)}\underbrace{\frac{1}{\omega_{n}}\left(\int_{B_{1}\left(x\right)}1^{q}\dd^{n}y\right)^{\frac{1}{q}}}_{=:C_{2}}\le\\
 & \le C_{1}\norm{\DD u}_{L^{q}\left(\mathbb{R}^{n}\right)}\left(\int_{B_{1}\left(x\right)}\left(\frac{1}{\norm{x-y}^{n-1}}\right)^{q}\dd^{n}y\right)^{\frac{1}{q}}+C_{2}\norm u_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
Is the pole integrable? The order of the pole is:
\begin{align*}
\left(n-1\right)q=\left(n-1\right)\frac{p}{p-1} & \sr <?{}n\\
\left(n-1\right)p & \sr <?{}n\left(p-1\right)\\
-p & \sr <?{}-n\\
n & <p
\end{align*}
This holds by assumption, thus the pole is integrable and we get:
\begin{align*}
\abs{\int_{B_{1}\left(x\right)}\frac{1}{\norm{x-y}^{\left(n-1\right)q}}} & \le C_{3}<\infty
\end{align*}
Thus holds:
\begin{align*}
\abs{u\left(x\right)} & \le C_{1}C_{3}\norm{\DD u}_{L^{q}\left(B_{1}\left(x\right)\right)}+C_{2}\norm u_{L^{p}\left(\mathbb{R}^{n}\right)}\le C_{4}\norm u_{H^{1,p}\left(\mathbb{R}^{n}\right)}
\end{align*}

\item Now estimate $\abs{u\left(x\right)-u\left(y\right)}$. For $x,y\in\mathbb{R}^{n}$
define $r:=\norm{x-y}$ and $W:=B_{r}\left(x\right)\cap B_{r}\left(y\right)$.\\
\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}[scale=0.7]
  \draw (-2,0) node[left]{$x$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \draw (2,0)  node[right]{$y$} +(0.1,0.1) -- +(-0.1,-0.1) +(0.1,-0.1) -- +(-0.1,0.1);
  \node at (0,3) {$W$};
  \draw (-2,0) circle (4) (2,0) circle (4);
  \path[even odd rule] (-4,-4) rectangle (4,4) (-2,0) circle (4) (2,0) circle (4)
  [path picture={
    \foreach \x in {-2,-1,...,3}
      \draw (-2,\x) -- (2,{\x -0.9});
  }];
\end{tikzpicture}\caption{Intersection of spheres}
\end{figure}
Thus we get:
\begin{align*}
\abs{u\left(x\right)-u\left(y\right)} & =\fint_{W}\abs{u\left(x\right)-u\left(y\right)}\dd^{n}z\le\fint_{W}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z+\fint_{W}\abs{u\left(z\right)-u\left(y\right)}\dd^{n}z
\end{align*}
\begin{align}
\fint_{W}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z & \le\frac{1}{\mu\left(W\right)}\int_{B_{r}\left(x\right)}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z=C_{5}\left(n\right)\fint_{B_{r}\left(x\right)}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z\label{eq:integrals-u}
\end{align}
The constant
\begin{align*}
C_{5}\left(n\right) & =\frac{\mu\left(B_{r}\right)}{\mu\left(W\right)}
\end{align*}
is independent of $r$, due to the proportionalities $\mu\left(B_{r}\right)\sim r^{n}\sim\mu\left(W\right)$.
With 1. follows:
\begin{align*}
\fint_{W}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z & \le C_{5}\left(n\right)\fint_{B_{r}\left(x\right)}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z\le\\
 & \le C_{5}\left(n\right)C_{1}\left(n\right)\cdot\int_{B_{r}\left(x\right)}\frac{\norm{\DD u\left(z\right)}}{\norm{x-z}^{n-1}}\dd^{n}z\le\\
 & \sr{\le}{\text{Hölder}}{}C_{6}\left(n\right)\norm{\DD u}_{L^{p}\left(B_{r}\left(x\right)\right)}\left(\int_{B_{r}\left(x\right)}\frac{1}{\norm{x-z}^{\left(n-1\right)\frac{p}{p-1}}}\dd^{n}z\right)^{\frac{p-1}{p}}
\end{align*}
\begin{align*}
\int_{B_{r}\left(x\right)}\frac{1}{\norm{x-z}^{\left(n-1\right)\frac{p}{p-1}}}\dd^{n}z & =\int_{0}^{r}\dd s\left(n\omega_{n}\right)s^{n-1}\frac{1}{s^{\left(n-1\right)\frac{p}{p-1}}}=C_{7}\left(n\right)r\cdot r^{n-1}\frac{1}{r^{\left(n-1\right)\frac{p}{p-1}}}=\\
 & =C_{7}\left(n\right)r^{n-\frac{\left(n-1\right)p}{p-1}}=C_{7}\left(n\right)r^{\frac{np-n-np+p}{p-1}}=C_{7}\left(n\right)r^{\frac{p-n}{p-1}}
\end{align*}
Thus follows:
\begin{align*}
\fint_{W}\abs{u\left(x\right)-u\left(z\right)}\dd^{n}z & \le\underbrace{C_{6}\left(n\right)\left(C_{7}\left(n\right)\right)^{\frac{p-1}{p}}}_{=:C_{8}\left(n\right)}\norm{\DD u}_{L^{p}\left(B_{r}\left(x\right)\right)}r^{\frac{p-n}{p-1}\cdot\frac{p-1}{p}}
\end{align*}
Estimating the other integral in (\ref{eq:integrals-u}) in the same
way, we obtain:
\begin{align*}
\abs{u\left(x\right)-u\left(y\right)} & \le\underbrace{2C_{8}\left(n\right)}_{=:C_{9}\left(n\right)}r^{1-\frac{n}{p}}\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}
With $\gamma:=1-\frac{n}{p}$ follows for all $x,y\in\mathbb{R}^{n}$
due to $r=\norm{x-y}$:
\begin{align*}
\frac{\abs{u\left(x\right)-u\left(y\right)}}{\norm{x-y}^{\gamma}} & \le C_{9}\left(n\right)\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}
\end{align*}

\item Combining 2. and 3., we get:
\begin{align*}
\norm u_{C^{0,\gamma}\left(\mathbb{R}^{n}\right)} & =\norm u_{C^{0}\left(\mathbb{R}^{n}\right)}+\sup_{x\not=y\in\mathbb{R}^{n}}\frac{\abs{u\left(x\right)-u\left(y\right)}}{\norm{x-y}^{\gamma}}\le\\
 & \le C_{4}\left(n\right)\norm u_{H^{1,p}\left(\mathbb{R}^{n}\right)}+C_{9}\left(n\right)\norm{\DD u}_{L^{p}\left(\mathbb{R}^{n}\right)}\le C\norm u_{H^{1,p}\left(\mathbb{R}^{n}\right)}
\end{align*}

\end{enumerate}
\qqed

We use a denseness argument to extend the statement to $H^{1,p}$-functions
and $C^{0,\alpha}$-functions. For $u\in H^{1,p}\left(\mathbb{R}^{n}\right)$,
weak derivatives $\DD^{\left(i\right)}u$ exist and are in $L^{p}$.
Note that $u\in L^{p}$ stands for an equivalence class of function
which differ on a set of measure zero. The inequality
\begin{align*}
\abs{u\left(x\right)} & \le c\norm u_{H^{1,p}\left(\mathbb{R}^{n}\right)}
\end{align*}
makes no sense on these classes.


\subsection{Theorem \textmd{(Hölder Continuous Representative)}}

Let $\Omega\subseteq\mathbb{R}^{n}$ be bounded with $C^{1}$-boundary.
Assume $n<p\le\infty$ and $u\in H^{1,p}\left(\Omega\right)$. Then
there is a representative $u^{*}$ of $u$ in $H^{1,p}\left(\Omega\right)$,
which is Hölder continuous $u\in C^{0,\gamma}\left(\overline{\Omega}\right)$
with $\gamma=1-\frac{n}{p}$ and:
\begin{align*}
\norm{u^{*}}_{C^{0,\gamma}} & \le c\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
In what follows, we always choose this representative and omit the
star.


\subsubsection*{Proof}

For $u\in H^{1,p}\left(\Omega\right)$ there are $u_{m}\in C_{0}^{1}\left(\mathbb{R}^{n}\right)$
with $u_{m}\big|_{\Omega}\to u$ converging in $H^{1,p}\left(\Omega\right)$
and
\begin{align*}
\norm{u_{m}}_{H^{1,p}\left(\mathbb{R}^{n}\right)} & \le c\norm{u_{m}}_{H^{1,p}\left(\Omega\right)}
\end{align*}
just as in the proof of Theorem \ref{sub:Thm-Proof-with-extension}.

According to Theorem \ref{sub:Thm-Morrey-Inequalities}, the Morrey
inequalities hold:
\begin{align*}
\norm{u_{m}-u_{l}}_{C^{0,\gamma}\left(\mathbb{R}^{n}\right)} & \le C\norm{u_{m}-u_{l}}_{H^{1,p}\left(\mathbb{R}^{n}\right)}\le cC\norm{u_{m}-u_{l}}_{H^{1,p}\left(\Omega\right)}\xrightarrow{m,l\to\infty}0
\end{align*}
Thus $u_{l}$ is a Cauchy sequence in $C^{0,\gamma}\left(\overline{\Omega}\right)$.
Hence converges $u_{l}\to u^{*}\in C^{0,\gamma}\left(\overline{\Omega}\right)$.
\begin{align*}
\norm{u^{*}}_{C^{0,\gamma}\left(\overline{\Omega}\right)}\xleftarrow{l\to\infty}\norm{u_{l}}_{C^{0,\gamma}\left(\overline{\Omega}\right)} & \le c\norm{u_{l}}_{H^{1,p}\left(\Omega\right)}\xrightarrow{l\to\infty}c\norm u_{H^{1,p}\left(\Omega\right)}
\end{align*}
\qqed


\chapter{Construction of Weak Solutions of Linear Elliptic PDEs}

Let $\Omega\subseteq\mathbb{R}^{n}$ be a open domain with smooth
boundary and $n\in\mathbb{N}_{>2}$. Consider the linear partial differential
operator in divergence form:
\begin{align*}
Lu & =-\frac{\partial}{\partial x^{i}}\left(a^{ij}\left(x\right)\frac{\partial}{\partial x^{j}}u\left(x\right)\right)+c\left(x\right)u\left(x\right)
\end{align*}
Assume that the $a^{ij}$ are uniformly elliptic, i.e. for all $\xi\in\mathbb{R}^{n}$
and all $x\in\Omega$ holds:
\begin{align*}
a^{ij}\left(x\right)\xi_{i}\xi_{j} & \ge c\norm{\xi}^{2}
\end{align*}
Also assume that the coefficients are smooth and uniformly bounded
$\abs{a^{ij}},\abs c\le K$. The first order term is missing!

We consider the Dirichlet problem:
\begin{align*}
Lu & =f\in C^{\infty}\left(\overline{\Omega}\right)\quad\text{in }\Omega\\
u\big|_{\partial\Omega} & =u_{0}\in C^{\infty}\left(\partial\Omega\right)
\end{align*}
Examples:
\begin{itemize}
\item Poisson equation in $\Omega\subseteq\mathbb{R}^{n}$
\item Let $\left(M,g\right)$ be a Riemannian manifold and $\Omega\subseteq M$
be contained in a coordinate chart $\left(x,U\right)$.\\
\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
  \draw plot[smooth cycle] coordinates {(-2,1) (-1,3) (0,3) (0.75,2.25) (3,1) (3.5,0) (3,-1) (-1,-1)};
  \draw[rotate=35] (0,1) circle (0.5 and 1);
  \draw plot[smooth cycle] coordinates {(1,0.5) (1.5,1) (2.5,0) (1,-0.5)};
  \node at (1,1) {$\Omega $};
  \path[even odd rule] plot[smooth cycle] coordinates {(1,0.5) (1.5,1) (2.5,0) (1,-0.5)}   [path picture={     \foreach \x in {0,1,...,2}       \draw (0,{0.5*\x}) -- (3,{0.5*\x -0.9});   }];
  \draw[dashed] (1.5,0.3) circle (1.2);
  \node at (1,1.7) {$U$};
  \draw[thick,decoration={markings,mark=at position 0.9999 with {\arrow[scale=2]{>}};},postaction={decorate}] (4,1) .. controls +(1,1) and +(-1,1) .. node[above]{$x$} (7,1);
  \draw (8,-1) rectangle (12,3) node [above right]{$\mathbb{R}^n$};
  \draw[dashed] (10,1) circle (1.5);
  \node at (9,2.6) {$x(U)$};
  \draw plot[smooth cycle] coordinates {(9,0.5) (11,0.3) (11,1.5) (10,1.8)};
  \node at (9.5,2) {$x(\Omega )$};
\end{tikzpicture}\caption{The chart maps $\Omega$ to $\mathbb{R}^{n}$.}
\end{figure}
The Laplace-Beltrami-Operator is:
\begin{align*}
\LBO u & =\frac{1}{\sqrt{\det\left(g\right)}}\frac{\partial}{\partial x^{i}}\left(\sqrt{\det\left(g\right)}g^{ij}\frac{\partial}{\partial x^{j}}u\right)
\end{align*}
\begin{align*}
\LBO u & =f\qquad\Leftrightarrow\qquad\frac{\partial}{\partial x^{i}}\left(\sqrt{\det\left(g\right)}g^{ij}\frac{\partial}{\partial x^{j}}u\right)=\sqrt{\det\left(g\right)}f=\tilde{f}
\end{align*}
This is in the desired divergence form.
\end{itemize}
%DATE: Fr 14.6.13

Reduction to the case $u_{0}=0$: Let $v\in C^{\infty}\left(\overline{\Omega}\right)$
with $v\big|_{\partial\Omega}=u_{0}$. Set $\tilde{u}:=u-v$.
\begin{align*}
L\tilde{u} & =f-Lv=:\tilde{f}\\
\tilde{u}\big|_{\partial\Omega} & =0
\end{align*}
A solution $\tilde{u}$ of this Dirichlet problem yields a solution
$u=\tilde{u}+v$ of the original problem.

Therefore, from now on we consider the problem:
\begin{align*}
Lu & =f\\
u\big|_{\partial\Omega} & =0
\end{align*}



\section{The Dirichlet Principle}

Define the \emph{action}:
\begin{align*}
S & :=\int\left(a^{ij}\left(x\right)\left(\frac{\partial}{\partial x^{i}}u\left(x\right)\right)\left(\frac{\partial}{\partial x^{j}}u\left(x\right)\right)+c\left(x\right)u^{2}\left(x\right)-2f\left(x\right)u\left(x\right)\right)\dd^{n}x
\end{align*}
Suppose $u$ is a critical point (for example a minimizer), then holds
for $\eta\in C_{0}^{\infty}\left(\Omega\right)$ formally:
\begin{align*}
0 & =\frac{\dd}{\dd\tau}S\left(u+\tau\eta\right)\big|_{\tau=0}=\\
 & =\int_{\Omega}\left(2a^{ij}\left(x\right)\left(\frac{\partial}{\partial x^{i}}u\left(x\right)\right)\left(\frac{\partial}{\partial x^{j}}\eta\left(x\right)\right)+2c\left(x\right)u\left(x\right)\eta\left(x\right)-2f\left(x\right)\eta\left(x\right)\right)\dd^{n}x=\\
 & \sr ={\text{integration}}{\text{by parts}}2\int_{\Omega}\eta\left(x\right)\underbrace{\left(-\frac{\partial}{\partial x^{j}}\left(a^{ij}\left(x\right)\left(\frac{\partial}{\partial x^{i}}u\left(x\right)\right)\right)+c\left(x\right)u\left(x\right)-f\left(x\right)\right)}_{=Lu-f}\dd^{n}x
\end{align*}
This has to hold for any $\eta$ and thus follows $Lu-f=0$.

Assume $u\in H^{1,2}\left(\Omega\right)$ for the integral over the
two partial derivatives in the action being well defined. To ensure
the boundary value $u\big|_{\partial\Omega}=0$ we assume even $u\in H_{0}^{1,2}\left(\Omega\right)$.
Then holds:
\begin{align*}
\int_{\Omega}\underbrace{a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}u\right)}_{\ge0}\dd^{n}x & \le\sup_{\Omega}\left(\abs{a^{ij}}\right)\int_{\Omega}\norm{\DD u}^{2}\dd^{n}x\le\sup_{\Omega}\left(\abs{a^{ij}}\right)\norm u_{H_{0}^{1,2}\left(\Omega\right)}^{2}<\infty\\
\int_{\Omega}\abs{c\left(x\right)u^{2}\left(x\right)}\dd^{n}x & \le\sup_{\Omega}\left(\abs c\right)\int_{\Omega}u^{2}\left(x\right)\dd^{n}x\le\sup_{\Omega}\left(\abs c\right)\norm u_{H_{0}^{1,2}\left(\Omega\right)}^{2}<\infty\\
\int_{\Omega}2\abs{f\left(x\right)u\left(x\right)}\dd^{n}x & \le2\norm f_{L^{2}\left(\Omega\right)}\norm u_{L^{2}\left(\Omega\right)}\le2\norm f_{L^{2}\left(\Omega\right)}\norm u_{H_{0}^{1,2}\left(\Omega\right)}<\infty
\end{align*}
We always assume that $\Omega$ is bounded. Then $f\in C^{\infty}\left(\overline{\Omega}\right)$
implies $f\in L^{2}\left(\Omega\right)$.

Furthermore, we know from the Gagliando-Nirenberg estimate
\begin{align*}
H_{0}^{1,2}\left(\Omega\right) & \hookrightarrow L^{p^{*}}\left(\Omega\right)
\end{align*}
with $p=2$ and:
\begin{align*}
\frac{1}{p^{*}} & =\frac{1}{2}-\frac{1}{n}<\frac{1}{2}\\
p^{*} & =\frac{2n}{n-2}>2
\end{align*}
This is an \emph{imbedding} (\emph{embedding}), i.e. an injective
mapping.

If $\Omega$ has finite volume, which it has in our case, since we
assume that $\Omega$ is bounded, it follows:
\begin{align*}
\norm u_{L^{2}\left(\Omega\right)} & =\left(\int_{\Omega}u^{2}\left(x\right)\dd^{n}x\right)^{\frac{1}{2}}\le\left(\int_{\Omega}u^{2\tilde{p}}\left(x\right)\dd^{n}x\right)^{\frac{1}{2\tilde{p}}}\left(\mu\left(\Omega\right)\right)^{\frac{1}{2\tilde{q}}}\le\tilde{c}\norm u_{L^{2\tilde{p}}\left(\Omega\right)}
\end{align*}
We chose $\tilde{p}$ such that $2\tilde{p}=p^{*}$ and use Poincaré's
inequality:
\begin{align*}
\norm u_{L^{2}\left(\Omega\right)} & \le\tilde{c}\norm u_{L^{p^{*}}\left(\Omega\right)}\le\underbrace{\tilde{c}C}_{=:c}\norm{\DD u}_{L^{2}\left(\Omega\right)}
\end{align*}
Thus the action can be viewed as a mapping:
\begin{align*}
S:H_{0}^{1,2}\left(\Omega\right) & \to\mathbb{R}
\end{align*}



\subsection{Theorem \textmd{(Action Bounded from Below)}}

$S$ is bounded from below, i.e. $S\left(u\right)\ge-K\in\mathbb{R}_{\le0}$
for all $u\in H_{0}^{1,2}\left(\Omega\right)$.


\subsubsection*{Proof}

The uniform ellipticity means:
\begin{align*}
a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}u\right) & \ge\underbrace{\lambda}_{>0}\norm{\DD u}^{2}
\end{align*}
Moreover holds $C\left(x\right)\ge0$. Therefore follows:
\begin{align*}
S\left(u\right) & \ge\int_{\Omega}\lambda\norm{\DD u}^{2}\dd^{n}x-\norm f_{L^{2}\left(\Omega\right)}\cdot\norm u_{L^{2}\left(\Omega\right)}\ge\\
 & \sr{\ge}{\text{Poincaré}}{}\frac{\lambda}{c^{2}}\norm u_{L^{2}\left(\Omega\right)}^{2}-\norm f_{L^{2}\left(\Omega\right)}\norm u_{L^{2}\left(\Omega\right)}=\\
 & =\frac{\lambda}{c^{2}}\left(\norm u_{L^{2}\left(\Omega\right)}-\frac{c^{2}}{2\lambda}\norm f_{L^{2}\left(\Omega\right)}\right)^{2}-\frac{c^{2}}{4\lambda}\norm f_{L^{2}\left(\Omega\right)}^{2}\ge\\
 & \ge-\frac{c^{2}}{4\lambda}\norm f_{L^{2}\left(\Omega\right)}^{2}=:-K
\end{align*}
\qqed

We now choose a \emph{minimizing sequence} (Minimalfolge), i.e. a
sequence $u_{n}\in H_{0}^{1,2}\left(\Omega\right)$ with:
\begin{align*}
S\left(u_{n}\right) & \to\inf_{H_{0}^{1,2}\left(\Omega\right)}\left(S\right)
\end{align*}
Hope:
\begin{itemize}
\item A subsequence of the $u_{n}$ converges (weakly?) in $H_{0}^{1,2}\left(\Omega\right)$
to $u\in H_{0}^{1,2}\left(\Omega\right)$.
\item ${\displaystyle S\left(u\right)=\lim_{n\to\infty}S\left(u_{n}\right)=\inf_{H_{0}^{1,2}\left(\Omega\right)}S}$
\end{itemize}
Such a minimizer satisfies the following Euler-Lagrange equation:


\subsection{Theorem \textmd{(Euler-Lagrange equation)}}

If $u\in H^{1,2}\left(\Omega\right)$ satisfies $S\left(u\right)=\inf_{H_{0}^{1,2}\left(\Omega\right)}\left(S\right)$,
then holds for all $\eta\in C_{0}^{\infty}\left(\Omega\right)$:
\begin{align*}
\int_{\Omega}\left(u\left(-\frac{\partial}{\partial x^{i}}a^{ij}\frac{\partial}{\partial x^{j}}\eta+c\eta\right)-f\eta\right)\dd^{n}x & =0
\end{align*}
Thus $u$ is a weak solution of the equation $Lu=f$.

For $u\in C^{2}\left(\Omega\right)$, we can integrate by parts twice,
which would give:
\begin{align*}
\int\left(Lu-f\right)\eta\dd^{n}x & =0
\end{align*}



\subsubsection*{Proof}

\begin{align*}
S\left(u+\tau\eta\right) & =\int_{\Omega}a^{ij}\left(\partial_{i}\left(u+\tau\eta\right)\right)\left(\partial_{j}\left(u+\tau\eta\right)\right)+c\left(u+\tau\eta\right)^{2}-2f\left(u+\tau\eta\right)\dd^{n}x=\\
 & =\tau^{2}\int_{\Omega}\left(a^{ij}\left(\partial_{i}\eta\right)\left(\partial_{j}\eta\right)+c\eta^{2}\right)\dd^{n}x+2\tau\int_{\Omega}\left(a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}\eta\right)+cu\eta-f\eta\right)\dd^{n}x+\\
 & \qquad+\int_{\Omega}\left(a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}u\right)+cu^{2}-2fu\right)\dd^{n}x
\end{align*}
$S\left(\tau\right)$ has a minimum at $\tau=0$. Therefore holds:
\begin{align*}
0 & =\frac{\dd}{\dd\tau}S\left(u+\tau\eta\right)\big|_{\tau=0}=2\int_{\Omega}\left(a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}\eta\right)+cu\eta-f\eta\right)\dd^{n}x\\
0 & =\int_{\Omega}\left(u\partial_{i}\left(-a^{ij}\left(\partial_{j}\eta\right)\right)+cu\eta-f\eta\right)\dd^{n}x
\end{align*}
\qqed


\section{Existence of Minimizers}

Let $\left(u_{n}\right)$ be a minimizing sequence. To show is $u_{n}\to u$
and:
\begin{align*}
S\left(u\right) & =\kappa:=\inf_{H_{0}^{1,2}\left(\Omega\right)}\left(S\right)
\end{align*}


More generally, let $\mathcal{H}$ be a Hilbert space. (In our case
we have $\mathcal{H}=H_{0}^{1,2}\left(\Omega\right)$.)
\begin{align*}
A\left(u,v\right) & :=\int_{\Omega}\left(a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}v\right)+cuv\right)\dd^{n}x\\
l\left(u\right) & :=-2\int_{\Omega}fu\dd^{n}x\\
S\left(u\right) & =A\left(u,u\right)+l\left(u\right)
\end{align*}
The bilinear map $A:\mathcal{H}\times\mathcal{H}\to\mathbb{R}$ and
the linear map $l:\mathcal{H}\to\mathbb{R}$ are continuous:
\begin{align*}
\abs{A\left(u,v\right)} & \le\opnorm A\norm u\cdot\norm v\\
\abs{l\left(u\right)} & \le\opnorm l\norm u
\end{align*}
Moreover $A$ is symmetric:
\begin{align*}
A\left(u,v\right) & =A\left(v,u\right)
\end{align*}



\subsection{Definition \textmd{(Coercive)}}

$A$ is called coercive (\foreignlanguage{ngerman}{koerziv, manchmal
auch elliptisch}) if there is a $\lambda\in\mathbb{R}_{>0}$ such
that for all $u\in\mathcal{H}$ holds:
\begin{align*}
A\left(u,u\right) & \ge\lambda\norm u_{\mathcal{H}}^{2}
\end{align*}
In our problem, this condition is fulfilled due to the uniform ellipticity:
\begin{align*}
A\left(u,u\right) & =\int_{\Omega}a^{ij}\left(\partial_{i}u\right)\left(\partial_{j}u\right)\dd^{n}x\sr{\ge}{\text{uniform}}{\text{ellipticity}}\lambda\int\norm{\DD u}^{2}\dd^{n}x\ge\\
 & \sr{\ge}{\text{Poincaré}}{}\frac{\lambda}{2}\int\norm{\DD u}^{2}\dd^{n}x+\frac{\lambda}{2c^{2}}\int\norm u^{2}\dd^{n}x\ge\tilde{c}\norm u_{H_{0}^{1,2}\left(\Omega\right)}^{2}
\end{align*}



\subsection{Theorem \textmd{(Unique Minimizer)}\label{sub:Thm-minimizer}}

Let $\left(\mathcal{H},\left\langle .,.\right\rangle \right)$ be
a Hilbert space with norm $\norm ._{\mathcal{H}}$, $V\subseteq\mathcal{H}$
closed and convex,
\begin{align*}
A:\mathcal{H}\times\mathcal{H} & \to\mathbb{R}
\end{align*}
bilinear, symmetric, continuous and coercive and
\begin{align*}
l:\mathcal{H} & \to\mathbb{R}
\end{align*}
linear and continuous.

Then the action $S\left(u\right):=A\left(u,u\right)+l\left(u\right)$
has a unique minimizer in $V$.


\subsubsection*{Proof}

The action is bounded from below:
\begin{align*}
S\left(v\right) & \ge\lambda\norm v_{\mathcal{H}}^{2}-\opnorm l\norm v_{\mathcal{H}}\sr{\ge}{\text{completing}}{\text{the square}}-\frac{\opnorm l^{2}}{2\lambda}
\end{align*}
Let $\kappa:=\inf_{V}\left(S\right)$. We choose a minimizing sequence
$u_{n}\in V$ with $S\left(u_{n}\right)\to\kappa$. We want to show
$u_{n}\to u$. Then by continuity follows:
\begin{align*}
S\left(u\right) & =\lim_{n\to\infty}\left(S\left(u_{n}\right)\right)=\kappa
\end{align*}
Since $\mathcal{H}$ is a Hilbert space and thus complete and $V$
is closed, we only have to verify the Cauchy condition, to show the
existence. Because $V$ is convex holds $\frac{1}{2}\left(u_{n}+u_{m}\right)\in V$
. So we get:
\begin{align*}
\kappa & =\inf_{V}\left(S\right)\le S\left(\frac{u_{n}+u_{m}}{2}\right)=A\left(\frac{u_{n}+u_{m}}{2},\frac{u_{n}+u_{m}}{2}\right)+l\left(\frac{u_{n}+u_{m}}{2}\right)=\\
 & =\frac{1}{4}A\left(u_{n},u_{n}\right)+\frac{1}{4}A\left(u_{m},u_{m}\right)+\frac{1}{2}A\left(u_{n},u_{m}\right)+\frac{1}{2}l\left(u_{n}\right)+\frac{1}{2}l\left(u_{m}\right)=\\
 & =\frac{1}{2}S\left(u_{n}\right)+\frac{1}{2}S\left(u_{m}\right)-\frac{1}{4}A\left(u_{n},u_{n}\right)-\frac{1}{4}A\left(u_{m},u_{m}\right)+\frac{1}{2}A\left(u_{n},u_{m}\right)=\\
 & =\underbrace{\frac{1}{2}S\left(u_{n}\right)}_{\to\frac{\kappa}{2}}+\underbrace{\frac{1}{2}S\left(u_{m}\right)}_{\to\frac{\kappa}{2}}\underbrace{-\frac{1}{4}A\left(u_{n}-u_{m},u_{n}-u_{m}\right)}_{\le0}
\end{align*}
Thus converges $A\left(u_{n}-u_{m},u_{n}-u_{m}\right)\xrightarrow{n,m\to\infty}0$.
\begin{align*}
\norm{u_{n}-u_{m}}^{2} & \le\frac{1}{\lambda}A\left(u_{n}-u_{m},u_{n}-u_{m}\right)\xrightarrow{n,m\to\infty}0
\end{align*}
Hence $\left(u_{n}\right)$ is a Cauchy sequence and $u_{n}\to u$.

\emph{Uniqueness}: Let $u_{1},u_{2}$ be minimizers, i.e. $S\left(u_{1}\right)=\kappa=S\left(u_{2}\right)$.
Then holds:
\begin{align*}
\kappa & \le S\left(\frac{u_{1}+u_{2}}{2}\right)=\underbrace{\frac{1}{2}S\left(u_{1}\right)+\frac{1}{2}S\left(u_{2}\right)}_{=\kappa}\underbrace{-\frac{1}{4}A\left(u_{1}-u_{2},u_{1}-u_{2}\right)}_{\le0}
\end{align*}
\begin{align*}
\Rightarrow\quad A\left(u_{1}-u_{2},u_{1}-u_{2}\right) & =0\\
\Rightarrow\qquad u_{1} & =u_{2}
\end{align*}
\qqed


\subsection{Corollary}

Under the conditions of Theorem \ref{sub:Thm-minimizer} and if in
addition $V$ is a closed linear subspace of $\mathcal{H}$, there
is a unique $u\in V$ such that for all $\varphi\in V$ holds:
\begin{align*}
2A\left(u,\varphi\right)+l\left(\varphi\right) & =0
\end{align*}



\subsubsection{Proof}

For a minimizer $u$, $S\left(u+\tau\varphi\right)$ is minimal at
$\tau=0$. This is a polynomial in $\tau$ and thus holds:
\begin{align*}
0 & =\frac{\dd}{\dd\tau}S\left(u+\tau\varphi\right)\big|_{\tau=0}=\frac{\dd}{\dd\tau}\left(A\left(u+\tau\varphi,u+\tau\varphi\right)+l\left(u+\tau\varphi\right)\right)\big|_{\tau=0}=2A\left(u,\varphi\right)+l\left(\varphi\right)
\end{align*}
\qqed

%DATE: Mi 19.6.13


\chapter{Convex Variational Problems}

(cf. \noun{J. Jost}: \emph{Partial Differential Equations,} Section
8.6)

We consider the action
\begin{align*}
S\left(u\right) & :=\int_{\Omega}f\left(x,\DD u\right)\dd^{n}x
\end{align*}
for $u\in H_{0}^{1,2}\left(\Omega\right)$. $\Omega\subseteq\mathbb{R}^{n}$
is assumed to be bounded with smooth boundary.


\section{Theorem \textmd{(Existence of Minimizer for \texorpdfstring{$S$}{S})}\label{sec:Thm-S-attains-minimum}}

Let $f:\Omega\times\mathbb{R}^{n}\to\mathbb{R}$ be a function with
the following properties:
\begin{enumerate}[label=\roman*)]
\item $f\left(.,v\right):\Omega\to\mathbb{R}$ is measurable for any $v\in\mathbb{R}^{n}$.
\item $f\left(x,.\right):\mathbb{R}^{n}\to\mathbb{R}$ is convex for all
$x\in\Omega$, i.e. for all $v,w\in\mathbb{R}^{n}$ and $\tau\in\left[0,1\right]$
holds:
\begin{align*}
f\left(x,\tau v+\left(1-\tau\right)w\right) & \le\tau f\left(x,v\right)+\left(1-\tau\right)f\left(x,w\right)
\end{align*}

\item Coercivity: There is a constant $\kappa\in\mathbb{R}_{>0}$ and an
integrable function $\gamma\in L^{1}\left(\Omega\right)$ such that
for all $x\in\Omega$ and $v\in\mathbb{R}^{n}$ holds:
\begin{align*}
f\left(x,v\right) & \ge-\gamma\left(x\right)+\kappa\norm v^{2}
\end{align*}

\end{enumerate}
Then $S$ attains its minimum, i.e. there is a $u_{0}\in H_{0}^{1,2}\left(\Omega\right)$
with:
\begin{align*}
S\left(u_{0}\right) & =\inf_{u\in H_{0}^{1,2}\left(\Omega\right)}\left(S\left(u\right)\right)
\end{align*}
Convexity is crucial for uniqueness.

\begin{figure}[H]
\noindent \centering{}\hfill{}\subfloat[convex: unique minimum]{\begin{tikzpicture}
 \begin{axis}[width=8cm, axis x line=middle, axis y line=middle, xtick={0}, ytick={0}, xlabel=$u$, ylabel=$S$, domain=-3:3,samples=300, ymin=0]
  \addplot[mark=none] {x^2+1};
 \end{axis}
\end{tikzpicture}}\hfill{}\subfloat[not convex: two minima]{\begin{tikzpicture}
 \begin{axis}[width=8cm, axis x line=middle, axis y line=middle, xtick={0}, ytick={0}, xlabel=$u$, ylabel=$S$, domain=-3:3,samples=300, ymin=0]
  \addplot[mark=none] {0.2*x^4-x^2+2};
 \end{axis}
\end{tikzpicture}}\hfill{}\caption{one-dimensional analogon}
\end{figure}


Convexity and coercivity are also essential for proving existence.
\begin{align*}
S:H_{0}^{1,2}\left(\Omega\right) & \to\mathbb{R}\cup\left\{ \infty\right\} \\
u & \mapsto\int_{\Omega}f\left(x,\DD u\right)\dd^{n}x=\underbrace{\int_{\Omega}\underbrace{\left(f\left(x,\DD u\right)+\gamma\right)}_{\ge0}\dd^{n}x}_{\text{could be }\infty}-\int_{\Omega}\underbrace{\gamma}_{\in L^{1}}\dd^{n}x
\end{align*}



\section{Lemma \textmd{(Lower Semi-Continuous)}\label{sec:Lem-J-low-sem-cont}}

The functional
\begin{align*}
J\left(v\right) & =\int_{\Omega}f\left(x,v\left(x\right)\right)\dd^{n}x
\end{align*}
for $v\in L^{2}\left(\Omega,\mathbb{R}^{n}\right)$ is lower semi-continuous,
i.e. if $v_{k}\to v$ converges in $L^{2}\left(\Omega,\mathbb{R}^{n}\right)$,
then holds:
\begin{align*}
J\left(v\right) & \le\liminf_{k\to\infty}J\left(v_{k}\right)
\end{align*}



\subsubsection*{Proof}

The function $v\in L^{2}\left(\Omega\right)$ is clearly measurable,
and since $f\left(x,.\right)$ is continuous (every convex function
is continuous) and $f\left(.,v\right)$ is measurable, it follows
that $f\left(x,v\left(x\right)\right)$ is measurable.

In more detail: Introduce a new function:
\begin{align*}
g:\Omega\times\Omega & \to\mathbb{R}\\
g\left(x,y\right) & =f\left(x,v\left(y\right)\right)
\end{align*}
$g\left(x,.\right)$ is measurable, because $f\left(x,.\right)$ is
measurable, and $g\left(.,y\right)$ is measurable as a composition
of a measurable function with a continuous function. Let
\begin{align*}
\iota:\Omega & \to\Omega\times\Omega\\
x & \mapsto\left(x,x\right)
\end{align*}
be the natural injection, which is continuous. So the composition
$f\left(x,v\left(x\right)\right)=g\left(\iota\left(x\right)\right)$
is measurable, because $g$ is measurable and $\iota$ continuous.

Let $v_{n}\to v$ converge in $L^{2}\left(\Omega,\mathbb{R}^{n}\right)$,
then $v_{k}\left(x\right)\to v\left(x\right)$ converges for almost
all $x\in\Omega$. Since $f\left(x,.\right)$ and $\norm .$ are continuous,
it holds: 
\begin{align*}
f\left(x,v_{k}\left(x\right)\right)-\kappa\norm{v_{k}\left(x\right)}^{2} & \to\underbrace{f\left(x,v\left(x\right)\right)-\kappa\norm{v\left(x\right)}}_{\ge-\gamma\left(x\right)\in L^{1}\left(\Omega\right)}
\end{align*}
Question:
\begin{align*}
\lim_{k\to\infty}\int f\left(x,v_{k}\left(x\right)\right)-\kappa\norm{v\left(x\right)}^{2}\dd^{n}x & \sr =?{}\int f\left(x,v\left(x\right)\right)-\kappa\norm{v\left(x\right)}^{2}\dd^{n}x
\end{align*}
In general this is wrong, because the integrand is not necessarily
bounded from above. But Fatou's lemma yields:
\begin{align*}
\liminf_{k\to\infty}\int f\left(x,v_{k}\left(x\right)\right)-\kappa\norm{v_{k}\left(x\right)}^{2}\dd^{n}x & \ge\int f\left(x,v\left(x\right)\right)-\kappa\norm{v\left(x\right)}^{2}\dd^{n}x
\end{align*}
Example to illuminate Fatou's Lemma:
\begin{align*}
g_{k}\left(x\right) & =\sqrt{k}e^{-kx^{2}}
\end{align*}


\begin{figure}[H]
\noindent \centering{}\begin{tikzpicture}
 \begin{axis}[width=12cm, axis x line=middle, axis y line=middle, xtick={0}, ytick={0}, xlabel=$x$,  domain=-3:3, samples=300, ymin=0, ymax=2.1]
  \addplot[mark=none] {exp(-x^2)};
  \addlegendentry{$g_1$}
  \addplot[blue, dashed,mark=none] {exp(-4*x^2)*2};
  \addlegendentry{$g_4$}
 \end{axis}
\end{tikzpicture}\caption{Example for Fatou's Lemma}
\end{figure}


$g_{k}\left(x\right)\to0$ converges almost everywhere, but the integral
does not converge to zero:
\begin{align*}
\int g_{k}\left(x\right)\dd^{n}x & =c>0=\int0\dd^{n}x
\end{align*}
Fatou's Lemma yields:
\begin{align*}
\liminf_{k\to\infty}\left(J\left(v_{k}\right)-\kappa\int_{\Omega}\norm{v_{k}}^{2}\dd^{n}x\right) & \ge J\left(v\right)-\kappa\int_{\Omega}\norm{v\left(x\right)}^{2}\dd^{n}x
\end{align*}
Since $v_{k}\to v$ converges in $L^{2}\left(\Omega\right)$, also
$\int_{\Omega}\norm{v_{k}}^{2}\dd^{n}x\to\int_{\Omega}\norm v^{2}\dd^{n}x$
converges and thus follows:
\begin{align*}
\liminf_{k\to\infty}J\left(v_{k}\right) & \ge J\left(v\right)
\end{align*}
\qqed


\section{Lemma \textmd{(Convex)}\label{sec:Lem-J-convex}}

$J$ is convex.


\subsubsection*{Proof}

\begin{align*}
J\left(\tau v+\left(1-\tau\right)w\right) & =\int_{\Omega}f\left(x,\tau v\left(x\right)+\left(1-\tau\right)w\left(x\right)\right)\dd^{n}x\le\\
 & \le\int_{\Omega}\tau f\left(x,v\left(x\right)\right)+\left(1-\tau\right)f\left(x,w\left(x\right)\right)\dd^{n}x=\\
 & =\tau J\left(v\right)+\left(1-\tau\right)J\left(w\right)
\end{align*}
\qqed

To summarize:


\section{Lemma}

The functional
\begin{align*}
S\left(u\right) & =\int_{\Omega}f\left(x,\DD u\right)\dd^{n}x=J\left(\DD u\right)
\end{align*}
is convex and lower semi-continuous on $H_{0}^{1,2}\left(\Omega\right)$.


\subsubsection*{Proof}

For $u\in H_{0}^{1,2}\left(\Omega\right)$ follows $\DD u\in L^{2}\left(\Omega\right)$
and the claim follows from the Lemmata \ref{sec:Lem-J-low-sem-cont}
and \ref{sec:Lem-J-convex}.\qqed


\section{Theorem \textmd{(Existence of Minimizer for \texorpdfstring{$S_{\lambda}$}{Sλ})}\label{sec:Thm-S_lambda-minimizer}}

Let $\left(\mathcal{H},\left\langle .,.\right\rangle \right)$ be
a Hilbert space and
\begin{align*}
S:\mathcal{H} & \to\mathbb{R}\cup\left\{ \infty\right\} 
\end{align*}
be a functional which is bounded from below and $S\not=\infty$. For
every $\lambda\in\mathbb{R}_{>0}$ and $u\in\mathcal{H}$, the functional
\begin{align*}
S_{\lambda}\left(y\right) & :=S\left(y\right)+\lambda\norm{u-y}^{2}
\end{align*}
has a unique minimizer $u_{\lambda}\in\mathcal{H}$, i.e.:
\begin{align*}
S\left(u_{\lambda}\right)+\lambda\norm{u-u_{\lambda}}^{2} & =\inf_{y\in\mathcal{H}}S_{\lambda}\left(y\right)
\end{align*}
If $\norm{u_{\lambda}}$ is bounded in the limit $\lambda\searrow0$,
then
\begin{align*}
\lim_{\lambda\searrow0}u_{\lambda} & :=u_{0}
\end{align*}
exists and $u_{0}$ is a minimizer of $S$, i.e.:
\begin{align*}
S\left(u_{0}\right) & =\inf_{y\in\mathcal{H}}S\left(y\right)
\end{align*}
Note: $u_{0}$ may still depend on $u$, so the uniqueness is unclear
at the moment. (Later we prove uniqueness.)

Before coming to the proof of the theorem, let us apply it to our
variational problem.


\subsubsection{Proof of Theorem \ref{sec:Thm-S-attains-minimum}}

\begin{align*}
S\left(y\right) & =\int_{\Omega}f\left(x,\DD y\right)\dd^{n}x
\end{align*}


For $S\not=\infty$ we have nothing to prove, so we assume $S\not=\infty$.
\begin{enumerate}
\item $S$ is bounded from below: By coercivity holds:
\begin{align*}
S\left(y\right) & \ge\int_{\Omega}\left(-\gamma\left(x\right)+\kappa\norm{\DD y}^{2}\right)\dd^{n}x
\end{align*}
Now we use Poincaré's inequality, which holds, since $\Omega$ is
bounded.
\begin{align*}
\norm y_{H_{0}^{1,2}\left(\Omega\right)}^{2} & \le C\left(\Omega\right)\int_{\Omega}\norm{\DD y}^{2}\dd^{n}x\sr{\le}{\kappa>0}{}\frac{C\left(\Omega\right)}{\kappa}\bigg(S\left(y\right)+\underbrace{\int_{\Omega}\gamma\left(x\right)\dd^{n}x}_{=:\tilde{C}<\infty}\bigg)
\end{align*}
Thus follows:
\begin{align*}
S\left(y\right) & \ge\frac{\kappa}{C\left(\Omega\right)}\norm y_{H_{0}^{1,2}\left(\Omega\right)}-\tilde{C}\ge-\tilde{C}
\end{align*}

\item $\norm{u_{\lambda}}$ stays bounded for $\lambda\searrow0$:
\begin{align*}
\norm{u_{\lambda}}_{H_{0}^{1,2}\left(\Omega\right)}^{2} & \le\frac{C\left(\Omega\right)}{\kappa}\left(S\left(u_{\lambda}\right)+\tilde{C}\right)\le\frac{C\left(\Omega\right)}{\kappa}\Big(\underbrace{S\left(u_{\lambda}\right)+\lambda\norm{u-u_{\lambda}}^{2}}_{=S_{\lambda}\left(u_{\lambda}\right)}+\tilde{C}\Big)\le\\
 & \sr{\le}{S_{\lambda}\left(u_{\lambda}\right)\le S_{\lambda}\left(u\right)}{}\frac{C\left(\Omega\right)}{\kappa}\left(S\left(u\right)+\lambda\norm{u-u}^{2}+\tilde{C}\right)=\frac{C\left(\Omega\right)}{\kappa}\left(S\left(u\right)+\tilde{C}\right)
\end{align*}
This upper bound is uniform in $\lambda$.
\end{enumerate}
This completes the proof, assuming that Theorem \ref{sec:Thm-S_lambda-minimizer}
holds.\qqed[\ref{sec:Thm-S-attains-minimum}]


\subsubsection{Proof of Theorem \ref{sec:Thm-S_lambda-minimizer}}

Let $y_{k}$ be a minimizing sequence of the functional $S\left(y\right)+\lambda\norm{u-y}^{2}$.
Set:
\begin{align*}
y_{k,l} & :=\frac{1}{2}\left(y_{k}+y_{l}\right)\\
s_{\lambda} & :=\inf_{y\in\mathcal{H}}\left(S\left(y\right)+\lambda\norm{u-y}^{2}\right)
\end{align*}
Then holds:
\begin{align*}
\norm{u-y_{k,l}}^{2} & =\norm{u-\frac{1}{2}\left(y_{k}+y_{l}\right)}^{2}=\norm{\frac{1}{2}\left(u-y_{k}\right)+\frac{1}{2}\left(u-y_{l}\right)}^{2}=\\
 & =\frac{1}{4}\norm{\left(u-y_{k}\right)+\left(u-y_{l}\right)}^{2}=\\
 & \sr ={\text{parallelogram}}{\text{law}}-\frac{1}{4}\norm{\left(u-y_{k}\right)-\left(u-y_{l}\right)}^{2}+\frac{1}{2}\norm{u-y_{k}}^{2}+\frac{1}{2}\norm{u-y_{l}}^{2}=\\
 & =-\frac{1}{4}\norm{y_{l}-y_{k}}^{2}+\frac{1}{2}\norm{u-y_{k}}^{2}+\frac{1}{2}\norm{u-y_{l}}
\end{align*}
So we get:
\begin{align*}
s_{\lambda} & \le S\left(y_{k,l}\right)+\lambda\norm{u-y_{k,l}}^{2}\le\\
 & \sr{\le}{S\text{ convex}}{}\frac{1}{2}S\left(y_{k}\right)+\frac{1}{2}S\left(y_{l}\right)+\frac{\lambda}{2}\norm{u-y_{k}}^{2}+\frac{\lambda}{2}\norm{u-y_{l}}^{2}-\frac{\lambda}{4}\norm{y_{k}-y_{l}}^{2}=\\
 & =\underbrace{\frac{1}{2}\left(S\left(y_{k}\right)+\lambda\norm{u-y_{k}}^{2}\right)}_{\to\frac{1}{2}s_{\lambda}}+\underbrace{\frac{1}{2}\left(S\left(y_{l}\right)+\lambda\norm{u-y_{l}}^{2}\right)}_{\to\frac{1}{2}s_{\lambda}}\underbrace{-\frac{\lambda}{4}\norm{y_{k}-y_{l}}^{2}}_{\le0}
\end{align*}
Therefore converges $y_{k}-y_{l}\xrightarrow{k,l\to\infty}0$ and
thus $y_{k}\to u_{\lambda}$. Now $u_{\lambda}$ is a minimizer, because
$S$ is lower semi-continuous.

The limit $u_{\lambda}$ is unique, because every minimizing sequence
converges, so for another minimizer $v_{\lambda}$ we consider the
sequence $\left(u_{\lambda},v_{\lambda},u_{\lambda},v_{\lambda},\ldots\right)$
and see $u_{\lambda}=v_{\lambda}$.

For $\lambda_{1},\lambda_{2}\in\mathbb{R}_{>0}$ with $\lambda_{1}<\lambda_{2}$
holds, because $u_{\lambda_{1}}$ is the minimizer of $S\left(.\right)+\lambda_{1}\norm{u-.}^{2}$.
\begin{align*}
S\left(u_{\lambda_{2}}\right)+\lambda_{1}\norm{u_{\lambda_{2}}-u}^{2} & \ge S\left(u_{\lambda_{1}}\right)+\lambda_{1}\norm{u_{\lambda_{1}}-u}\qquad\qquad/+\left(\lambda_{2}-\lambda_{1}\right)\norm{u-u_{\lambda_{2}}}^{2}\\
S\left(u_{\lambda_{2}}\right)+\lambda_{2}\norm{u-u_{\lambda_{2}}}^{2} & \ge S\left(u_{\lambda_{1}}\right)+\lambda_{2}\norm{u-u_{\lambda_{1}}}^{2}+\left(\lambda_{1}-\lambda_{2}\right)\left(\norm{u-u_{\lambda_{1}}}^{2}-\norm{u-u_{\lambda_{2}}}^{2}\right)
\end{align*}
Now holds, because $u_{\lambda_{2}}$ is the minimizer of $S\left(.\right)+\lambda_{2}\norm{u-.}^{2}$.
\begin{align*}
S\left(u_{\lambda_{2}}\right)+\lambda_{2}\norm{u-u_{\lambda_{2}}}^{2} & \le S\left(u_{\lambda_{1}}\right)+\lambda_{2}\norm{u-u_{\lambda_{1}}}^{2}
\end{align*}
In total we get:
\begin{align*}
S\left(u_{\lambda_{1}}\right)+\lambda_{2}\norm{u-u_{\lambda_{1}}}^{2} & \ge S\left(u_{\lambda_{1}}\right)+\lambda_{2}\norm{u-u_{\lambda_{1}}}^{2}+\left(\lambda_{1}-\lambda_{2}\right)\left(\norm{u-u_{\lambda_{1}}}^{2}-\norm{u-u_{\lambda_{2}}}^{2}\right)\\
0 & \ge\underbrace{\left(\lambda_{1}-\lambda_{2}\right)}_{<0}\left(\norm{u-u_{\lambda_{1}}}^{2}-\norm{u-u_{\lambda_{2}}}^{2}\right)\\
0 & \le\norm{u-u_{\lambda_{1}}}^{2}-\norm{u-u_{\lambda_{2}}}^{2}\\
\norm{u-u_{\lambda_{2}}}^{2} & \le\norm{u-u_{\lambda_{1}}}^{2}
\end{align*}
Thus $\norm{u-u_{\lambda}}$ is increasing as $\lambda\searrow0$.

By assumption holds uniformly in $\lambda$:
\begin{align*}
\norm{u-u_{\lambda}} & \le\norm u+\norm{u_{\lambda}}<C
\end{align*}
Thus $\norm{u-u_{\lambda}}$ converges as $\lambda\searrow0$, i.e.
for all $\varepsilon\in\mathbb{R}_{>0}$ exists a $\lambda_{0}$ such
that for all $\lambda_{1},\lambda_{2}\in\left(0,\lambda_{0}\right)$
holds:
\begin{align*}
\abs{\norm{u-u_{\lambda_{1}}}^{2}-\norm{u-u_{\lambda_{2}}}^{2}} & <\frac{\varepsilon}{2}
\end{align*}
Now again we use the convexity. Define:
\begin{align*}
u_{1,2} & :=\frac{1}{2}\left(u_{\lambda_{1}}+u_{\lambda_{2}}\right)
\end{align*}
Without loss of generality we assume $S\left(u_{\lambda_{1}}\right)\ge S\left(u_{\lambda_{2}}\right)$
so we get:
\begin{align*}
S\left(u_{1,2}\right) & \le S\left(u_{\lambda_{1}}\right)\\
0 & \le S\left(u_{\lambda_{1}}\right)-S\left(u_{1,2}\right)
\end{align*}
\begin{align*}
0 & \le S\left(u_{\lambda_{1}}\right)+\lambda_{1}\norm{u-u_{1,2}}^{2}-\left(S\left(u_{1,2}\right)+\lambda_{1}\norm{u-u_{1,2}}^{2}\right)=\\
 & \sr ={\text{parallelogram}}{\text{law}}S\left(u_{\lambda_{1}}\right)+\lambda_{1}\left(\frac{1}{2}\norm{u-u_{\lambda_{1}}}^{2}+\frac{1}{2}\norm{u-u_{\lambda_{2}}}^{2}-\frac{1}{4}\norm{u_{\lambda_{1}}-u_{\lambda_{2}}}^{2}\right)-S_{\lambda_{1}}\left(u_{1,2}\right)=\\
 & =S\left(u_{\lambda_{1}}\right)+\lambda_{1}\norm{u-u_{\lambda_{1}}}^{2}+\frac{\lambda_{1}}{2}\left(\norm{u-u_{\lambda_{2}}}^{2}-\norm{u-u_{\lambda_{1}}}^{2}\right)-\frac{\lambda_{1}}{4}\norm{u_{\lambda_{1}}-u_{\lambda_{2}}}^{2}-S_{\lambda_{1}}\left(u_{1,2}\right)\le\\
 & \le S_{\lambda_{1}}\left(u_{1,2}\right)+\frac{\lambda_{1}}{2}\left(\norm{u-u_{\lambda_{2}}}^{2}-\norm{u-u_{\lambda_{1}}}^{2}\right)-\frac{\lambda_{1}}{4}\norm{u_{\lambda_{1}}-u_{\lambda_{2}}}^{2}-S_{\lambda_{1}}\left(u_{1,2}\right)=\\
 & =\frac{\lambda_{1}}{2}\left(\norm{u-u_{\lambda_{2}}}^{2}-\norm{u-u_{\lambda_{1}}}^{2}\right)-\frac{\lambda_{1}}{4}\norm{u_{\lambda_{1}}-u_{\lambda_{2}}}^{2}\le\\
 & \le\frac{\lambda_{1}}{4}\varepsilon-\frac{\lambda_{1}}{4}\norm{u_{\lambda_{1}}-u_{\lambda_{2}}}^{2}
\end{align*}
This gives:
\begin{align*}
\norm{u_{\lambda_{1}}-u_{\lambda_{2}}}^{2} & \le\varepsilon
\end{align*}
Since $\varepsilon$ is arbitrary, we conclude that the limit ${\displaystyle \lim_{\lambda\searrow0}u_{\lambda}}=:u_{0}$
exists.

By lower semi-continuity of $S$ follows:
\begin{align*}
S\left(u_{0}\right) & \le\lim_{\lambda\searrow0}S\left(u_{\lambda}\right)=\inf_{u\in\mathcal{H}}S\left(u\right)
\end{align*}
This implies:
\begin{align*}
S\left(u_{0}\right) & =\inf_{u\in\mathcal{H}}S\left(u\right)
\end{align*}
\qqed[\ref{sec:Thm-S_lambda-minimizer}]


\part*{Appendix\thispagestyle{empty}}

\addcontentsline{toc}{part}{Appendix}

\fancyhead[R]{}
\fancyhead[C]{Appendix}


\chapter*{Acknowledgements}

\addcontentsline{toc}{section}{\hspace*{2.7em}Acknowledgements}

\fancyhead[R]{Acknowledgements}

My special thanks goes to Professor Finster, who gave this lecture
and allowed me to publish this script of the lecture.

I would also like to thank all those, who found errors by careful
reading and told me of them.

\vspace{1cm}


\hfill{}Andreas Völklein

\label{END}
\end{document}
